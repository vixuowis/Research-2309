{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f2f55-1b3f-4df2-b302-8c7b3e38ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8f1db5-f4b0-48d7-9467-b45337c9d662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911,\n",
       " {'code': \"###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\\n###Output: <<<domain>>>: Natural Language Processing Sentence Similarity\\n<<<api_call>>>: AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>:1. We first import the necessary classes and modules from the transformers package. This includes AutoTokenizer and AutoModel for loading the pre-trained models from Hugging Face.\\n2. We use the AutoModel.from_pretrained() method to load the 'princeton-nlp/unsup-simcse-roberta-base' model, which is specially designed for calculating sentence similarity.\\n3. To build the recommendation feature, we process the text of previously liked articles and compute sentence embeddings. For each new article, we compute its sentence embedding and compare it to the embeddings of previously liked articles.\\n4. If the similarity between the new article's embedding and any previous liked articles' embeddings is above a certain threshold, the new article is recommended to the user.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\",\n",
       "  'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "  'provider': 'Hugging Face Transformers',\n",
       "  'api_data': {'domain': 'Natural Language Processing Sentence Similarity',\n",
       "   'framework': 'Hugging Face Transformers',\n",
       "   'functionality': 'Feature Extraction',\n",
       "   'api_name': 'princeton-nlp/unsup-simcse-roberta-base',\n",
       "   'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "   'api_arguments': None,\n",
       "   'python_environment_requirements': ['transformers'],\n",
       "   'example_code': None,\n",
       "   'performance': {'dataset': None, 'accuracy': None},\n",
       "   'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.'}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "def load_jsonl_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            d = json.loads(l)\n",
    "            data.append(d)\n",
    "            \n",
    "    return data\n",
    "\n",
    "hf_eval_data = load_jsonl_data(\"gorilla/data/apibench/huggingface_eval.json\")\n",
    "len(hf_eval_data), hf_eval_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194308e8-c453-46e7-b50b-fd3e56887410",
   "metadata": {},
   "source": [
    "# 1. Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7f4489-990e-4cc3-9887-cb928604a0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'instruction': '###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.',\n",
       "  'domain': 'Natural Language Processing Sentence Similarity',\n",
       "  'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "  'api_provider': 'Hugging Face Transformers',\n",
       "  'code': \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\"},\n",
       " '###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instruction: apibench - {lib}_train.json - code - instruction\n",
    "\n",
    "import re\n",
    "\n",
    "def get_code_parts_from_apibench_data(data):\n",
    "    text = data['code']\n",
    "    instruction, _ = text.split(\"\\n###Output\")\n",
    "    \n",
    "    # Extracting domain, api_call, api_provider, and code using regular expressions\n",
    "    domain_pattern = r'<<<domain>>>: (.+?)\\n'\n",
    "    api_call_pattern = r'<<<api_call>>>: (.+?)\\n'\n",
    "    api_provider_pattern = r'<<<api_provider>>>: (.+?)\\n'\n",
    "    code_pattern = r'<<<code>>>: (.+)'\n",
    "\n",
    "    domain = re.search(domain_pattern, text).group(1)\n",
    "    api_call = re.search(api_call_pattern, text).group(1)\n",
    "    api_provider = re.search(api_provider_pattern, text).group(1)\n",
    "    code = re.search(code_pattern, text, re.DOTALL).group(1).strip()\n",
    "\n",
    "    return {\n",
    "        'instruction': instruction, \n",
    "        'domain': domain, \n",
    "        'api_call': api_call, \n",
    "        'api_provider': api_provider, \n",
    "        'code': code\n",
    "    }\n",
    "\n",
    "d = hf_eval_data[0]\n",
    "code_parts = get_code_parts_from_apibench_data(d)\n",
    "code_parts, code_parts['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd51b42c-cd38-4c17-a819-aebb1150d095",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\n",
      "{'code': '###Instruction: Design a feature for a social media website to '\n",
      "         'recommend articles to users based on how similar the articles are to '\n",
      "         'their previously liked articles.\\n'\n",
      "         '###Output: <<<domain>>>: Natural Language Processing Sentence '\n",
      "         'Similarity\\n'\n",
      "         '<<<api_call>>>: '\n",
      "         \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\"\n",
      "         '<<<api_provider>>>: Hugging Face Transformers\\n'\n",
      "         '<<<explanation>>>:1. We first import the necessary classes and '\n",
      "         'modules from the transformers package. This includes AutoTokenizer '\n",
      "         'and AutoModel for loading the pre-trained models from Hugging Face.\\n'\n",
      "         '2. We use the AutoModel.from_pretrained() method to load the '\n",
      "         \"'princeton-nlp/unsup-simcse-roberta-base' model, which is specially \"\n",
      "         'designed for calculating sentence similarity.\\n'\n",
      "         '3. To build the recommendation feature, we process the text of '\n",
      "         'previously liked articles and compute sentence embeddings. For each '\n",
      "         'new article, we compute its sentence embedding and compare it to the '\n",
      "         'embeddings of previously liked articles.\\n'\n",
      "         \"4. If the similarity between the new article's embedding and any \"\n",
      "         \"previous liked articles' embeddings is above a certain threshold, \"\n",
      "         'the new article is recommended to the user.\\n'\n",
      "         '<<<code>>>: from transformers import AutoTokenizer, AutoModel\\n'\n",
      "         'tokenizer = '\n",
      "         \"AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\"\n",
      "         'model = '\n",
      "         \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\",\n",
      " 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
      " 'provider': 'Hugging Face Transformers',\n",
      " 'api_data': {'domain': 'Natural Language Processing Sentence Similarity',\n",
      "              'framework': 'Hugging Face Transformers',\n",
      "              'functionality': 'Feature Extraction',\n",
      "              'api_name': 'princeton-nlp/unsup-simcse-roberta-base',\n",
      "              'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
      "              'api_arguments': None,\n",
      "              'python_environment_requirements': ['transformers'],\n",
      "              'example_code': None,\n",
      "              'performance': {'dataset': None, 'accuracy': None},\n",
      "              'description': 'An unsupervised sentence embedding model trained '\n",
      "                             'using the SimCSE approach with a Roberta base '\n",
      "                             'architecture.'}}\n"
     ]
    }
   ],
   "source": [
    "for d in hf_eval_data:\n",
    "    code_parts = get_code_parts_from_apibench_data(d)\n",
    "    print(code_parts['instruction'])\n",
    "    pprint.pp(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218abb4-6f0c-4b31-9da2-18cef79b3b7b",
   "metadata": {},
   "source": [
    "# 2. Function / Test Function\n",
    "- code part -> gpt -> function\n",
    "- dataset 问题，先通过 prompt 解决一部分，需要对应到 huggingface dataset 名称才能对应\n",
    "- prompt:\n",
    "    generate following code based on above infomation:\n",
    "    1. function with：\n",
    "    - detailed comments\n",
    "    - function description\n",
    "    2. test function with：\n",
    "    - test dataset\n",
    "    - using assert in test function\n",
    "    - do not compare number strictly\n",
    "    - if dataset is provided in performance - dataset, load the dataset, then select several sample from the dataset, otherwise, using online source, do not leave blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47e97fe-933a-43aa-93a2-8e06b6213673",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: openai==0.28.1 in /root/miniconda3/lib/python3.8/site-packages (0.28.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.8/site-packages (from openai==0.28.1) (3.8.6)\n",
      "Requirement already satisfied: requests>=2.20 in /root/miniconda3/lib/python3.8/site-packages (from openai==0.28.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.8/site-packages (from openai==0.28.1) (4.61.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (3.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (2021.5.30)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.8/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /root/miniconda3/lib/python3.8/site-packages (from scipy) (1.22.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tenacity in /root/miniconda3/lib/python3.8/site-packages (8.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.8/site-packages (0.5.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /root/miniconda3/lib/python3.8/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /root/miniconda3/lib/python3.8/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.8/site-packages (2.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests) (3.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests) (2.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28.1\n",
    "!pip install scipy\n",
    "!pip install tenacity\n",
    "!pip install tiktoken\n",
    "!pip install termcolor \n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6b8f8f-34ca-4891-9bc9-c03b5c82b1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Beijing'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://autoagents-ca-east.openai.azure.com\"\n",
    "openai.api_key = \"2864ce19a46540b2a0943df607ca6225\"\n",
    "openai.api_version = \"2023-12-01-preview\"\n",
    "\n",
    "def get_tool_call_res(prompt, tool):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      engine=\"gpt-4\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ],\n",
    "      tools = [tool],\n",
    "      tool_choice=\"auto\"\n",
    "    )\n",
    "    args = response.choices[0].message.tool_calls[0].function.arguments\n",
    "    \n",
    "    return json.loads(args)\n",
    "\n",
    "tool = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"location\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "        },\n",
    "        \"unit\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"location\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "res = get_tool_call_res(\"how is the weather of beijing\", tool)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d073c72-fad2-421e-9436-0715fc26f067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_name': 'extract_medical_embeddings',\n",
       " 'function_import': 'from transformers import AutoTokenizer, AutoModel',\n",
       " 'function_code': 'def extract_medical_embeddings(medical_term: str) -> torch.Tensor:\\n    \"\"\"\\n    Extract embedding vectors for a medical term using a pre-trained model.\\n\\n    Args:\\n        medical_term (str): The medical term to be converted into an embedding vector.\\n\\n    Returns:\\n        torch.Tensor: A tensor representing the embedding of the medical term.\\n\\n    Raises:\\n        ValueError: If the medical_term is empty.\\n    \"\"\"\\n    if not medical_term:\\n        raise ValueError(\\'The medical term cannot be empty.\\')\\n    tokenizer = AutoTokenizer.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\n    model = AutoModel.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\n    inputs = tokenizer(medical_term, return_tensors=\\'pt\\')\\n    outputs = model(**inputs)\\n    embeddings = outputs.last_hidden_state\\n    return embeddings',\n",
       " 'test_function_code': \"def test_extract_medical_embeddings():\\n    print('Testing started.')\\n    # Prepare a medical term for the test case\\n    medical_term = 'diabetes'\\n\\n    # Testing case 1: Valid medical term\\n    print('Testing case [1/1] started.')\\n    embeddings = extract_medical_embeddings(medical_term)\\n    assert embeddings is not None, f'Test case [1/1] failed: no embeddings returned.'\\n    assert embeddings.shape[1] > 0, f'Test case [1/1] failed: embedding vector is empty.'\\n    print('Testing finished.')\\n    return 'All test cases passed.'\",\n",
       " 'call_test_function_line': 'test_extract_medical_embeddings()',\n",
       " 'requirements_install': '!pip install -U transformers'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_function_from_data(data, err):\n",
    "    prompt = f\"\"\"\n",
    "**基本信息：**\n",
    "{data}\n",
    "\n",
    "**任务要求：**\n",
    "1. 请设计一个需求，用一句话描述。\n",
    "2. 根据上述描述，生成实现该需求的代码。    \n",
    "3. function comments should follow Google Python Style Guide, includes args, returns, and raises\n",
    "4. 根据生成的代码，编写相应的测试函数。\n",
    "\n",
    "**测试样例代码：**\n",
    "1. 函数开始打印 Testing stared\n",
    "2. 测试用例开始打印 Testing case [x/x] started\n",
    "3. 函数结束打印 Testing finished\n",
    "```python\n",
    "def test_...():\n",
    "    print(\"Testing started.\")\n",
    "    dataset = load_dataset(\"...\")\n",
    "    sample_data = dataset[0]  # 如果是图像数据，从数据集中抽取一个样本\n",
    "\n",
    "    # 测试用例 1：...\n",
    "    print(\"Testing case [1/3] started.\")\n",
    "    assert assert 1, f\"Test case [1/3] failed: ...\"\n",
    "\n",
    "    # 测试用例 2：...\n",
    "    print(\"Testing case [2/3] started.\")\n",
    "    assert assert 2, f\"Test case [2/3] failed: ...\"\n",
    "\n",
    "    # 测试用例 3：...\n",
    "    print(\"Testing case [3/3] started.\")\n",
    "    assert assert 3, f\"Test case [3/3] failed: ...\"\n",
    "    print(\"Testing finished.\")\n",
    "\n",
    "# 运行测试函数\n",
    "test_...()\n",
    "```\"\"\"\n",
    "    tool = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_codes\",\n",
    "            \"description\": \"生成基于输入的函数代码（function_code）和测试函数代码（test_function_code）。\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"function_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"函数名称\"\n",
    "                    },\n",
    "                    \"function_import\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"在函数代码之前导入必要的库\"\n",
    "                    },\n",
    "                    \"function_code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"独立的函数代码定义和功能的注释、代码实现、分步注释\"\n",
    "                    },\n",
    "                    \"test_function_code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"独立的测试函数代码，不包含调用代码，参照测试样例的代码\"\n",
    "                    },\n",
    "                    \"call_test_function_line\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"调用测试函数的代码行\"\n",
    "                    },\n",
    "                    \"requirements_install\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"安装必要的依赖，使用最新的，e.g. !pip install -U package_1 package_2\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"function_name\", \"function_import\", \"function_code\", \"test_function_code\", \"call_test_function_line\", \"requirements_install\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = get_tool_call_res(prompt, tool)\n",
    "    return res\n",
    "\n",
    "res = get_function_from_data(hf_eval_data[1], \"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d10a1015-d50a-468e-8007-932156d86735",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': '###Instruction: The user is interested in a tool to find relationships between medical terms.\\n###Output: <<<domain>>>: Multimodal Feature Extraction\\n<<<api_call>>>: AutoModel.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. We import the necessary classes from the transformers package provided by Hugging Face.\\n2. We then call the \"AutoModel.from_pretrained\" method with the argument \\'GanjinZero/UMLSBert_ENG\\' to load this pretrained model.\\n3. This model, which is particularly suitable for finding relationships between medical terms, can be used to convert medical terms into embeddings (dense vectors).\\n4. These embeddings can then be compared to find similarities and relationships between various medical terms.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\nmodel = AutoModel.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\n\\ninputs = tokenizer(medical_term, return_tensors=\"pt\")\\noutputs = model(**inputs)\\nembeddings = outputs.last_hidden_state\\n', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER'}}\n",
      "----------------------\n",
      "!pip install -U transformers\n",
      "----------------------\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "----------------------\n",
      "def extract_medical_embeddings(medical_term: str) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Extract embedding vectors for a medical term using a pre-trained model.\n",
      "\n",
      "    Args:\n",
      "        medical_term (str): The medical term to be converted into an embedding vector.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: A tensor representing the embedding of the medical term.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the medical_term is empty.\n",
      "    \"\"\"\n",
      "    if not medical_term:\n",
      "        raise ValueError('The medical term cannot be empty.')\n",
      "    tokenizer = AutoTokenizer.from_pretrained('GanjinZero/UMLSBert_ENG')\n",
      "    model = AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\n",
      "    inputs = tokenizer(medical_term, return_tensors='pt')\n",
      "    outputs = model(**inputs)\n",
      "    embeddings = outputs.last_hidden_state\n",
      "    return embeddings\n",
      "----------------------\n",
      "def test_extract_medical_embeddings():\n",
      "    print('Testing started.')\n",
      "    # Prepare a medical term for the test case\n",
      "    medical_term = 'diabetes'\n",
      "\n",
      "    # Testing case 1: Valid medical term\n",
      "    print('Testing case [1/1] started.')\n",
      "    embeddings = extract_medical_embeddings(medical_term)\n",
      "    assert embeddings is not None, f'Test case [1/1] failed: no embeddings returned.'\n",
      "    assert embeddings.shape[1] > 0, f'Test case [1/1] failed: embedding vector is empty.'\n",
      "    print('Testing finished.')\n",
      "    return 'All test cases passed.'\n",
      "----------------------\n",
      "test_extract_medical_embeddings()\n"
     ]
    }
   ],
   "source": [
    "print(hf_eval_data[1])\n",
    "print(\"----------------------\")\n",
    "print(res['requirements_install'])\n",
    "print(\"----------------------\")\n",
    "print(res['function_import'])\n",
    "print(\"----------------------\")\n",
    "print(res['function_code'])\n",
    "print(\"----------------------\")\n",
    "print(res['test_function_code'])\n",
    "print(\"----------------------\")\n",
    "print(res['call_test_function_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f8c95-c743-489a-9e29-e4404da01dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1...2...3...4...Retrying... (Attempt 1/3)...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...40...41...42...43...44...45...46...47...48...49...50...51...52...53...54...55...56...57...58...59...60...61...62...63...64...65...66...67...68...69...70...71...72...73...74...75...76...77...78...79...80...81...82...83...84...85...86...87...88...89...90...91...92...93...94...95...96...97...98...99...100...101...102...103...104...105...106...107...108...109...110...111...112...113...114...115...116...117...118...Retrying... (Attempt 1/3)...119...120...121...122...123...124...125...126...127...128...129...130...131...132...133...134...135...136...137...138...139...140...141...142...143...144...145...146...147...148...149...150...151...152...153...154...155...156...157...158...159...160...161...162...163...164...165...166...167...168...169...170...171...172...173...174...175...176...177...178...179...180...181...182...183...184...185...186...187...Retrying... (Attempt 1/3)...188...189...190...191...192...193...194...195...196...197...198...199...200...201...202...203...204...205...206...207...208...209...210...211...212...213...214...215...216...217...218...219...220...221...222...223...224...225...226...227...228...229...230...231...232...233...234...235...236...237...238...239...240...241...242...243...244...245...246...247...248...249...250...251...252...253...254...255...256...257...258...259...260...261...262...263...264...265...266...267...268...269...Retrying... (Attempt 1/3)...270...271...272...273...274...275...276...277...278...279...280...281...282...283...284...285...286...287...288...289...290...291...292...293...294...295...296...297...298...299...300...301...302...303...304...305...306...307...308...309...310...311...312...313...314...315...316...317...318...319...320...321...322...323...324...325...326...327...328...329...330...331...332...333...334...335...336...337...338...339...340...341...342..."
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# err_dir = \"output/hf-eval-data-v2\"\n",
    "output_dir = \"output/hf-eval-data-v4_2\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "for idx, d in enumerate(hf_eval_data):\n",
    "    print(idx + 1, end=\"...\")\n",
    "    formatted_number = str(idx + 1).zfill(5)\n",
    "    \n",
    "    # 跳过已生成的\n",
    "    matching_files = glob.glob(f\"{output_dir}/f{formatted_number}_*\")\n",
    "    if matching_files:\n",
    "        print(\"skip\", end=\"...\")\n",
    "        continue\n",
    "    \n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            # err = get_v2_err(err_dir, idx + 1)\n",
    "            resp = get_function_from_data(d, \"\")\n",
    "\n",
    "            # 写入 prompt\n",
    "            with open(f\"{output_dir}/f{formatted_number}_{resp['function_name']}.prompt\", 'w') as f:\n",
    "                f.write(str(d) + \"\\n\\n\")\n",
    "                # f.write(str(err))\n",
    "\n",
    "            # 写入 python\n",
    "            with open(f\"{output_dir}/f{formatted_number}_{resp['function_name']}.py\", 'w') as f:\n",
    "                f.write(\"# requirements_file --------------------\\n\\n\")\n",
    "                f.write(resp['requirements_install'])\n",
    "                \n",
    "                f.write(\"\\n\\n# function_import --------------------\\n\\n\")\n",
    "                f.write(resp['function_import'])\n",
    "\n",
    "                f.write(\"\\n\\n# function_code --------------------\\n\\n\")\n",
    "                f.write(resp['function_code'])\n",
    "\n",
    "                f.write(\"\\n\\n# test_function_code --------------------\\n\\n\")\n",
    "                f.write(resp['test_function_code'])\n",
    "                \n",
    "                f.write(\"\\n\\n# call_test_function_line --------------------\\n\\n\")\n",
    "                f.write(resp['call_test_function_line'])\n",
    "\n",
    "            break  # Break out of the loop if the operation is successful\n",
    "        except Exception:\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{max_retries})\", end=\"...\")\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\", end=\"...\")\n",
    "                traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e7dac1e-105a-4efa-af88-ae2d33d6d4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...!...!...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...40...41...42...43...44...45...46...47...48...49...50...51...52...53...54...55...56...57...58...59...60...61...62...63...64...65...66...67...68...69...70...71...72...73...74...75...76...77...78...79...80...81...82...83...84...85...86...87...88...89...90...91...92...93...94...95...96...97...98...99...!...!...100...101...102...103...104...!...!...!...105...106...107...108...109...110...111...112...113...114...115...116...117...118...119...120...121...122...123...124...125...126...127...128...129...130...131...132...133...134...135...136...137...138...139...140...141...142...143...144...145...146...!...!...!...!...147...148...149...150...151...152...153...154...155...156...157...158...159...160...161...162...163...164...165...166...167...168...169...170...171...172...173...174...175...176...177...178...179...180...181...182...183...184...185...!...!...186...187...188...189...190...191...192...193...!...!...194...195...!...!...196...197...198...199...200...201...202...203...204...205...206...207...208...209...210...211...212...213...214...215...216...217...218...219...220...221...222...223...224...225...226...227...228...229...230...231...232...233...234...235...236...237...238...239...240...241...242...243...244...245...246...247...!...248...249...250...251...252...253...254...255...256...257...258...259...260...261...262...263...264...265...266...267...268...269...270...271...272...!...!...!...273...274...275...276...!...!...277...!...!...278...279...280...281...282...283...284...285...286...287...288...289...290...291...292...293...294...295...296...297...298...299...300...301...302...303...304...305...306...307...308...309...310...311...312...313...314...315...316...317...318...319...320...321...322...323...324...325...326...327...328...329...330...331...332...333...334...335...336...337...338...339...340...341...342...343...344...345...346...347...348...349...350...351...352...353...354...355...356...357...!...!...!...!...358...!...!...!...359...360...361...362...363...364...365...366...367...368...369...370...371...372...373...374...375...376...!...!...377...378...379...380...381...382...383...384...385...386...387...388...389...390...391...392...393...394...395...396...397...398...399...400...401...402...403...404...405...406...407...408...409...410...411...412...413...414...415...416...417...418...419...420...421...422...423...424...425...426...427...428...429...430...431...432...433...434...435...436...437...438...439...440...441...442...443...444...445...446...447...448...449...450...451...452...453...454...455...!...!...!...!...456...457...458...!...!...!...459...460...461...462...463...464...!...!...465...466...467...468...469...470...!...471...472...473...474...475...476...477...478...479...480...481...482...483...484...485...486...487...488...489...490...491...492...493...494...495...496...497...498...499...500...501...502...503...504...505...506...507...508...509...510...511...512...513...514...515...516...517...518...519...520...521...522...523...524...525...526...527...528...529...530...531...532...533...534...535...536...537...538...539...540...541...542...543...544...545...546...547...548...549...550...551...552...553...554...555...556...!...!...!...!...557...558...559...560...561...562...!...!...!...563...564...565...566...567...568...569...570...571...572...573...574...575...576...577...578...579...580...581...582...583...584...585...586...587...588...589...590...591...592...593...594...595...596...597...598...599...600...601...602...603...604...605...606...607...608...609...610...611...612...613...614...615...616...617...618...619...620...621...622...623...624...625...626...627...628...629...630...631...632...633...634...635...636...637...638...639...640...641...642...643...644...!...!...!...!...645...!...!...!...!...646...647...648...!...!...!...649...650...651...652...653...654...655...656...657...658...659...660...661...662...663...664...665...666...667...668...669...670...671...672...673...674...675...676...677...678...679...680...681...682...683...684...685...686...687...688...689...690...691...692...693...694...695...696...697...698...699...!...700...701...702...703...704...705...706...707...708...709...710...711...712...713...714...715...716...717...!...!...!...!...718...719...720...721...722...723...724...725...726...727...728...729...730...731...732...733...734...735...736...!...!...737...738...739...740...741...742...743...744...745...746...747...!...!...748...!...!...749...!...!...750...751...752...753...754...755...756...757...758...759...760...761...762...763...764...765...766...767...768...769...770...771...772...773...774...775...776...777...778...779...780...781...782...783...784...785...786...787...788...789...790...791...792...793...794...795...796...797...798...799...800...801...802...803...804...805...806...807...808...809...810...811...812...813...814...815...816...817...818...819...820...821...822...823...824...825...826...827...828...829...830...!...831...832...833...834...835...836...837...838...839...840...841...842...843...!...!...844...845...846...!...!...847...848...!...!...849...850...851...852...853...854...855...856...857...!...!...!...!...858...859...860...861...862...863...864...865...866...867...868...869...870...871...872...873...874...875...876...877...878...879...880...881...882...883...884...885...886...887...888...889...890...891...892...893...894...895...896...897...898...899...900...901...902...903...904...905...906...907...908...909...910...911..."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# 指定要遍历的目录\n",
    "directory_path = 'output/hf-eval-data-v4_2'\n",
    "output_path = 'output/hf-eval-data-v4_3'\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 使用 glob.glob 获取所有 .py 文件的列表\n",
    "python_files = glob.glob(f'{directory_path}/*.py')\n",
    "\n",
    "# 遍历文件列表\n",
    "for idx, file_path in enumerate(python_files):\n",
    "    print(idx, end=\"...\")\n",
    "\n",
    "    # 在这里可以对每个 .py 文件执行需要的操作\n",
    "    # 例如，读取文件内容、分析代码等\n",
    "    \n",
    "    file_content = \"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "        \n",
    "    # print(file_content)\n",
    "    # req line\n",
    "    pip_line_idx = -1\n",
    "    for idx, l in enumerate(file_content.split(\"\\n\")):\n",
    "        if \"!pip install\" in l:\n",
    "            packages = l.split(\"!pip install -U \")[1].split(\" \")\n",
    "            new_packages = []\n",
    "            \n",
    "            for p in packages:\n",
    "                if \"==\" in p:\n",
    "                    p = p.split(\"==\")[0]\n",
    "                    print(\"!\",end='...')\n",
    "                new_packages.append(p)\n",
    "                \n",
    "            packages_json = json.dumps(new_packages)\n",
    "            \n",
    "            prog = \\\n",
    "f\"\"\"\n",
    "import subprocess\n",
    "\n",
    "requirements = {packages_json}\n",
    "\n",
    "for package in requirements:\n",
    "    subprocess.run(['pip', 'install', '-U', package])\n",
    "\"\"\"\n",
    "            \n",
    "            # print(prog)\n",
    "            pip_line_idx = idx\n",
    "            break\n",
    "            \n",
    "    if pip_line_idx != -1:\n",
    "        content_lines = file_content.split(\"\\n\")\n",
    "        final_prog = \"\\n\".join(content_lines[:pip_line_idx]) + prog + \"\\n\".join(content_lines[pip_line_idx+1:])\n",
    "        # print(final_prog)\n",
    "        \n",
    "        # print(output_path + \"/\" + file_path.split(\"/\")[-1])\n",
    "        with open(output_path + \"/\" + file_path.split(\"/\")[-1], 'w') as f:\n",
    "            f.write(final_prog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cf9a3-ce9b-448f-a0a6-678c736397b2",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "979ec2e2-17ad-4865-9e6e-f6359a5b0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...40...41...42...43...44...45...46...47...48..."
     ]
    }
   ],
   "source": [
    "# output 中的结果，检查一下有没有 Test Finished 的\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "output_dir = \"output/hf-eval-data-v4-valid-result/codellama-13b-python-eval/\"\n",
    "\n",
    "results = []\n",
    "valid_paths = []\n",
    "\n",
    "python_files = glob.glob(f'{output_dir}/*.py')\n",
    "\n",
    "# 遍历文件列表\n",
    "for idx, file_path in enumerate(python_files):\n",
    "    print(idx, end=\"...\")\n",
    "    # print(file_path)\n",
    "    \n",
    "    err_file = file_path.split(\".\")[0] + \".err\"\n",
    "    \n",
    "    out_file = file_path.split(\".\")[0] + \".out\"\n",
    "    \n",
    "    stats = [\"\", [], \"\"]\n",
    "    \n",
    "    try:\n",
    "        with open(out_file) as f:\n",
    "            for l in f:\n",
    "                l = l.strip()\n",
    "\n",
    "                if \"Testing started\" in l:\n",
    "                    stats[0] = l\n",
    "                if \"Testing case\" in l:\n",
    "                    stats[1].append(l)\n",
    "                if \"Testing finish\" in l:\n",
    "                    stats[2] = l\n",
    "                    valid_paths.append(file_path)\n",
    "    \n",
    "        # print(stats)\n",
    "\n",
    "        results.append(stats)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d64b958-60ea-4a3c-a6e3-de794ba3c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.6530612244897959\n",
      "26 0.5306122448979592\n",
      "2 0.04081632653061224\n",
      "2 0.04081632653061224\n",
      "8 0.16326530612244897\n"
     ]
    }
   ],
   "source": [
    "start_cnt = 0\n",
    "case1_start = 0\n",
    "case2_start = 0\n",
    "case3_start = 0\n",
    "finish_cnt = 0\n",
    "\n",
    "for start, cases, finish in results:\n",
    "    if start != '':\n",
    "        start_cnt += 1\n",
    "    if len(cases) == 1:\n",
    "        case1_start += 1\n",
    "    if len(cases) == 2:\n",
    "        case2_start += 1\n",
    "    if len(cases) == 3:\n",
    "        case3_start += 1\n",
    "    if finish != '':\n",
    "        finish_cnt += 1\n",
    "\n",
    "print(start_cnt, start_cnt / len(results))\n",
    "print(case1_start, case1_start / len(results))\n",
    "print(case2_start, case2_start / len(results))\n",
    "print(case3_start, case3_start / len(results))\n",
    "print(finish_cnt, finish_cnt / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ae3058-115e-42fa-8831-b5799def5a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee8956b-b31f-4b78-bb54-018f28cbcf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid 输出到一个文件夹里\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "target_dir = \"output/hf-eval-data-v4-valid\"\n",
    "try:\n",
    "    os.mkdir(target_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for v in valid_paths:\n",
    "    shutil.copy(v, target_dir + \"/\" + v.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcce28-3979-4625-9286-9690950e890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a2af5-6048-473f-8d62-55797783790a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
