{'code': "###Instruction: We are building a location recommendation system that identifies possible locations for new stores based on images from potential locations. Use the StreetCLIP model to generate probabilities for various cities.\n###Output: <<<domain>>>: Multimodal Zero-Shot Image Classification\n<<<api_call>>>: CLIPModel.from_pretrained('geolocal/StreetCLIP')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the required libraries, which include the Image class from PIL, requests to download images, and the CLIPProcessor and CLIPModel from transformers.\n2. Load the pretrained 'geolocal/StreetCLIP' model using the CLIPModel.from_pretrained() method.\n3. Instantiate a processor with the same pretrained 'geolocal/StreetCLIP' model using the CLIPProcessor.from_pretrained() method.\n4. Assuming you have images of potential store locations, provide a list of city options to classify images.\n5. Process the text (city options) and images using the instantiated processor, which prepares the inputs to pass through the loaded model.\n6. Compute the logits and probabilities for each city option, and use those probabilities to determine which cities are most suitable for opening new stores.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained('geolocal/StreetCLIP')\nprocessor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\nimage_url = 'https://example.com/potential_location_image.jpg'\nimage = Image.open(requests.get(image_url, stream=True).raw)\nchoices = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\ninputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\n", 'api_call': "CLIPModel.from_pretrained('geolocal/StreetCLIP')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Geolocalization', 'api_name': 'geolocal/StreetCLIP', 'api_call': "CLIPModel.from_pretrained('geolocal/StreetCLIP')", 'api_arguments': {'pretrained_model_name_or_path': 'geolocal/StreetCLIP'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': [{'name': 'IM2GPS', 'accuracy': {'25km': 28.3, '200km': 45.1, '750km': 74.7, '2500km': 88.2}}, {'name': 'IM2GPS3K', 'accuracy': {'25km': 22.4, '200km': 37.4, '750km': 61.3, '2500km': 80.4}}]}, 'description': 'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.'}}

