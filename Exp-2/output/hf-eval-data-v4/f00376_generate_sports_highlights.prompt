{'code': "###Instruction: Create a sports highlight generator that takes video clips as input and identifies the category of the sports activity happening in the video.\n###Output: <<<domain>>>: Computer Vision Video Classification\n<<<api_call>>>: VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: To create a sports highlight generator, follow these steps:\n1. Import the necessary libraries, which are 'VideoMAEImageProcessor' and 'VideoMAEForVideoClassification' from transformers, 'numpy', and 'torch'.\n2. Load the 'MCG-NJU/videomae-small-finetuned-kinetics' model, which is trained on the Kinetics-400 dataset to classify video clips into various categories.\n3. Create a video input as a list of numpy arrays, where each array represents a frame in the video sequence.\n4. Process the video input using the 'VideoMAEImageProcessor', which takes the input video and converts it into a format suitable for the model.\n5. Pass the processed input to the 'VideoMAEForVideoClassification' model and obtain the output logits.\n6. Find the index of the maximum class logits and use the model's configuration to obtain the predicted class label.\n7. Use the predicted class to generate sports highlights for the video clips.\n\n<<<code>>>: from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\n\n# video should be a list of numpy arrays representing video frames\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\n\ninputs = processor(video, return_tensors='pt')\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])", 'api_call': "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'videomae-small-finetuned-kinetics', 'api_call': "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')", 'api_arguments': {'video': 'list(np.random.randn(16, 3, 224, 224))'}, 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\ninputs = processor(video, return_tensors='pt')\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])", 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 79.0, 'top-5': 93.8}}, 'description': 'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}}

