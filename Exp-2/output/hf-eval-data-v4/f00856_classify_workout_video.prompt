{'code': "###Instruction: Our customer is a fitness platform. We need to analyze workout videos for offering customized workout plans.\n###Output: <<<domain>>>: Computer Vision Video Classification\n<<<api_call>>>: VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary classes from the transformers package, such as VideoMAEImageProcessor and VideoMAEForPreTraining.\n2. Use the from_pretrained method of the VideoMAEImageProcessor and VideoMAEForPreTraining classes to load the pre-trained model 'MCG-NJU/videomae-base-short'. This model has been designed for video analysis tasks, which is exactly what we need for analyzing workout videos.\n3. Prepare the workout video by loading it and converting it into a sequence of frames suitable for the model. The frames should be resized to 224x224 pixels and fed to the model as a list of numpy arrays.\n4. Use the model to perform video classification by feeding the processed video frames to the model. The output will contain features that can help the fitness platform in offering customized workout plans to its users.\n<<<code>>>: from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\n\nnum_frames = 16\n# Load workout video frames here\nvideo = list(np.random.randn(16, 3, 224, 224)) # Replace this with actual frames from the workout video\n\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short')\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\n\npixel_values = processor(images=video, return_tensors='pt').pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n\n# Use the outputs for providing customized workout plans.\n", 'api_call': "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'MCG-NJU/videomae-base-short', 'api_call': "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')", 'api_arguments': {'pretrained_model_name_or_path': 'MCG-NJU/videomae-base-short'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.'}}

