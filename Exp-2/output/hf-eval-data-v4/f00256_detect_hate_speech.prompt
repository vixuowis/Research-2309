{'code': '###Instruction: Build a system for detecting hate speech from social media comments in Korean.\n###Output: <<<domain>>>: Natural Language Processing Feature Extraction\n<<<api_call>>>: BartModel.from_pretrained(\'gogamza/kobart-base-v2\')\n<<<api_provider>>>: PyTorch Transformers\n<<<explanation>>>: 1. Import the necessary classes from the transformers library, such as PreTrainedTokenizerFast for tokenization and BartModel for creating the model.\n2. Load the KoBART-based tokenizer with the \'gogamza/kobart-base-v2\' model name, which is specifically designed for processing Korean text.\n3. Load the pre-trained KoBART model to build the system for detecting hate speech.\n4. Tokenize the input text (Korean comments) and pass the tokens to the model to obtain the features associated with the text.\n5. Combine these extracted features with machine learning techniques (such as logistic regression or support vector machines) to create a classifier for detecting hate speech.\n<<<code>>>: from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\'gogamza/kobart-base-v2\')\nmodel = BartModel.from_pretrained(\'gogamza/kobart-base-v2\')\n\ninput_text = "한국어 텍스트"\ntokens = tokenizer(input_text, return_tensors="pt")\nfeatures = model(**tokens)[\'last_hidden_state\']\n', 'api_call': "BartModel.from_pretrained('gogamza/kobart-base-v2')", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': "BartModel.from_pretrained('gogamza/kobart-base-v2')", 'api_arguments': {'tokenizer': "PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': "from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.'}}

