{'code': "###Instruction: A company developing an application for transcribing customer service calls requires a model that can understand spoken language.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary classes from the transformers library, including the WhisperProcessor and WhisperForConditionalGeneration.\n2. Load the pre-trained 'openai/whisper-medium' model using the from_pretrained method of the WhisperForConditionalGeneration class.\n3. Use the WhisperProcessor to preprocess the audio sample, converting it to input features that the model can handle.\n4. Pass the input features to the model to generate a predicted transcription.\n5. Decode the prediction back to human-readable text using the batch_decode method of the WhisperProcessor.\n6. This process can be applied to customer service call recordings, allowing the app to create transcriptions for further analysis or documentation.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-medium')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\n\nsample_audio_file = 'audio_file_path.wav'\n# Replace 'audio_file_path.wav' with the path to your audio file\nsample = {'array': lib_cap_path, 'sampling_rate': 16000}\ninput_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-medium', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')", 'api_arguments': ['sample', 'sampling_rate', 'language', 'task', 'skip_special_tokens'], 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 2.9}, {'name': 'LibriSpeech (other)', 'accuracy': 5.9}, {'name': 'Common Voice 11.0', 'accuracy': 53.87}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.'}}

