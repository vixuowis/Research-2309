{'code': '###Instruction: I need a tool to analyze a chart image and generate a summary of the information contained within the chart.\n###Output: <<<domain>>>: Multimodal Image-to-Text\n<<<api_call>>>: Pix2StructForConditionalGeneration.from_pretrained(\'google/deplot\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We import the required classes Pix2StructForConditionalGeneration and Pix2StructProcessor from the transformers library and PIL.Image as well as requests for loading images.\n2. We load the \'google/deplot\' model using Pix2StructForConditionalGeneration.from_pretrained() and the processor from Pix2StructProcessor.from_pretrained().\n3. Then, open the chart image using the PIL library and the image URL.\n4. The processor is used to prepare the inputs for the model by providing the chart image and a text prompt \'Generate underlying data table of the figure below:\'.\n5. Call the model\'s generate function with the prepared inputs and the maximum number of new tokens allowed.\n6. The model will then generate a summary of the image (a linearized table) which can be decoded using the processor.\n<<<code>>>: from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nimport requests\nfrom PIL import Image\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\'google/deplot\')\nprocessor = Pix2StructProcessor.from_pretrained(\'google/deplot\')\nurl = "https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, text="Generate underlying data table of the figure below:", return_tensors="pt")\npredictions = model.generate(**inputs, max_new_tokens=512)\nsummary = processor.decode(predictions[0], skip_special_tokens=True)\n', 'api_call': "Pix2StructForConditionalGeneration.from_pretrained('google/deplot')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': "Pix2StructForConditionalGeneration.from_pretrained('google/deplot')", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nimport requests\nfrom PIL import Image\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.'}}

