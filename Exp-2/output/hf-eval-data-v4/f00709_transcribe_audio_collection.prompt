{'code': "###Instruction: I have a collection of audio recordings from different sources, and I want to convert them into text to make an archive.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. The first step should be to import the necessary libraries, the WhisperProcessor, and the WhisperForConditionalGeneration model from the transformers library.\n2. Next, load the WhisperProcessor and the WhisperForConditionalGeneration model using the from_pretrained method.\n3. Iterate through your audio samples and process each audio sample with the WhisperProcessor to create the input features.\n4. For each input feature, use the WhisperForConditionalGeneration model to generate the predicted token IDs.\n5. Decode these token IDs using the processor and obtain the transcriptions. This will give you the transcribed text for each audio sample in your collection.\n6. Save these transcriptions in an organized manner for your archive.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n\ntranscriptions = []\nfor audio_sample in audio_samples:\n    input_features = processor(audio_sample['array'], sampling_rate=audio_sample['sampling_rate'], return_tensors='pt').input_features\n    predicted_ids = model.generate(input_features)\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    transcriptions.append(transcription)\n\narchive = create_archive(transcriptions)", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-small', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')", 'api_arguments': {'language': 'english', 'task': 'transcribe'}, 'python_environment_requirements': {'transformers': 'latest', 'datasets': 'latest'}, 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-small)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)', 'model.config.forced_decoder_ids = None', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'print(transcription)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '3.432 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.'}}

