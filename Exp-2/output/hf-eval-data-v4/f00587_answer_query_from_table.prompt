{'code': '###Instruction: Develop a table-based question answering system that can answer queries based on the input data.\n###Output: <<<domain>>>: Natural Language Processing Table Question Answering\n<<<api_call>>>: BartForConditionalGeneration.from_pretrained(\'microsoft/tapex-base-finetuned-wtq\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries, which include TapexTokenizer and BartForConditionalGeneration from transformers, and pandas to handle tables.\n2. Use the tokenizer and model classes\' from_pretrained() method to load the tokenizer and model, which in this case are \'TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\' and \'BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\'.\n3. Prepare the table using pandas and convert the table to a pandas DataFrame using the .from_dict() method.\n4. Provide the query for which the answer needs to be found from the table.\n5. Tokenize the input (table and query) using the loaded tokenizer and retrieve a tensor.\n6. Generate the output by passing the tensor as input to the model.\n7. Decode the output tensor to get the answer in a human-readable format.\n<<<code>>>: from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(\'microsoft/tapex-base-finetuned-wtq\')\nmodel = BartForConditionalGeneration.from_pretrained(\'microsoft/tapex-base-finetuned-wtq\')\ndata = {\n    \'year\': [1896, 1900, 1904, 2004, 2008, 2012],\n    \'city\': [\'athens\', \'paris\', \'st. louis\', \'athens\', \'beijing\', \'london\']\n}\ntable = pd.DataFrame.from_dict(data)\nquery = "In which year did beijing host the Olympic Games?"\nencoding = tokenizer(table=table, query=query, return_tensors=\'pt\')\noutputs = model.generate(**encoding)\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(answer)', 'api_call': "BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/tapex-base-finetuned-wtq', 'api_call': "BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')", 'api_arguments': {'tokenizer': 'TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)', 'model': 'BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)', 'table': 'pd.DataFrame.from_dict(data)', 'query': 'query'}, 'python_environment_requirements': ['transformers', 'pandas'], 'example_code': 'from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikitablequestions', 'accuracy': 'Not provided'}, 'description': 'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.'}}

