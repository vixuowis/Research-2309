{'code': '###Instruction: They are planning a trip to Germany and want to spend some leisure time in the parks of Munich, find out how to ask a question about the location of parks in Munich in German.\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: T5ForConditionalGeneration.from_pretrained(\'google/flan-t5-large\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary classes, T5Tokenizer and T5ForConditionalGeneration, from the transformers library.\n2. Load the \'google/flan-t5-large\' model and tokenizer using the from_pretrained method.\n3. Create an input text specifying the task: translating a question from English to German about the location of parks in Munich.\n4. Tokenize the input text using the T5Tokenizer.\n5. Generate the German translation using the T5ForConditionalGeneration model.\n6. Decode the generated output to obtain the translated question in German.\n<<<code>>>: from transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\'google/flan-t5-large\')\nmodel = T5ForConditionalGeneration.from_pretrained(\'google/flan-t5-large\')\ninput_text = "translate English to German: Where are the parks in Munich?"\ninput_ids = tokenizer(input_text, return_tensors=\'pt\').input_ids\noutputs = model.generate(input_ids)\ntranslated_question = tokenizer.decode(outputs[0])\n', 'api_call': "T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Language model', 'api_name': 'google/flan-t5-large', 'api_call': "T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')", 'api_arguments': ['input_text', 'input_ids', 'outputs'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-large)\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-large)\ninput_text = translate English to German: How old are you?\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research.'}}

