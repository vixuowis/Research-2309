{'code': "###Instruction: I have a collection of audio recordings from interviews that I need transcribed into text. How can I achieve this using automatic speech recognition?\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the relevant classes from the transformers package; this includes WhisperProcessor for handling audio data and WhisperForConditionalGeneration for the ASR model.\n2. Load the pre-trained model 'openai/whisper-tiny.en' using WhisperForConditionalGeneration.\n3. Load the audio recordings one by one, convert them to a compatible format for the model using WhisperProcessor, and then pass them to the model for transcription.\n4. Use the batch_decode method to convert the model's output into human-readable text transcripts.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-tiny.en')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\n\n# Load your audio files here\n# audio_data is a placeholder, replace it with your audio data variable\ninput_features = processor(audio_data, sampling_rate=16000, return_tensors='pt').input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription', 'api_name': 'openai/whisper-tiny.en', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')", 'api_arguments': {'model_name': 'openai/whisper-tiny.en'}, 'python_environment_requirements': ['transformers', 'datasets', 'torch'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 8.437}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.'}}

