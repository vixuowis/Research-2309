{'code': "###Instruction: I need to classify images of animals into their specific categories.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Import the necessary libraries, which are 'pipeline' from transformers.\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'.\n3. The loaded model will be used for zero-shot image classification, which is a task where the model makes a prediction for an image that might not match any of the examples in the training data.\n4. The model will infer the category of the animal based on the image provided to it.\n<<<code>>>: from transformers import pipeline\nclassifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\nimage_path = 'path/to/image.jpg'\ncategories = ['cat', 'dog', 'bird', 'fish']\nresult = classifier(image_path, categories)\n", 'api_call': "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft', 'api_call': "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')", 'api_arguments': {'image_path': 'Path to the image file', 'class_names': 'List of comma-separated class names'}, 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '75.9-76.9%'}, 'description': 'A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.'}}

