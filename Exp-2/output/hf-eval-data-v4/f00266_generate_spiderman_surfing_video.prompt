{'code': '###Instruction: A company is having an ad campaign with a superhero theme. They want to generate a video of Spiderman surfing to showcase as a social media ad.\n###Output: <<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b-legacy\', torch_dtype=torch.float16)\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Import the required modules and functions, including torch, DiffusionPipeline, DPMSolverMultistepScheduler, and export_to_video.\n2. Load the pre-trained text-to-video model \'damo-vilab/text-to-video-ms-1.7b-legacy\' using the from_pretrained method of DiffusionPipeline.\n3. Set the scheduler of the diffusion model to DPMSolverMultistepScheduler and offload the model to CPU with pipe.enable_model_cpu_offload().\n4. Provide the prompt "Spiderman is surfing" to initiate the video generation process.\n5. Pass the prompt and num_inference_steps=25 to the pipeline and generate the video frames.\n6. Export the frames to a video file using the export_to_video function.\n<<<code>>>: import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\npipe = DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b-legacy\', torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = "Spiderman is surfing"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)\n', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy', torch_dtype=torch.float16)", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b-legacy', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy', torch_dtype=torch.float16)", 'api_arguments': ['prompt', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': 'import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': ['LAION5B', 'ImageNet', 'Webvid'], 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.'}}

