{'code': '###Instruction: We are building an app to improve people\'s public speaking skills. We want to analyze their emotional speech as feedback.\n###Output: <<<domain>>>: Audio Audio Classification\n<<<api_call>>>: Wav2Vec2ForCTC.from_pretrained(\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary classes from the transformers package, including Wav2Vec2ForCTC and Wav2Vec2Processor.\n2. Load the pre-trained model \'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\' using the Wav2Vec2ForCTC.from_pretrained method. The model is fine-tuned for speech emotion recognition (SER) tasks, making it suitable for our purpose of analyzing the emotional speech of public speaking practice sessions.\n3. Use the Wav2Vec2Processor to convert the audio files to InputFeatures.\n4. Pass the processed inputs into the model, retrieve the top K emotion probabilities, and assign their labels.\n5. The detected emotions in speech can then be provided as feedback to users on their public speaking skills.\n<<<code>>>: from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nmodel = Wav2Vec2ForCTC.from_pretrained(\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\')\nemotions = [\'angry\', \'calm\', \'disgust\', \'fearful\', \'happy\', \'neutral\', \'sad\', \'surprised\']\n# Load and preprocess audio files with Wav2Vec2Processor\n# Assume audio_files list contains the path of the recorded audio files\nser_outputs = []\nfor audio in audio_files:\n    input_features = processor(audio, return_tensors="pt", padding=True, sampling_rate=16000)\n    logits = model(**input_features).logits\n    predicted_emotion = torch.argmax(logits, dim=-1).item()\n    ser_outputs.append(emotions[predicted_emotion])\n', 'api_call': "Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Emotion Recognition', 'api_name': 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', 'api_call': "Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')", 'api_arguments': 'wav2vec2, tokenizer', 'python_environment_requirements': 'transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3', 'example_code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor', 'performance': {'dataset': 'RAVDESS', 'accuracy': 0.8223}, 'description': "The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']."}}

