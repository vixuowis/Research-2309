{'code': "###Instruction: In surveillance operations, transcripts are used to turn the audio feed into chunks such that after processing transcripts there are no speakers talking over each other in the output.\n###Output: <<<domain>>>: Audio Voice Activity Detection\n<<<api_call>>>: Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\n<<<api_provider>>>: pyannote.audio\n<<<explanation>>>: 1. Import the Pipeline class from the pyannote.audio library.\n2. Use the Pipeline.from_pretrained() function to create a pretrained speaker diarization model, specifying the model name as 'pyannote/speaker-diarization@2.1' and an access token.\n3. This model will separate speakers in an audio recording, ensuring that speakers are not overlapping in the output.\n4. The pipeline can be applied to an audio file to perform speaker diarization, wherein the output will return speaker labels and time segments for grouped speech.\n5. Once processed, the output can be saved in an RTTM (Rich Text Time-Marked) format, facilitating further analysis or transcript synchronization.\n<<<code>>>: from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\ndiarization = pipeline('audio_file.wav')\nwith open('audio_file.rttm', 'w') as rttm:\n    diarization.write_rttm(rttm)\n", 'api_call': "Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Speaker diarization', 'api_name': 'johnislarry/cloned-pyannote-speaker-diarization-endpoint', 'api_call': "Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': 'pyannote.audio 2.0', 'example_code': {'load_pipeline': 'from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'apply_pipeline': 'diarization = pipeline(audio.wav)', 'save_output': 'with open(audio.rttm, w) as rttm:\n  diarization.write_rttm(rttm)'}, 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.'}}

