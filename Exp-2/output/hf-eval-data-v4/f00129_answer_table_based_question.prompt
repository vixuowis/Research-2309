{'code': '###Instruction: As a researcher, I am trying to find an answer to my question in a table containing information about animals and their characteristics.\n###Output: <<<domain>>>: Natural Language Processing Table Question Answering\n<<<api_call>>>: AutoModelForSeq2SeqLM.from_pretrained(\'neulab/omnitab-large-finetuned-wtq\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries, including \'AutoTokenizer\', \'AutoModelForSeq2SeqLM\', and \'pd\' from the relevant packages.\n2. Instantiate the tokenizer and model using the \'neulab/omnitab-large-finetuned-wtq\' checkpoint.\n3. Define the table data in a Pandas DataFrame with information about animals and their characteristics. For example, the table might include columns for \'Animal\', \'Habitat\', and \'Average Lifespan\'.\n4. Specify the question to be answered, which relates to the information present in the table.\n5. Use the tokenizer to create an encoding of the table and the query.\n6. Use the model to generate an output based on the encoding.\n7. Decode the output to get the final answer to the question.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(\'neulab/omnitab-large-finetuned-wtq\')\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\'neulab/omnitab-large-finetuned-wtq\')\ndata = {\n    \'Animal\': [\'Tiger\', \'Lion\', \'Giraffe\', \'Elephant\'],\n    \'Habitat\': [\'Forest\', \'Grassland\', \'Savanna\', \'Savanna\'],\n    \'Average Lifespan\': [10, 12, 25, 50],\n}\ntable = pd.DataFrame.from_dict(data)\nquery = "What is the average lifespan of a giraffe?"\nencoding = tokenizer(table=table, query=query, return_tensors=\'pt\')\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'neulab/omnitab-large-finetuned-wtq', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq')", 'api_arguments': {'table': 'pd.DataFrame', 'query': 'str'}, 'python_environment_requirements': {'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM', 'pandas': 'pd'}, 'example_code': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikitablequestions', 'accuracy': None}, 'description': 'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.'}}

