# requirements_file --------------------

!pip install -U transformers

# function_import --------------------

from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer

# function_code --------------------

def chatbot_response(question_text):
    """
    Takes a question as input and uses a pre-trained Blenderbot chatbot to generate a response.

    Parameters:
        question_text (str): A question string for which the chatbot generates an answer.

    Returns:
        str: The response generated by the chatbot.
    """
    model = BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
    tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot_small-90M')
    inputs = tokenizer(question_text, return_tensors='pt')
    outputs = model.generate(**inputs)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# test_function_code --------------------

def test_chatbot_response():
    print("Testing chatbot_response function.")
    sample_questions = [
        "What are the admission requirements?",
        "What extracurricular activities are offered?",
        "Can you tell me about the school's academic programs?"
    ]

    for i, question in enumerate(sample_questions):
        print(f"Testing case [{i+1}/{len(sample_questions)}] started.")
        response = chatbot_response(question)
        assert isinstance(response, str), f"Test case [{i+1}/{len(sample_questions)}] failed: response is not a string"
        print(f"Test case [{i+1}/{len(sample_questions)}] passed.")

    print("Testing finished.")

# Run the test function
test_chatbot_response()