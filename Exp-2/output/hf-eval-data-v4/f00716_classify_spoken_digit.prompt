{'code': "###Instruction: A kid's educational toy company is including a voice assistant that recognizes simple voice commands from children. They need an example on how to recognize spoken digits.\n###Output: <<<domain>>>: Audio Audio Classification\n<<<api_call>>>: pipeline('audio-classification', model='MIT/ast-finetuned-speech-commands-v2')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the required pipeline function from the transformers library provided by Hugging Face.\n2. Define the classifier by specifying the 'audio-classification' pipeline and the pre-trained 'MIT/ast-finetuned-speech-commands-v2' model.\n3. The classifier can now be used to analyze spoken digit audio samples and classify them into appropriate digits.\n4. The provided audio files should be in the expected format for the audio classification pipeline to work efficiently.\n<<<code>>>: from transformers import pipeline\nspoken_digit_classifier = pipeline('audio-classification', model='MIT/ast-finetuned-speech-commands-v2')\ndigit_prediction = spoken_digit_classifier(audio_sample_path)\n", 'api_call': "pipeline('audio-classification', model= 'MIT/ast-finetuned-speech-commands-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mazkooleg/0-9up-ast-ft', 'api_call': "pipeline('audio-classification', model= 'MIT/ast-finetuned-speech-commands-v2')", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.26.1, Pytorch 1.11.0+cpu, Datasets 2.10.0, Tokenizers 0.12.1', 'example_code': '', 'performance': {'dataset': 'mazkooleg/0-9up_google_speech_commands_augmented_raw', 'accuracy': 0.9979}, 'description': 'This model is a fine-tuned version of MIT/ast-finetuned-speech-commands-v2 on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0210, Accuracy: 0.9979'}}

