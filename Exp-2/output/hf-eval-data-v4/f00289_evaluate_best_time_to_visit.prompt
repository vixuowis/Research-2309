{'code': '###Instruction: I am a traveler and I want to know the best time to visit a Chinese historical site. Take a look at an image and tell me whether it is a good time to visit the site or not.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: ChineseCLIPModel.from_pretrained(\'OFA-Sys/chinese-clip-vit-base-patch16\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the ChineseCLIPModel and ChineseCLIPProcessor from the transformers library provided by Hugging Face.\n2. Load the pretrained model \'OFA-Sys/chinese-clip-vit-base-patch16\' and processor.\n3. Process the input image using the processor, and extract features from the given image using the model.\n4. Classify the image as either "good time to visit" or "not a good time to visit" using the logits per image generated by the model.\n<<<code>>>: from PIL import Image\nfrom transformers import ChineseCLIPModel, ChineseCLIPProcessor\n\nmodel = ChineseCLIPModel.from_pretrained(\'OFA-Sys/chinese-clip-vit-base-patch16\')\nprocessor = ChineseCLIPProcessor.from_pretrained(\'OFA-Sys/chinese-clip-vit-base-patch16\')\n\nimage = Image.open("path_to_image.jpg")\ntexts = ["好的参观时间", "不是好的参观时间"]\n\ninputs = processor(images=image, text=texts, return_tensors=\'pt\')\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1).tolist()\nresult = dict(zip(texts, probs[0]))\n\ngood_time_to_visit = result[\'好的参观时间\'] > result[\'不是好的参观时间\']', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'OFA-Sys/chinese-clip-vit-base-patch16', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')", 'api_arguments': {'pretrained_model_name_or_path': 'OFA-Sys/chinese-clip-vit-base-patch16'}, 'python_environment_requirements': {'transformers': 'ChineseCLIPProcessor, ChineseCLIPModel'}, 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [, , , ]\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'MUGE Text-to-Image Retrieval': {'accuracy': {'Zero-shot R@1': 63.0, 'Zero-shot R@5': 84.1, 'Zero-shot R@10': 89.2, 'Finetune R@1': 68.9, 'Finetune R@5': 88.7, 'Finetune R@10': 93.1}}, 'Flickr30K-CN Retrieval': {'accuracy': {'Zero-shot Text-to-Image R@1': 71.2, 'Zero-shot Text-to-Image R@5': 91.4, 'Zero-shot Text-to-Image R@10': 95.5, 'Finetune Text-to-Image R@1': 83.8, 'Finetune Text-to-Image R@5': 96.9, 'Finetune Text-to-Image R@10': 98.6, 'Zero-shot Image-to-Text R@1': 81.6, 'Zero-shot Image-to-Text R@5': 97.5, 'Zero-shot Image-to-Text R@10': 98.8, 'Finetune Image-to-Text R@1': 95.3, 'Finetune Image-to-Text R@5': 99.7, 'Finetune Image-to-Text R@10': 100.0}}, 'COCO-CN Retrieval': {'accuracy': {'Zero-shot Text-to-Image R@1': 69.2, 'Zero-shot Text-to-Image R@5': 89.9, 'Zero-shot Text-to-Image R@10': 96.1, 'Finetune Text-to-Image R@1': 81.5, 'Finetune Text-to-Image R@5': 96.9, 'Finetune Text-to-Image R@10': 99.1, 'Zero-shot Image-to-Text R@1': 63.0, 'Zero-shot Image-to-Text R@5': 86.6, 'Zero-shot Image-to-Text R@10': 92.9, 'Finetune Image-to-Text R@1': 83.5, 'Finetune Image-to-Text R@5': 97.3, 'Finetune Image-to-Text R@10': 99.2}}, 'Zero-shot Image Classification': {'accuracy': {'CIFAR10': 96.0, 'CIFAR100': 79.7, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}}}}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.'}}

