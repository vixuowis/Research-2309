{'code': '###Instruction: Generate a video for a cooking show intro with the text "Chef John\'s Culinary Adventures."\n###Output: <<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b\', torch_dtype=torch.float16, variant=fp16)\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>:1. Import the required libraries, including torch, DiffusionPipeline, DPMSolverMultistepScheduler, and export_to_video.\n2. Load the pre-trained model \'damo-vilab/text-to-video-ms-1.7b\' using the DiffusionPipeline class. Set the torch_dtype to torch.float16 and variant to fp16 for better performance.\n3. Set the multi-step scheduler using the DPMSolverMultistepScheduler.from_config() function.\n4. Enable the CPU offloading to save GPU memory during the process.\n5. Provide the prompt "Chef John\'s Culinary Adventures" and generate video frames using the model. Set the num_inference_steps to 25.\n6. Export the generated video frames to the desired video format using the export_to_video() function.\n<<<code>>>: import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b\', torch_dtype=torch.float16, variant=\'fp16\')\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = "Chef John\'s Culinary Adventures"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)\n', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b', torch_dtype=torch.float16, variant=fp16)", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b', torch_dtype=torch.float16, variant=fp16)", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.'}}

