{'code': '###Instruction: Can you initiate a conversation with an AI model that plays the role of a friend who just got back from a conference?\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: AutoModelForSeq2SeqLM.from_pretrained(\'allenai/cosmo-xl\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import necessary libraries such as torch (to utilize GPU if available), AutoTokenizer, and AutoModelForSeq2SeqLM from transformers package.\n2. Initialize the tokenizer and model by loading them from the pre-trained \'allenai/cosmo-xl\' model. This model is designed to generate conversational text based on given situation and role instructions.\n3. Create two helper functions, set_input() and generate(). The set_input() function processes the situation narrative, role instruction, and conversation history, while the generate() function uses the model to generate a response based on these inputs.\n4. Set a situation narrative, role instruction, and initial conversation text.\n5. Call the generate() function with these inputs to obtain an output response.\n<<<code>>>: import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\ntokenizer = AutoTokenizer.from_pretrained(\'allenai/cosmo-xl\')\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\'allenai/cosmo-xl\').to(device)\n\ndef set_input(situation_narrative, role_instruction, conversation_history):\n    input_text = " <turn> ".join(conversation_history)\n    if role_instruction != "":\n        input_text = "{} <sep> {}".format(role_instruction, input_text)\n    if situation_narrative != "":\n        input_text = "{} <sep> {}".format(situation_narrative, input_text)\n    return input_text\n\ndef generate(situation_narrative, role_instruction, conversation_history):\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors=\'pt\').to(device)\n    outputs = model.generate(inputs[\'input_ids\'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\nsituation = "Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi."\ninstruction = "You are Cosmo and you are talking to a friend."\nconversation = ["Hey, how was your trip to Abu Dhabi?"]\nresponse = generate(situation, instruction, conversation)\nprint(response)', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'allenai/cosmo-xl', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl')", 'api_arguments': {'pretrained_model_name_or_path': 'allenai/cosmo-xl'}, 'python_environment_requirements': {'torch': 'latest', 'transformers': 'latest'}, 'example_code': {'import': ['import torch', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'initialize': ['device = torch.device(cuda if torch.cuda.is_available() else cpu)', 'tokenizer = AutoTokenizer.from_pretrained(allenai/cosmo-xl)', 'model = AutoModelForSeq2SeqLM.from_pretrained(allenai/cosmo-xl).to(device)'], 'example': ['def set_input(situation_narrative, role_instruction, conversation_history):', ' input_text =  <turn> .join(conversation_history)', 'if role_instruction != :', " input_text = {} &lt;sep&gt; {}'.format(role_instruction, input_text)", 'if situation_narrative != :', " input_text = {} &lt;sep&gt; {}'.format(situation_narrative, input_text)", 'return input_text', 'def generate(situation_narrative, role_instruction, conversation_history):', ' input_text = set_input(situation_narrative, role_instruction, conversation_history)', ' inputs = tokenizer([input_text], return_tensors=pt).to(device)', ' outputs = model.generate(inputs[input_ids], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)', ' response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)', ' return response', 'situation = Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.', 'instruction = You are Cosmo and you are talking to a friend.', 'conversation = [', ' Hey, how was your trip to Abu Dhabi?', ']', 'response = generate(situation, instruction, conversation)', 'print(response)']}, 'performance': {'dataset': {'allenai/soda': '', 'allenai/prosocial-dialog': ''}, 'accuracy': ''}, 'description': 'COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation.'}}

