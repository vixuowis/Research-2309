{'code': '###Instruction: I need a function to find the most relevent passage given a question and several candidate passages.\n###Output: <<<domain>>>: Natural Language Processing Text Classification\n<<<api_call>>>: AutoModelForSequenceClassification.from_pretrained(\'cross-encoder/ms-marco-MiniLM-L-6-v2\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries: AutoTokenizer, and AutoModelForSequenceClassification from transformers and torch.\n2. Load the pre-trained model cross-encoder/ms-marco-MiniLM-L-6-v2 using the AutoModelForSequenceClassification.from_pretrained function.\n3. Load the pre-trained tokenizer using the AutoTokenizer.from_pretrained function.\n4. Tokenize the given question and candidate passages with the tokenizer. The tokenizer will return input features that are compatible with the model.\n5. Evaluate the model with the tokenized features. The model outputs a logits score for each passage, which represents the relevance of each passage to the given question.\n6. Sort the passages in descending order based on the logits score.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained(\'cross-encoder/ms-marco-MiniLM-L-6-v2\')\ntokenizer = AutoTokenizer.from_pretrained(\'cross-encoder/ms-marco-MiniLM-L-6-v2\')\nquestion = "How many people live in Berlin?"\ncandidate_passages = ["Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.", "New York City is famous for the Metropolitan Museum of Art."]\nfeatures = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors=\'pt\')\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\nprint(sorted_passages[0])', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Information Retrieval', 'api_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')", 'api_arguments': {'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)", 'performance': {'dataset': 'MS Marco Passage Reranking', 'accuracy': 'MRR@10: 39.01%'}, 'description': 'This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order.'}}

