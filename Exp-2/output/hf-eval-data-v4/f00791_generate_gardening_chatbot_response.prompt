{'code': '###Instruction: I want to create a chatbot for discussing gardening topics and tips, which can refer to external information while generating responses.\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: AutoModelForSeq2SeqLM.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import the required classes from the transformers library including AutoTokenizer and AutoModelForSeq2SeqLM.\n2. We then initialize the tokenizer and model for \'microsoft/GODEL-v1_1-base-seq2seq\' using the from_pretrained method.\n3. We create a function named \'generate\' which takes the instruction, knowledge, and dialog as arguments. The instruction contains the user\'s input, knowledge represents relevant external information, and dialog is the previous dialog context.\n4. Format the input by combining the instruction, external knowledge, and dialog string with special tokens (like [CONTEXT], [KNOWLEDGE]) to separate different sections.\n5. We use the generate method of the model to obtain a suitable response based on the input query and the knowledge.\n6. Decode the response using the tokenizer and skip special tokens to get the final message.\n7. Return the generated output from the chatbot.\n- Note: In the actual conversation flow, you must organize the dialog, provide external knowledge, and input from users, and call the \'generate\' function to obtain the response.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\n\ndef generate(instruction, knowledge, dialog):\n    if knowledge != \'\':\n        knowledge = \'[KNOWLEDGE] \' + knowledge\n    dialog = \' EOS \'.join(dialog)\n    query = f"{instruction} [CONTEXT] {dialog} {knowledge}"\n    input_ids = tokenizer(query, return_tensors=\'pt\').input_ids\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return output\n', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/GODEL-v1_1-base-seq2seq', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'api_arguments': ['instruction', 'knowledge', 'dialog'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'N/A'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.'}}

