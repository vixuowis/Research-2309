{'code': "###Instruction: Our blog platform accepts voice notes, and we want to transcribe these notes to text.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the required classes from the transformers and datasets packages. This includes WhisperProcessor, WhisperForConditionalGeneration, and load_dataset.\n2. Load the pre-trained 'openai/whisper-large' model using the WhisperForConditionalGeneration.from_pretrained method. \n3. Also, load the processor for the 'openai/whisper-large' model using the WhisperProcessor.from_pretrained method.\n4. Load your voice note as an audio file and extract its sampling rate information.\n5. Use the processor to convert the audio input into input features suitable for the pre-trained model.\n6. Generate predicted IDs using the Whisper model by passing through the input features.\n7. Decode the predicted IDs into text transcription using the processor's batch_decode function.\n8. The transcribed text can now be used as content for your blog platform.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\nmodel.config.forced_decoder_ids = None\n\n##_ Load voice note and get sampling_rate here _##\n\ninput_features = processor(audio, sampling_rate=sampling_rate, return_tensors='pt').input_features\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Automatic Speech Recognition and Speech Translation', 'api_name': 'openai/whisper-large', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')", 'api_arguments': ['audio', 'sampling_rate'], 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-large)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-large)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\n\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 3.0}, {'name': 'LibriSpeech (other)', 'accuracy': 5.4}, {'name': 'Common Voice 11.0', 'accuracy': 54.8}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.'}}

