{'code': '###Instruction: Create a method to determine, as an entertainment recommendation system, which category a text message about a daily activity belongs to.\n###Output: <<<domain>>>: Natural Language Processing Zero-Shot Classification\n<<<api_call>>>: AutoModelForSequenceClassification.from_pretrained(\'facebook/bart-large-mnli\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We import the required classes from the transformers library provided by Hugging Face. This includes the AutoModelForSequenceClassification for the model and the AutoTokenizer for tokenizing text.\n2. We will load the pre-trained model \'facebook/bart-large-mnli\', which is trained for zero-shot text classification using the natural language inference (NLI) approach.\n3. Next, we tokenize the input sequence (text message) and construct hypotheses for each candidate category. In this case, we might have categories like \'travel\', \'cooking\', and \'dancing\'.\n4. Then, we pass the tokenized input to the model and obtain logits for the entailment and contradiction relationship between the input sequence and each hypothesis.\n5. We convert the logits to label probabilities, which indicate the likeliness of each category.\n<<<code>>>: from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ndef classify_text(sequence: str, candidate_labels: list):\n    nli_model = AutoModelForSequenceClassification.from_pretrained(\'facebook/bart-large-mnli\')\n    tokenizer = AutoTokenizer.from_pretrained(\'facebook/bart-large-mnli\')\n\n    probs_list = []\n    for label in candidate_labels:\n        hypothesis = f\'This example is {label}.\'\n        inputs = tokenizer(sequence, hypothesis, return_tensors=\'pt\', truncation=True)\n        logits = nli_model(**inputs)[0]\n        entail_contradiction_logits = logits[:, [0, 2]]\n        probs = entail_contradiction_logits.softmax(dim=1)\n        prob_label_is_true = probs[:, 1].item()\n        probs_list.append(prob_label_is_true)\n\n    category_index = probs_list.index(max(probs_list))\n    return candidate_labels[category_index]\n\ntext_message = "I spent hours in the kitchen trying a new recipe."\ncategories = [\'travel\', \'cooking\', \'dancing\']\nresult = classify_text(text_message, categories)', 'api_call': "AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'NLI-based Zero Shot Text Classification', 'api_name': 'facebook/bart-large-mnli', 'api_call': "AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')", 'api_arguments': {'sequence_to_classify': 'one day I will see the world', 'candidate_labels': "['travel', 'cooking', 'dancing']"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline'}, 'example_code': {'with_pipeline': "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\nsequence_to_classify = 'one day I will see the world'\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)", 'with_manual_pytorch': "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\npremise = sequence\nhypothesis = f'This example is {label}.'\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]"}, 'performance': {'dataset': 'multi_nli', 'accuracy': 'Not specified'}, 'description': 'This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities.'}}

