{'code': "###Instruction: Our company is cooperating with a German partner. We have some materials in English, but need to translate them into German.\n\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\n<<<api_provider>>>: Transformers\n<<<explanation>>>: 1. We first import the necessary classes from the transformers package. This includes MBartForConditionalGeneration for the multilingual sequence-to-sequence model and MBart50TokenizerFast for tokenization.\n2. We then use the from_pretrained method of the MBartForConditionalGeneration class to load the pre-trained model 'facebook/mbart-large-50'. This model has been trained with multilingual capabilities, which is perfect for translating between different languages.\n3. Set the source language (English) and target language (German) for the tokenizer.\n4. With the tokenizer and model prepared, we can then initiate the process of translation, providing the English text as input.\n5. The output will be the translated German text.\n<<<code>>>: from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50', src_lang='en_XX', tgt_lang='de_DE')\nsrc_text = 'Here is the English material to be translated...'\ntranslated_output = model.generate(**tokenizer(src_text, return_tensors='pt'))\ntgt_text = tokenizer.batch_decode(translated_output, skip_special_tokens=True)\n", 'api_call': "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Multilingual Sequence-to-Sequence', 'api_name': 'facebook/mbart-large-50', 'api_call': "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')", 'api_arguments': {'src_lang': 'en_XX', 'tgt_lang': 'ro_RO'}, 'python_environment_requirements': {'transformers': 'MBartForConditionalGeneration, MBart50TokenizerFast'}, 'example_code': "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50', src_lang='en_XX', tgt_lang='ro_RO')\nsrc_text = ' UN Chief Says There Is No Military Solution in Syria'\ntgt_text = 'Şeful ONU declară că nu există o soluţie militară în Siria'\nmodel_inputs = tokenizer(src_text, return_tensors='pt')\nwith tokenizer.as_target_tokenizer():\n labels = tokenizer(tgt_text, return_tensors='pt').input_ids\nmodel(**model_inputs, labels=labels)", 'performance': {'dataset': 'Multilingual Denoising Pretraining', 'accuracy': 'Not specified'}, 'description': "mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the 'Multilingual Denoising Pretraining' objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper."}}

