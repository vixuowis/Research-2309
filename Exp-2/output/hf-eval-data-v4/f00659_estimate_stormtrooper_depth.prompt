{'code': '###Instruction: In this Star Wars movie scene, I want to create a depth estimation for the stormtroopers.\n###Output: <<<domain>>>: Computer Vision Image-to-Image\n<<<api_call>>>: ControlNetModel.from_pretrained(\'lllyasviel/sd-controlnet-depth\')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Import the necessary libraries, such as diffusers, transformers, accelerate, PIL, numpy, and torch.\n2. Install the required packages using pip.\n3. Load the pretrained model \'lllyasviel/sd-controlnet-depth\' using ControlNetModel.from_pretrained() method.\n4. Create a pipeline using \'depth-estimation\' and the pretrained model.\n5. Load the image of the scene with the stormtroopers using the load_image() function from the diffusers library.\n6. Estimate the depth of the stormtroopers in the image by passing it through the depth_estimator pipeline.\n7. Save the depth-estimated image with the desired output file name.\n\n<<<code>>>: \nfrom transformers import pipeline\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom diffusers.utils import load_image\ndepth_estimator = pipeline(\'depth-estimation\')\nimage = load_image(\'https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\')\nimage = depth_estimator(image)[\'depth\']\nimage = np.array(image)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\ncontrolnet = ControlNetModel.from_pretrained(\'lllyasviel/sd-controlnet-depth\', torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\'runwayml/stable-diffusion-v1-5\', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\ndepth_output = pipe("Stormtrooper\'s lecture", image, num_inference_steps=20).images[0]\ndepth_output.save(\'./images/stormtrooper_depth_out.png\')\n', 'api_call': "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', "depth_estimator = pipeline('depth-estimation')", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', "image = depth_estimator(image)['depth']", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', "image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]", "image.save('./images/stormtrooper_depth_out.png')"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.'}}

