{'code': "###Instruction: Provide me a pipeline for extracting relevant information from an insurance policy document.\n###Output: <<<domain>>>: Multimodal Document Question Answer\n<<<api_call>>>: pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the pipeline function from the transformers library provided by Hugging Face.\n2. Initialize the Document-question-answering (DocVQA) pipeline with the 'jinhybr/OCR-DocVQA-Donut' model. This model is capable of extracting relevant information from an input image by jointly processing visual and textual information.\n3. Given an image of the insurance policy document, we can use the DocVQA model to ask specific questions and retrieve relevant information, for example, the policy number, coverage amount, beneficiary, and term period.\n<<<code>>>: from transformers import pipeline\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\n\n# Example questions\nquestions = ['What is the policy number?', 'What is the coverage amount?', 'Who is the beneficiary?', 'What is the term period?']\n\n# Extract information from the insurance policy document image\nanswers = {}\nfor question in questions:\n    result = doc_vqa(image_path='path/to/image.jpg', question=question)\n    answers[question] = result['answer']\n", 'api_call': "pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': "pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': "doc_vqa(image_path='path/to/image.jpg', question='What is the title?')", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.'}}

