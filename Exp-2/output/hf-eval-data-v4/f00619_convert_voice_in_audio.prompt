{'code': "###Instruction: I work at a call center, and I need to convert a recorded customer call into an audio file with a different voice without changing the content.\n\n###Output: <<<domain>>>: Audio Audio-to-Audio\n<<<api_call>>>: SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. First, we import the necessary classes and libraries, which include SpeechT5Processor, SpeechT5ForSpeechToSpeech, and SpeechT5HifiGan from the transformers package.\n2. We load the example speech from file and retrieve the sampling rate.\n3. We create instances of the SpeechT5Processor, SpeechT5ForSpeechToSpeech, and SpeechT5HifiGan by calling the from_pretrained method with the appropriate model names: 'microsoft/speecht5_vc', and 'microsoft/speecht5_hifigan'.\n4. We preprocess the input audio using SpeechT5Processor, resulting in input tensor representations.\n5. We load speaker embeddings for the desired target voice to be used during generation.\n6. We call the model's generate_speech method to produce the converted speech, specifying the input tensor, speaker_embeddings, and vocoder.\n7. Finally, we save the resulting speech as a .wav file.\n<<<code>>>: from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\nimport numpy as np\nimport soundfile as sf\nimport torch\n\nexample_speech, sampling_rate = sf.read('input_audio.wav')\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\n\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\n\nsf.write('converted_speech.wav', speech.numpy(), samplerate=16000)", 'api_call': "SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/speecht5_vc', 'api_call': "SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')", 'api_arguments': {'audio': 'example_speech', 'sampling_rate': 'sampling_rate', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'datasets', 'numpy', 'torch', 'soundfile'], 'example_code': "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\ndataset = dataset.sort('id')\nsampling_rate = dataset.features['audio'].sampling_rate\nexample_speech = dataset[0]['audio']['array']\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\nimport numpy as np\nimport torch\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\nimport soundfile as sf\nsf.write('speech.wav', speech.numpy(), samplerate=16000)", 'performance': {'dataset': 'CMU ARCTIC', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.'}}

