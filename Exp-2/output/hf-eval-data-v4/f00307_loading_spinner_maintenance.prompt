{'code': '###Instruction: We want to code a loading spinner to display when our system is undergoing maintenance. Can you generate the code for us?\n###Input: <noinput>\n###Output: <<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoTokenizer.from_pretrained(\'Salesforce/codegen-350M-multi\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import AutoTokenizer and AutoModelForCausalLM from transformers library.\n2. Load the pre-trained model \'Salesforce/codegen-350M-multi\' using AutoTokenizer and AutoModelForCausalLM to create tokenizer and model instances respectively.\n3. Utilize the generated model for program synthesis by given a textual prompt as input, which could be something like "Create a simple loading spinner for maintenance."\n4. Tokenize the input text using the tokenizer instance and feed the tokenized input into the model.\n5. Generate executable code based on the input prompt.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\'Salesforce/codegen-350M-multi\')\nmodel = AutoModelForCausalLM.from_pretrained(\'Salesforce/codegen-350M-multi\')\ntext = "Create a simple loading spinner for maintenance."\ninput_ids = tokenizer(text, return_tensors=\'pt\').input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'api_call': "AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': "AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')", 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\ntext = def hello_world():\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}}

