{'code': '###Instruction: Our marketing team wants a video commercial for our product. They are looking for a video with a couple sitting in a cafe and laughing while using our product.\n###Output: <<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b\', torch_dtype=torch.float16, variant=\'fp16\')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Import the necessary packages, including torch and DiffusionPipeline from the diffusers library.\n2. Load the pretrained model \'damo-vilab/text-to-video-ms-1.7b\' using the DiffusionPipeline class. Set torch_dtype as torch.float16 and variant as fp16 for faster performance.\n3. Create the prompt text ("A couple sitting in a cafe and laughing while using our product") for the video generation task.\n4. Use the pipeline to generate the video frames using the given prompt and a specified number of inference steps (e.g., 25).\n5. Export the video frames to a video file (e.g., \'output_video.mp4\').\n<<<code>>>: import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b\', torch_dtype=torch.float16, variant=\'fp16\')\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = "A couple sitting in a cafe and laughing while using our product"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames, \'output_video.mp4\')\n', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b', torch_dtype=torch.float16, variant=fp16)", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b', torch_dtype=torch.float16, variant=fp16)", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.'}}

