{'code': "###Instruction: A new project demands to classify videos for a social media platform. Let us create a video classification pipeline.\n###Output: <<<domain>>>: Computer Vision Video Classification\n<<<api_call>>>: VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the required library, like VideoMAEImageProcessor and VideoMAEForPreTraining from the transformers library provided by Hugging Face.\n2. Use the from_pretrained method to load the 'MCG-NJU/videomae-base' model and create the video classification model called VideoMAEForPreTraining.\n3. Also, create a preprocessor using the VideoMAEImageProcessor with the same model name, which will preprocess video frames to be compatible with the model.\n4. Preprocess the video frames using the preprocessor and extract pixel values.\n5. Pass the preprocessed frames as input to the model to get the video classification results.\n<<<code>>>: from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base')\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\npixel_values = processor(video, return_tensors='pt').pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss\n", 'api_call': "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'MCG-NJU/videomae-base', 'api_call': "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')", 'api_arguments': ['video'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'To be provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.'}}

