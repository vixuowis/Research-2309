{'code': "###Instruction: My lecture has been recorded, and I would like to transcribe the audio to create a transcript of my presentation.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary classes from the transformers library provided by Hugging Face. This includes WhisperProcessor and WhisperForConditionalGeneration.\n2. Initialize the WhisperProcessor and the WhisperForConditionalGeneration model using the 'openai/whisper-large-v2' pretrained model for automatic speech recognition.\n3. Process your lecture's audio file with the processor to generate input features and make sure to pass the appropriate sampling rate of the audio file.\n4. Use the Whisper ASR model to generate the predicted_ids from the input_features.\n5. Decode the predicted_ids using the processor to obtain the transcription of your lecture.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\ninput_features = processor(audio_data, sampling_rate=audio_sampling_rate, return_tensors='pt').input_features\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Automatic Speech Recognition and Speech Translation', 'api_name': 'openai/whisper-large-v2', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')", 'api_arguments': {'forced_decoder_ids': "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"}, 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')", "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')", 'model.config.forced_decoder_ids = None', "ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')", "sample = ds[0]['audio']", "input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.'}}

