2023-12-01 03:57:18.636740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-01 03:57:19.463412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "./f00892_fill_mask_chinese.py", line 55, in <module>
    test_fill_mask_chinese()
  File "./f00892_fill_mask_chinese.py", line 33, in test_fill_mask_chinese
    result1 = fill_mask_chinese(text1)
  File "./f00892_fill_mask_chinese.py", line 22, in fill_mask_chinese
    return unmasker(text)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 270, in __call__
    outputs = super().__call__(inputs, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1140, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1146, in run_single
    model_inputs = self.preprocess(inputs, **preprocess_params)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 123, in preprocess
    self.ensure_exactly_one_mask_token(model_inputs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 112, in ensure_exactly_one_mask_token
    self._ensure_exactly_one_mask_token(input_ids)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 100, in _ensure_exactly_one_mask_token
    raise PipelineException(
transformers.pipelines.base.PipelineException: No mask_token ([MASK]) found on the input
