{'code': "###Instruction: I want to organize my images based on the scene content. The categories I want are landscape, cityscape, beach, forest, and animals.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. We import the pipeline function from the transformers library provided by Hugging Face.\n2. The pipeline function is used to create an image-classification model, which is capable of categorizing images into predefined categories even if it has not been explicitly trained on those categories.\n3. We specify the model 'laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg' to be loaded. This model is based on the CLIP architecture and has been pre-trained on a large dataset, making it suitable for zero-shot image classification tasks.\n4. We can use the created classifier to classify images into categories like 'landscape', 'cityscape', 'beach', 'forest', and 'animals'.\n<<<code>>>: from transformers import pipeline\nclip = pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\nresult = clip(image_path, class_names=['landscape', 'cityscape', 'beach', 'forest', 'animals'])\n", 'api_call': "pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg', 'api_call': "pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')", 'api_arguments': {'image_path': './path/to/image.jpg', 'class_names': 'class1,class2,class3'}, 'python_environment_requirements': 'transformers', 'example_code': "from transformers import pipeline\nclip = pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\nclip('./path/to/image.jpg', 'class1,class2,class3')", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '75.9%'}, 'description': 'A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on LAION-2B (english), a subset of LAION-5B, using OpenCLIP. The models are trained at 256x256 image resolution and achieve a 75.9 top-1 zero-shot accuracy on ImageNet-1k.'}}

