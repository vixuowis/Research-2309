{'code': '###Instruction: We have a picture of a landscape, and we\'d like to add a building and a river in this picture.\n###Output: <<<domain>>>: Computer Vision Image-to-Image\n<<<api_call>>>: ControlNetModel.from_pretrained(\'lllyasviel/control_v11e_sd15_ip2p\')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Import the required libraries and classes, such as PIL and the various classes like ControlNetModel and StableDiffusionControlNetPipeline from the diffusers package.\n2. Load the initial image of the landscape into a variable named control_image, which will be used as the starting point for the model to manipulate.\n3. Define the prompt to describe the desired transformation, in this case "add a building and a river".\n4. Load the pre-trained ControlNetModel using the provided checkpoint name \'lllyasviel/control_v11e_sd15_ip2p\' and set the torch_dtype to torch.float16.\n5. Create a StableDiffusionControlNetPipeline with the loaded ControlNetModel and necessary settings.\n6. Set the scheduler to UniPCMultistepScheduler.\n7. Enable CPU offload for the model.\n8. Use the .pipe method to apply the specified prompt to the source image with a set number of inference steps.\n9. Save the transformed image with the desired modifications as \'image_out.png\'.\n<<<code>>>: from PIL import Image\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\n\ncontrol_image = load_image(\'landscape.jpg\').convert(\'RGB\')\nprompt = "add a building and a river"\n\ncontrolnet = ControlNetModel.from_pretrained(\'lllyasviel/control_v11e_sd15_ip2p\', torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\'runwayml/stable-diffusion-v1-5\', controlnet=controlnet, torch_dtype=torch.float16)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\ngenerator = torch.manual_seed(0)\n\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save(\'image_out.png\')\n', 'api_call': "ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': "ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\nprompt = make it on fire\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\nimage.save('images/image_out.png')", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.'}}

