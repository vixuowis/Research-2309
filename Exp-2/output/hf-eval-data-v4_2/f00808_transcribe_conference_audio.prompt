{'code': "###Instruction: An audio file is recorded in a conference and we need the text version of the conversation for record-keeping purposes.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary classes from the transformers library: WhisperProcessor and WhisperForConditionalGeneration. Also, import the load_dataset function from the datasets package.\n2. Instantiate a WhisperProcessor object by calling from_pretrained('openai/whisper-small'), which automatically downloads the Whisper pre-trained model for speech-to-text transcription.\n3. Instantiate a WhisperForConditionalGeneration object by calling from_pretrained('openai/whisper-small') as well.\n4. Load the conference audio file with the load_dataset function. \n5. Convert the audio input into features suitable for the Whisper model using the processor's method to process the audio file and obtain input_features.\n6. Use the generate() method of the Whisper model object to get the predicted_ids, which represent the predicted text.\n7. Decode the predicted_ids using the batch_decode() method of the processor to obtain the final text transcription of the conference audio file.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n\naudio_data = load_audio(conference_audio_file)\ninput_features = processor(audio_data[array], sampling_rate=audio_data[sampling_rate], return_tensors='pt').input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-small', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')", 'api_arguments': {'language': 'english', 'task': 'transcribe'}, 'python_environment_requirements': {'transformers': 'latest', 'datasets': 'latest'}, 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-small)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)', 'model.config.forced_decoder_ids = None', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'print(transcription)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '3.432 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.'}}

