{'code': '###Instruction: We were given an audio file of a company presentation, and we need it transcribed verbatim.', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-medium', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')", 'api_arguments': ['sample', 'sampling_rate', 'language', 'task', 'skip_special_tokens'], 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 2.9}, {'name': 'LibriSpeech (other)', 'accuracy': 5.9}, {'name': 'Common Voice 11.0', 'accuracy': 53.87}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.'}}

