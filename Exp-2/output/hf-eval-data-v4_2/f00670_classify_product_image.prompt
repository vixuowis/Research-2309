{'code': '###Instruction: Our e-commerce platform needs automatic classification of product images without retraining. Develop a classifier for the e-commerce platform.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: ChineseCLIPModel.from_pretrained(\'OFA-Sys/chinese-clip-vit-large-patch14\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary libraries such as PIL, requests, and transformers.\n2. Load the pre-trained ChineseCLIPModel using the \'OFA-Sys/chinese-clip-vit-large-patch14\' model.\n3. Initialize the ChineseCLIPProcessor using the same \'OFA-Sys/chinese-clip-vit-large-patch14\' model.\n4. Load the image using the Image class from PIL and URL, or from your e-commerce platform\'s product image.\n5. Define the target category labels for the classification of product images.\n6. Process the image features and text features separately.\n7. Normalize the features and compute the similarity score between the image and text labels using the model.\n8. Obtain the probabilities of each category using the softmax function and determine the most likely category.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\n\nmodel = ChineseCLIPModel.from_pretrained(\'OFA-Sys/chinese-clip-vit-large-patch14\')\nprocessor = ChineseCLIPProcessor.from_pretrained(\'OFA-Sys/chinese-clip-vit-large-patch14\')\n\nimage = Image.open(requests.get(\'image_url\', stream=True).raw)\n# replace \'image_url\' with a URL or filepath containing the product image\n\ntexts = [\'Category_1\', \'Category_2\', \'Category_3\']\n# replace with your category labels\n\ninputs = processor(images=image, return_tensors=\'pt\')\n\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) \n\ninputs = processor(text=texts, padding=True, return_tensors=\'pt\')\n\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) \n\ninputs = processor(text=texts, images=image, return_tensors=\'pt\', padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\ncategory_index = probs.argmax().item() \ncategory = texts[category_index]\n\nprint("Predicted category:", category)', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')", 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': {'libraries': ['transformers', 'PIL', 'requests']}, 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = []\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.'}}

