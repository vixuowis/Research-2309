{'code': "###Instruction: Determine if one Russian sentence logically contradicts the information provided by another Russian sentence.\n###Output: <<<domain>>>: Natural Language Processing Zero-Shot Classification\n<<<api_call>>>: AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries, which are AutoTokenizer and AutoModelForSequenceClassification from transformers.\n2. Load the 'cointegrated/rubert-base-cased-nli-threeway' model using the AutoModelForSequenceClassification.from_pretrained method, and its corresponding tokenizer using AutoTokenizer.from_pretrained method.\n3. Use the tokenizer to tokenize both input sentences (in Russian) and convert them into tensors.\n4. Pass the tensors to the model, which will predict the logical relationship between the input sentences.\n5. Interpret the output logits as probabilities using softmax function, and map the predicted class back to its label ('entailment', 'contradiction', or 'neutral').\n6. Check if the predicted label is 'contradiction' and print the result.\n<<<code>>>: import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nif torch.cuda.is_available():\n    model.cuda()\nsentence1 = 'Russian sentence 1'\nsentence2 = 'Russian sentence 2'\nwith torch.inference_mode():\n    out = model(**tokenizer(sentence1, sentence2, return_tensors='pt').to(model.device))\n    proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\npredicted_label = {v: proba[k] for k, v in model.config.id2label.items()}\nprint('Contradiction detected:', predicted_label['contradiction'] > predicted_label['neutral'] and predicted_label['contradiction'] > predicted_label['entailment'])", 'api_call': "AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Natural Language Inference', 'api_name': 'cointegrated/rubert-base-cased-nli-threeway', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')", 'api_arguments': ['text1', 'text2'], 'python_environment_requirements': ['transformers', 'sentencepiece'], 'example_code': "import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nif torch.cuda.is_available():\n model.cuda()\ntext1 = '.'\ntext2 = '.'\nwith torch.inference_mode():\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\nprint({v: proba[k] for k, v in model.config.id2label.items()})", 'performance': {'dataset': ['JOCI', 'MNLI', 'MPE', 'SICK', 'SNLI', 'ANLI', 'NLI-style FEVER', 'IMPPRES'], 'accuracy': {'ROC AUC': {'entailment': 0.91, 'contradiction': 0.71, 'neutral': 0.79}}}, 'description': 'This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.'}}

