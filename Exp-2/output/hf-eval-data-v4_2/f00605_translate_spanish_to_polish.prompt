{'code': "###Instruction: We need a solution for creating Polish subtitles for YouTube videos in Spanish. The AI should provide the translation.\n###Output: <<<domain>>>: Natural Language Processing Translation\n<<<api_call>>>: MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. We first import the necessary classes from the transformers library provided by Hugging Face. These include MBartForConditionalGeneration for the translation model and MBart50TokenizerFast for tokenizing the text.\n2. We then use the from_pretrained method of the MBartForConditionalGeneration class to load the pre-trained model 'facebook/mbart-large-50-many-to-many-mmt'. This model has been trained to translate text between any pair of 50 languages, including Spanish to Polish.\n3. We create an MBart50TokenizerFast instance and set its source language to Spanish (es_ES) and tokenize the Spanish text (e.g., YouTube video transcript).\n4. The model is then used to generate translated text (Polish subtitles) by setting the forced_bos_token_id to Polish (pl_PL).\n5. Finally, we decode the generated tokens into a human-readable string (Polish subtitles).\n<<<code>>>: from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\ntokenizer.src_lang = 'es_ES'\nencoded_spanish = tokenizer(spanish_text, return_tensors='pt')\ngenerated_tokens = model.generate(**encoded_spanish, forced_bos_token_id=tokenizer.lang_code_to_id['pl_PL'])\npolish_subtitles = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n", 'api_call': "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face', 'functionality': 'Text2Text Generation', 'api_name': 'facebook/mbart-large-50-many-to-many-mmt', 'api_call': "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mbart-large-50-many-to-many-mmt'}, 'python_environment_requirements': {'transformers': '4.0.0'}, 'example_code': 'from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_hi model = MBartForConditionalGeneration.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer = MBart50TokenizerFast.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer.src_lang = hi_IN\nencoded_hi = tokenizer(article_hi, return_tensors=pt)\ngenerated_tokens = model.generate(\n **encoded_hi,\n forced_bos_token_id=tokenizer.lang_code_to_id[fr_XX]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'performance': {'dataset': 'Multilingual Translation', 'accuracy': 'Not specified'}, 'description': 'mBART-50 many-to-many multilingual machine translation model can translate directly between any pair of 50 languages. It was introduced in the Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.'}}

