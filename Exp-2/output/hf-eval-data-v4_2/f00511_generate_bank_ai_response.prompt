{'code': '###Instruction: We are creating an AI assistant for banking clients. The customers should be able to talk to the bot to do various tasks. We need to have a meaningful dialogue with them.\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: AutoModelForSeq2SeqLM.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import AutoTokenizer and AutoModelForSeq2SeqLM from the transformers library.\n2. Download the tokenizer and the pre-trained model \'microsoft/GODEL-v1_1-base-seq2seq\' using the from_pretrained method. This model is designed for goal-directed dialogs and can generate intelligent conversational responses grounded in external text.\n3. Define a generate function that takes an instruction string, knowledge string, and dialog history. This function will be responsible for processing the inputs and generating a suitable response.\n4. Implement the generate function to tokenize the input, pass it through the model, and decode the generated response.\n5. Use this function to create the AI assistant for banking clients, allowing them to converse with the AI, receive information, and perform various tasks.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\n\ndef generate(instruction, knowledge, dialog):\n    if knowledge != \'\':\n        knowledge = \'[KNOWLEDGE] \' + knowledge\n    dialog = \' EOS \'.join(dialog)\n    query = f"{instruction} [CONTEXT] {dialog} {knowledge}"\n    input_ids = tokenizer(f"{query}", return_tensors=\'pt\').input_ids\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return output\n', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/GODEL-v1_1-base-seq2seq', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'api_arguments': ['instruction', 'knowledge', 'dialog'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'N/A'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.'}}

