{'code': "###Instruction: Develop a system that can detect voices in a podcast to find out if guests are speaking or not. \n\n###Output: <<<domain>>>: Audio Voice Activity Detection\n<<<api_call>>>: Model.from_pretrained('pyannote/segmentation', use_auth_token='ACCESS_TOKEN_GOES_HERE')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. First, we need to install the pyannote.audio library (version 2.1.1), which provides tools for voice activity detection and other audio-related tasks.\n2. Next, we will use the pre-trained 'pyannote/segmentation' model from the Hugging Face Model Hub.\n3. We'll import the VoiceActivityDetection pipeline from the pyannote.audio.pipelines module.\n4. Create an instance of the VoiceActivityDetection pipeline using the pre-trained model.\n5. Use the pipeline to process the audio file and detect voice activity.\n6. This will give us an output indicating when speakers are active in the podcast, which can be further analyzed to identify guests' spoken segments.\n<<<code>>>: from pyannote.audio.pipelines import VoiceActivityDetection\nfrom pyannote.audio import Model\n\nmodel = Model.from_pretrained('pyannote/segmentation', use_auth_token='ACCESS_TOKEN_GOES_HERE')\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n 'onset': 0.5, 'offset': 0.5,\n 'min_duration_on': 0.0,\n 'min_duration_off': 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline('audio.wav')\n# replace 'audio.wav' with the path to your podcast audio file", 'api_call': "Model.from_pretrained('pyannote/segmentation', use_auth_token='ACCESS_TOKEN_GOES_HERE')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Speaker segmentation, Voice activity detection, Overlapped speech detection, Resegmentation, Raw scores', 'api_name': 'pyannote/segmentation', 'api_call': "Model.from_pretrained('pyannote/segmentation', use_auth_token='ACCESS_TOKEN_GOES_HERE')", 'api_arguments': {'use_auth_token': 'ACCESS_TOKEN_GOES_HERE'}, 'python_environment_requirements': 'pyannote.audio 2.1.1', 'example_code': {'voice_activity_detection': 'from pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n onset: 0.5, offset: 0.5,\n min_duration_on: 0.0,\n min_duration_off: 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(audio.wav)', 'overlapped_speech_detection': 'from pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(audio.wav)', 'resegmentation': 'from pyannote.audio.pipelines import Resegmentation\npipeline = Resegmentation(segmentation=model, diarization=baseline)\npipeline.instantiate(HYPER_PARAMETERS)\nresegmented_baseline = pipeline({audio: audio.wav, baseline: baseline})'}, 'performance': {'dataset': {'AMI Mix-Headset': {'voice_activity_detection_accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}, 'overlapped_speech_detection_accuracy': {'onset': 0.448, 'offset': 0.362, 'min_duration_on': 0.116, 'min_duration_off': 0.187}, 'resegmentation_accuracy': {'onset': 0.542, 'offset': 0.527, 'min_duration_on': 0.044, 'min_duration_off': 0.705}}, 'DIHARD3': {'voice_activity_detection_accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}, 'overlapped_speech_detection_accuracy': {'onset': 0.43, 'offset': 0.32, 'min_duration_on': 0.091, 'min_duration_off': 0.144}, 'resegmentation_accuracy': {'onset': 0.592, 'offset': 0.489, 'min_duration_on': 0.163, 'min_duration_off': 0.182}}, 'VoxConverse': {'voice_activity_detection_accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}, 'overlapped_speech_detection_accuracy': {'onset': 0.587, 'offset': 0.426, 'min_duration_on': 0.337, 'min_duration_off': 0.112}, 'resegmentation_accuracy': {'onset': 0.537, 'offset': 0.724, 'min_duration_on': 0.41, 'min_duration_off': 0.563}}}}, 'description': 'A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework.'}}

