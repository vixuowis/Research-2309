{'code': "###Instruction: We are developing a voice-controlled device. Help categorize spoken commands into specific keywords to trigger various actions.\n###Output: <<<domain>>>: Audio Audio Classification\n<<<api_call>>>: pipeline('audio-classification', model='superb/wav2vec2-base-superb-ks')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary libraries, in this case, we need 'pipeline' from transformers.\n2. Use the 'pipeline' function from transformers to create an audio classification model with the specified model checkpoint 'superb/wav2vec2-base-superb-ks'.\n3. This model is specifically trained for keyword spotting to recognize pre-registered keywords in speech.\n4. Once the model is loaded, we can use it to classify the spoken commands into specific keywords which can then be used to trigger various actions in the voice-controlled device.\n<<<code>>>: from transformers import pipeline\naudio_classifier = pipeline('audio-classification', model='superb/wav2vec2-base-superb-ks')\nspoken_command = 'path_to_audio_file.wav'\n# replace 'path_to_audio_file.wav' with the path to an audio file containing the spoken command\nkeyword = audio_classifier(spoken_command, top_k=1)\n", 'api_call': "pipeline('audio-classification', model='superb/wav2vec2-base-superb-ks')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'superb/wav2vec2-base-superb-ks', 'api_call': "pipeline('audio-classification', model='superb/wav2vec2-base-superb-ks')", 'api_arguments': {'model': 'superb/wav2vec2-base-superb-ks'}, 'python_environment_requirements': ['torch', 'transformers', 'torchaudio', 'datasets'], 'example_code': 'from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-ks)\nlabels = classifier(dataset[0][file], top_k=5)', 'performance': {'dataset': 'Speech Commands dataset v1.0', 'accuracy': {'s3prl': 0.9623, 'transformers': 0.9643}}, 'description': 'Wav2Vec2-Base for Keyword Spotting (KS) task in the SUPERB benchmark. The base model is pretrained on 16kHz sampled speech audio. The KS task detects preregistered keywords by classifying utterances into a predefined set of words. The model is trained on the Speech Commands dataset v1.0.'}}

