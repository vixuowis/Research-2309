{'code': '###Instruction: We are an animal rescue organization, and we are looking for a way to automatically identify if images uploaded to our site contain cats, dogs, or birds.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: pipeline(\'image-classification\', model=\'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>:1. Import the Hugging Face transformers library and the \'pipeline\' class to build the image classification pipeline.\n2. Load the pre-trained \'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\' model using the \'pipeline\' function. This model has been trained on a large dataset of images and is capable of zero-shot image classification, making it suitable for identifying cats, dogs, and birds in images.\n3. Create a list of class names representing the categories we need the model to identify ("cat, dog, bird").\n4. Use the model to classify the image by passing it the image path and the list of class names.\n5. The model will return the predicted category along with a confidence score, which can be used to make decisions based on the content of the image.\n<<<code>>>: from transformers import pipeline\nimage_path = \'your_image_path.jpg\'\n# Replace \'your_image_path.jpg\' with the path to the image you want to classify\nmodel = pipeline(\'image-classification\', model=\'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\')\nclass_names = \'cat, dog, bird\'\nresults = model(image_path, class_names=class_names)', 'api_call': "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup', 'api_call': "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')", 'api_arguments': 'image_path, class_names', 'python_environment_requirements': 'transformers', 'example_code': "results = model(image_path, class_names='cat, dog, bird')", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '76.9'}, 'description': 'A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).'}}

