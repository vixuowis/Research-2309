{'code': "###Instruction: The company wants to use geolocalization techniques to identify the location of a given image.\n###Output: <<<domain>>>: Multimodal Zero-Shot Image Classification\n<<<api_call>>>: CLIPModel.from_pretrained('geolocal/StreetCLIP')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries, including PIL for image manipulation, requests for file retrieval, and transformers for the model and processor.\n2. Load the pretrained CLIP model 'geolocal/StreetCLIP', which is optimized for image geolocalization capabilities.\n3. Create a CLIP processor using the same 'geolocal/StreetCLIP' model, which will help us reformat the input data.\n4. Retrieve the image from a URL or local file, and process it using PIL's Image.open() function.\n5. Create a list of possible choices for the location of the image, so the model can compare and identify the closest match.\n6. Use the processor to convert the text choices and image into tensors, and pass these into the model.\n7. The model will then output the estimated probabilities for each location, which can help us determine the most likely match for the image.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained('geolocal/StreetCLIP')\nprocessor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\nurl = 'https://image_url_here.jpeg'\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco']\ninputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)", 'api_call': "CLIPModel.from_pretrained('geolocal/StreetCLIP')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Geolocalization', 'api_name': 'geolocal/StreetCLIP', 'api_call': "CLIPModel.from_pretrained('geolocal/StreetCLIP')", 'api_arguments': {'pretrained_model_name_or_path': 'geolocal/StreetCLIP'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': [{'name': 'IM2GPS', 'accuracy': {'25km': 28.3, '200km': 45.1, '750km': 74.7, '2500km': 88.2}}, {'name': 'IM2GPS3K', 'accuracy': {'25km': 22.4, '200km': 37.4, '750km': 61.3, '2500km': 80.4}}]}, 'description': 'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.'}}

