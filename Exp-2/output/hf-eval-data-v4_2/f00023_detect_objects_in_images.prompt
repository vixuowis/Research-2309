{'code': '###Instruction: A client from real estate agency needs to get a list of objects present in a series of pictures to prepare their property listings.\n###Output: <<<domain>>>: Computer Vision Object Detection\n<<<api_call>>>: OwlViTForObjectDetection.from_pretrained(\'google/owlvit-base-patch16\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary libraries such as OwlViTProcessor, OwlViTForObjectDetection from the transformers package and Image from PIL package.\n2. Instantiate the OwlViTProcessor and OwlViTForObjectDetection classes using the from_pretrained method with the model \'google/owlvit-base-patch16\' to process the input data and detect objects in images.\n3. Prepare the image and texts representing the objects of interest. The image can be loaded from a URL or a local file using the Image class from the PIL package. For this task, the texts should represent objects commonly found in properties listed by real estate agencies.\n4. Use the processor to process the input data and generate appropriate tensors.\n5. The prepared input tensors are then passed into the OwlViTForObjectDetection model, which performs zero-shot object detection. Finally, the processor\'s post_process method is called to decode the detected objects and gather the results.\n<<<code>>>: from transformers import OwlViTProcessor, OwlViTForObjectDetection\nfrom PIL import Image\nimport requests\nprocessor = OwlViTProcessor.from_pretrained(\'google/owlvit-base-patch16\')\nmodel = OwlViTForObjectDetection.from_pretrained(\'google/owlvit-base-patch16\')\n\nurl = \'http://images.cocodataset.org/val2017/000000039769.jpg\'\nimage = Image.open(requests.get(url, stream=True).raw)\n\ntexts = [[\'a photo of a living room\', \'a photo of a kitchen\', \'a photo of a bedroom\', \'a photo of a bathroom\']]\ninputs = processor(text=texts, images=image, return_tensors="pt")\n\noutputs = model(**inputs)\n_target_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'api_call': "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch16', 'api_call': "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')", 'api_arguments': ['texts', 'images'], 'python_environment_requirements': ['requests', 'PIL', 'torch', 'transformers'], 'example_code': 'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.'}}

