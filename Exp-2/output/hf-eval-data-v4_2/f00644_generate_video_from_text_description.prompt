{'code': '###Instruction: I am organizing a virtual party and want to create a short, autogenerated video based on a text description (e.g., "cats playing with laser pointer"). Can you accomplish this with a text-to-video generation API?\n###Output: <<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b\', torch_dtype=torch.float16, variant=\'fp16\')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. First, install the necessary libraries (diffusers, transformers, and accelerate) using pip.\n2. Then, import the required classes and functions, such as DiffusionPipeline, DPMSolverMultistepScheduler, and export_to_video.\n3. Load the \'damo-vilab/text-to-video-ms-1.7b\' model by creating a DiffusionPipeline instance using the from_pretrained method.\n4. Configure the scheduler and enable CPU offload for the model.\n5. Provide a text description (e.g., "cats playing with laser pointer") as input to the model and specify the number of inference steps.\n6. Finally, use the export_to_video function to save the generated video frames as a video file.\n<<<code>>>: import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\'damo-vilab/text-to-video-ms-1.7b\', torch_dtype=torch.float16, variant=\'fp16\')\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = "cats playing with laser pointer"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b', torch_dtype=torch.float16, variant=fp16)", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b', torch_dtype=torch.float16, variant=fp16)", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.'}}

