{'code': "###Instruction: We are working on environmental studies and there is a specific image we need to geolocalize. Let's get the probabilities of different cities for the given image.\n###Output: <<<domain>>>: Multimodal Zero-Shot Image Classification\n<<<api_call>>>: CLIPModel.from_pretrained('geolocal/StreetCLIP')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries and classes, which include Image from PIL, requests, CLIPProcessor, and CLIPModel from transformers.\n2. Load the 'geolocal/StreetCLIP' model and processor using the from_pretrained method.\n3. Fetch the image from the URL, or use an image file instead.\n4. Define a list of possible city names that you want to geolocalize the image to.\n5. Process the texts and image using the CLIPProcessor, and then pass the input to the model.\n6. Compute probabilities for different cities using softmax activations on the outputs.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained('geolocal/StreetCLIP')\nprocessor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\nurl = 'https://path_to_your_image.com/image.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco']\n\ninputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1).tolist()[0]\ncity_probs = dict(zip(choices, probs))\n\nprint(city_probs)", 'api_call': "CLIPModel.from_pretrained('geolocal/StreetCLIP')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Geolocalization', 'api_name': 'geolocal/StreetCLIP', 'api_call': "CLIPModel.from_pretrained('geolocal/StreetCLIP')", 'api_arguments': {'pretrained_model_name_or_path': 'geolocal/StreetCLIP'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': [{'name': 'IM2GPS', 'accuracy': {'25km': 28.3, '200km': 45.1, '750km': 74.7, '2500km': 88.2}}, {'name': 'IM2GPS3K', 'accuracy': {'25km': 22.4, '200km': 37.4, '750km': 61.3, '2500km': 80.4}}]}, 'description': 'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.'}}

