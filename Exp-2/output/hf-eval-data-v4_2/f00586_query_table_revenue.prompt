{'code': '###Instruction: We need to extract information from a table to answer a user\'s question regarding a company\'s revenue.\n###Output: <<<domain>>>: Natural Language Processing Table Question Answering\n<<<api_call>>>: TapasForQuestionAnswering.from_pretrained(\'google/tapas-base-finetuned-wikisql-supervised\')\n<<<api_provider>>>: Transformers\n<<<explanation>>>: 1. We have to use TAPAS, which is a powerful language model trained on a large corpus of Wikipedia data. Its primary goal is to answer questions based on tabular data.\n2. We need to load the \'google/tapas-base-finetuned-wikisql-supervised\' model, which has been fine-tuned for answering questions based on the WikiSQL dataset.\n3. With the model in place, we can interact with it and pass it the user\'s question along with the table data.\n4. The model will then process the table and return the answer to the user\'s query, which we can extract from the output.\n<<<code>>>: from transformers import TapasForQuestionAnswering\nmodel = TapasForQuestionAnswering.from_pretrained(\'google/tapas-base-finetuned-wikisql-supervised\')\nquestion = "What was the revenue of the company in 2020?"\ntable_data = [\n  {"Year": "2018", "Revenue": "$20M"},\n  {"Year": "2019", "Revenue": "$25M"},\n  {"Year": "2020", "Revenue": "$30M"},\n]\nanswer = model.predict(question, table_data)\nprint(answer)\n', 'api_call': "TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wikisql-supervised')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-base-finetuned-wikisql-supervised', 'api_call': "TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wikisql-supervised')", 'api_arguments': ['question', 'table'], 'python_environment_requirements': ['PyTorch', 'TensorFlow'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'wikisql', 'accuracy': 'Not provided'}, 'description': 'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It was pretrained with two objectives: Masked language modeling (MLM) and Intermediate pre-training. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQA and WikiSQL.'}}

