{'code': '###Instruction: We need to detect objects in an image provided by a user and describe their locations.\n###Output: <<<domain>>>: Computer Vision Object Detection\n<<<api_call>>>: OwlViTForObjectDetection.from_pretrained(\'google/owlvit-large-patch14\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import required libraries and modules including Image from PIL, requests, torch, OwlViTProcessor, and OwlViTForObjectDetection from transformers.\n2. Load the OwlViTForObjectDetection model and OwlViTProcessor from the Hugging Face model hub using the specified model name.\n3. Get the input image from the user, load it with PIL.Image and use the processor to prepare the input tensors.\n4. Make predictions using the model with the provided input tensors.\n5. Post-process the predictions to obtain bounding boxes, scores, and labels.\n6. Set a score threshold and filter the results based on the threshold.\n7. Iterate through the filtered results and print the detected objects, their confidence scores, and bounding box locations.\n<<<code>>>: import requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\nprocessor = OwlViTProcessor.from_pretrained(\'google/owlvit-large-patch14\')\nmodel = OwlViTForObjectDetection.from_pretrained(\'google/owlvit-large-patch14\')\nimage = Image.open(requests.get(image_url, stream=True).raw)\ntexts = ["a photo of a cat", "a photo of a dog"]\ninputs = processor(text=texts, images=image, return_tensors="pt")\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\nscore_threshold = 0.1\n\nfor i, text in enumerate(texts):\n    boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]\n    for box, score, label in zip(boxes, scores, labels):\n        box = [round(i, 2) for i in box.tolist()]\n        if score >= score_threshold:\n            print(f"Detected {text} with confidence {round(score.item(), 3)} at location {box}")\n', 'api_call': "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.'}}

