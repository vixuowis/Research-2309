{'code': "###Instruction: I am a lawyer, I have a lot of text. I need a system which reads context and answers the questions based on the context.\n###Output: <<<domain>>>: Natural Language Processing Question Answering\n<<<api_call>>>: pipeline('question-answering', model='philschmid/distilbert-onnx')\n<<<api_provider>>>: Transformers\n<<<explanation>>>: 1. Import the 'pipeline' function from the transformers library.\n2. Use the 'pipeline' function to load the model, which in this case is 'philschmid/distilbert-onnx', a pre-trained model fine-tuned for question answering tasks.\n3. The loaded model will read the context provided and answer the given question based on the contextual information.\n4. Provide the required context and question, and the model will give you the most plausible answer to your question.\n<<<code>>>: from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\ncontext_text = 'Your context text here...'\nquestion = 'Your question here...'\nanswer = qa_pipeline({'context': context_text, 'question': question})\n", 'api_call': "pipeline('question-answering', model='philschmid/distilbert-onnx')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'philschmid/distilbert-onnx', 'api_call': "pipeline('question-answering', model='philschmid/distilbert-onnx')", 'api_arguments': {'model': 'philschmid/distilbert-onnx'}, 'python_environment_requirements': ['transformers', 'onnx'], 'example_code': {'Compute': "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"}, 'performance': {'dataset': 'squad', 'accuracy': 'F1 score: 87.1'}, 'description': 'This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.'}}

