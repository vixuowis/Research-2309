{'code': "###Instruction: We are building an app for tour guides to transcribe what they say during the tour. Then the transcript will be translated into sign language.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import the necessary library classes like WhisperProcessor, WhisperForConditionalGeneration from the transformers package and load our dataset using the datasets package.\n2. We then create a WhisperProcessor to preprocess the spoken language input and transform it into a format acceptable by the Whisper model.\n3. We initialize the ASR model with the from_pretrained method.\n4. We load a sample from our dataset or record the tour guide's speech using a microphone live during the tour.\n5. The speech is then preprocessed by the WhisperProcessor.\n6. We use the model to generate the text transcription of the speech input.\n7. Now that the spoken words have been transcribed, the transcript can be translated into sign language for those who are hearing impaired.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-base')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')\nds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\n\nsample = ds[0]['audio']\ninput_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Automatic Speech Recognition and Speech Translation', 'api_name': 'openai/whisper-base', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')", 'api_arguments': {'model_name': 'openai/whisper-base', 'input_features': 'input_features', 'forced_decoder_ids': 'forced_decoder_ids'}, 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-base)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-base)', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '5.009 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.'}}

