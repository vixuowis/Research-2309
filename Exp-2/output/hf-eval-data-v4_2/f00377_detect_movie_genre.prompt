{'code': "###Instruction: I am a movie director and I need to detect the genre of a movie based on its actions.\n###Output: <<<domain>>>: Computer Vision Video Classification\n<<<api_call>>>: VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import necessary classes from transformers and decord packages. This includes VideoMAEForVideoClassification for the video classification model and VideoReader for processing video data.\n2. We use the from_pretrained method of the VideoMAEForVideoClassification class to load the pre-trained model 'nateraw/videomae-base-finetuned-ucf101'. This model has been trained for video action recognition tasks, which helps in detecting the genre of a movie based on its actions.\n3. We load the video data to be analyzed using the appropriate import classes.\n4. This model analyzes the video and identifies the various actions occurring in it. Based on these actions, the genre of the movie can be determined.\n<<<code>>>: from transformers import VideoMAEForVideoClassification, VideoMAEFeatureExtractor\nfrom decord import VideoReader\nimport torch\nimport numpy as np\n\nvideo_filename = 'path/to/video_file.mp4'\n# replace with the path to your video file\n\nvideoreader = VideoReader(video_filename)\n\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\nframes = videoreader.get_batch(list(range(0, len(videoreader), 4))) # Sample every 4 frames\ninputs = feature_extractor(list(frames.asnumpy()), return_tensors='pt')\noutputs = model(**inputs)\n\npredicted_label = outputs.logits.argmax(-1).item()\n\nprint(model.config.id2label[predicted_label])", 'api_call': "VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Action Recognition', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': "VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': ['transformers', 'decord', 'huggingface_hub'], 'example_code': 'from decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\nnp.random.seed(0)\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n converted_len = int(clip_len * frame_sample_rate)\n end_idx = np.random.randint(converted_len, seg_len)\n start_idx = end_idx - converted_len\n indices = np.linspace(start_idx, end_idx, num=clip_len)\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n return indices\nfile_path = hf_hub_download(\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\n)\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\nvideoreader.seek(0)\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\nvideo = videoreader.get_batch(indices).asnumpy()\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\ninputs = feature_extractor(list(video), return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition'}}

