{'code': "###Instruction: We are developing an app to classify food images. We have a set of images and want to use a pre-trained model for classification.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the 'pipeline' function from transformers library provided by Hugging Face.\n2. Use 'pipeline' function to create an image classification model using the pretrained model 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'.\n3. The loaded model can be used for zero-shot image classification, meaning it will make predictions for images it has not encountered during training.\n4. The model can be used with a set of images to classify the food items in them. You can provide a list of food classes to the model for more accurate results.\n<<<code>>>: from transformers import pipeline\nimage_classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\nfood_classes = ['pizza', 'sushi', 'sandwich', 'salad', 'cake']\n# provide your list of food classes\nresult = image_classifier(image_path, possible_class_names=food_classes)\n", 'api_call': "pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k', 'api_call': "pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')", 'api_arguments': ['image', 'possible_class_names'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '80.1'}, 'description': 'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.'}}

