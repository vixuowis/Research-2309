{'code': '###Instruction: We want to recommend workouts to our users, based on the type of sports they enjoy. Help us classify sports videos.\n###Output: <<<domain>>>: Computer Vision Video Classification\n<<<api_call>>>: VideoMAEForVideoClassification.from_pretrained(\'MCG-NJU/videomae-large-finetuned-kinetics\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary classes and functions from the transformers library. This includes the VideoMAEImageProcessor and VideoMAEForVideoClassification classes.\n2. Load the pre-trained model \'MCG-NJU/videomae-large-finetuned-kinetics\'. This model has been fine-tuned on the Kinetics-400 dataset, which contains a large number of sports videos.\n3. Process the video frames using the VideoMAEImageProcessor. This will convert the input video frames into a format that can be fed into the model.\n4. Use the VideoMAEForVideoClassification model to classify the processed video frames into one of the sports categories.\n<<<code>>>: from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\n\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(\'MCG-NJU/videomae-large-finetuned-kinetics\')\nmodel = VideoMAEForVideoClassification.from_pretrained(\'MCG-NJU/videomae-large-finetuned-kinetics\')\n\ninputs = processor(video, return_tensors=\'pt\')\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    print("Predicted class:", model.config.id2label[predicted_class_idx])\n', 'api_call': "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'MCG-NJU/videomae-large-finetuned-kinetics', 'api_call': "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')", 'api_arguments': ['video'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 84.7, 'top-5': 96.5}}, 'description': 'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}}

