{'code': '###Instruction: Create a Python function named "summarize_russian_text" that accepts a Russian text as input and returns a brief summary of the text.\n###Output: <<<domain>>>: Natural Language Processing Summarization\n<<<api_call>>>: T5ForConditionalGeneration.from_pretrained(\'cointegrated/rut5-base-absum\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries and functions, such as T5ForConditionalGeneration and T5Tokenizer from the transformers library.\n2. Load the model and tokenizer using the \'cointegrated/rut5-base-absum\' model.\n3. Define the "summarize_russian_text" function, which takes a Russian text input.\n4. Use the tokenizer to tokenize the input text and create input tensors that can be fed to the model.\n5. Generate a summary using the model\'s \'generate\' function with appropriate parameters for summarization.\n6. Decode the generated text to produce the final summary. \n<<<code>>>: import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nMODEL_NAME = \'cointegrated/rut5-base-absum\'\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel.cuda()\nmodel.eval()\n\ndef summarize_russian_text(text, max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, **kwargs):\n    x = tokenizer(text, return_tensors=\'pt\', padding=True).to(model.device)\n    with torch.inference_mode():\n        out = model.generate(x, max_length=max_length, num_beams=num_beams, do_sample=do_sample, repetition_penalty=repetition_penalty, **kwargs)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\n# Example usage:\nrussian_text = "Пример оригинального русского текста здесь..."\nsummary = summarize_russian_text(russian_text)\nprint(summary)', 'api_call': "T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Abstractive Russian Summarization', 'api_name': 'cointegrated/rut5-base-absum', 'api_call': "T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')", 'api_arguments': {'n_words': 'int', 'compression': 'float', 'max_length': 'int', 'num_beams': 'int', 'do_sample': 'bool', 'repetition_penalty': 'float'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': "import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nMODEL_NAME = 'cointegrated/rut5-base-absum'\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel.cuda();\nmodel.eval();\ndef summarize(\n text, n_words=None, compression=None,\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \n <strong>kwargs\n):\n \n Summarize the text\n The following parameters are mutually exclusive:\n - n_words (int) is an approximate number of words to generate.\n - compression (float) is an approximate length ratio of summary and original text.\n \n if n_words:\n text = '[{}] '.format(n_words) + text\n elif compression:\n text = '[{0:.1g}] '.format(compression) + text\n x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\n with torch.inference_mode():\n out = model.generate(\n </strong>x, \n max_length=max_length, num_beams=num_beams, \n do_sample=do_sample, repetition_penalty=repetition_penalty, \n **kwargs\n )\n return tokenizer.decode(out[0], skip_special_tokens=True)", 'performance': {'dataset': ['csebuetnlp/xlsum', 'IlyaGusev/gazeta', 'mlsum'], 'accuracy': 'Not provided'}, 'description': 'This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.'}}

