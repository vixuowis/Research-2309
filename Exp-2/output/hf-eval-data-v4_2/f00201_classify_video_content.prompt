{'code': "###Instruction: People in my company need an automatic solution to categorize videos based on their content. The system should be able to recognize the main theme of a video with high accuracy.\n###Output: <<<domain>>>: Computer Vision Video Classification\n<<<api_call>>>: VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import VideoMAEImageProcessor and VideoMAEForVideoClassification from the transformers library.\n2. Load the pre-trained model 'MCG-NJU/videomae-base-short-finetuned-kinetics' using the VideoMAEForVideoClassification.from_pretrained() method. This model is fine-tuned on the Kinetics-400 dataset and is capable of classifying videos based on their content.\n3. Use the VideoMAEImageProcessor to process the video frames before feeding them into the model.\n4. Pass the processed video frames to the model and obtain the logits for each class from model's outputs.\n5. The class with the highest logit value is the predicted class for the video.\n<<<code>>>: from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\n\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\ninputs = processor(video, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])", 'api_call': "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'MCG-NJU/videomae-base-short-finetuned-kinetics', 'api_call': "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')", 'api_arguments': ['video'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\ninputs = processor(video, return_tensors='pt')\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])", 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 79.4, 'top-5': 94.1}}, 'description': 'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}}

