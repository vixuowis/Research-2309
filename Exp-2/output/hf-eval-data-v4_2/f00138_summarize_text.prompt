{'code': '###Instruction: We are building an app to summarize long articles for users. We need a solution to create a condensed summary of the given text.\n###Output: <<<domain>>>: Natural Language Processing Summarization\n<<<api_call>>>: BartForConditionalGeneration.from_pretrained(\'sshleifer/distilbart-cnn-12-6\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary classes from the transformers library, specifically BartTokenizer and BartForConditionalGeneration.\n2. Load the pre-trained model \'sshleifer/distilbart-cnn-12-6\' using the BartForConditionalGeneration class. This model is trained for text summarization tasks, which is exactly what we need for our app.\n3. Also load the associated tokenizer using the BartTokenizer class.\n4. Tokenize the input text using the tokenizer and pass the input to the loaded model.\n5. Decode the model\'s output to get the summary of the input text.\n<<<code>>>: from transformers import BartTokenizer, BartForConditionalGeneration\nmodel = BartForConditionalGeneration.from_pretrained(\'sshleifer/distilbart-cnn-12-6\')\ntokenizer = BartTokenizer.from_pretrained(\'sshleifer/distilbart-cnn-12-6\')\ninput_text = "Long article text..."\ninputs = tokenizer(input_text, return_tensors=\'pt\')\nsummary_ids = model.generate(inputs[\'input_ids\'], num_beams=4, max_length=50, early_stopping=True)\nsummary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': "BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'sshleifer/distilbart-cnn-12-6', 'api_call': "BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')", 'api_arguments': '', 'python_environment_requirements': 'huggingface/transformers', 'example_code': '', 'performance': {'dataset': [{'name': 'cnn_dailymail', 'accuracy': {'Rouge 2': '22.12', 'Rouge-L': '36.99'}}]}, 'description': "DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library."}}

