{'code': '###Instruction: You have just met a person that speaks French. As a hotel manager, you need to tell them, "Welcome to our hotel, we hope you enjoy your stay." in French.\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained(\'facebook/m2m100_418M\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers library.\n2. Load the model \'facebook/m2m100_418M\' using M2M100ForConditionalGeneration.from_pretrained() method, which is trained for multilingual translation.\n3. Load the tokenizer associated with the multilingual model using M2M100Tokenizer.from_pretrained().\n4. Assign the source language code (e.g., English would be \'en\') to the tokenizer\'s src_lang attribute.\n5. Encode the given text (in this case, the English welcome message) using the tokenizer\'s encoding method. This will provide the necessary input tensor for the model.\n6. Generate the translated text in French by using the model\'s generate() method with the forced_bos_token_id parameter set to the target language code (here, French: \'fr\').\n7. Decode the generated tokens using the tokenizer\'s batch_decode() method to obtain the translated text.\n<<<code>>>: from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\nenglish_text = "Welcome to our hotel, we hope you enjoy your stay."\nmodel = M2M100ForConditionalGeneration.from_pretrained(\'facebook/m2m100_418M\')\ntokenizer = M2M100Tokenizer.from_pretrained(\'facebook/m2m100_418M\')\ntokenizer.src_lang = \'en\'\nencoded_input = tokenizer(english_text, return_tensors=\'pt\')\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(\'fr\'))\nfrench_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n', 'api_call': "M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Multilingual Translation', 'api_name': 'facebook/m2m100_418M', 'api_call': "M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')", 'api_arguments': {'encoded_input': 'Encoded input text', 'target_lang': 'Target language code'}, 'python_environment_requirements': ['transformers', 'sentencepiece'], 'example_code': ['from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer', 'hi_text = ', 'chinese_text = ', 'model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)', 'tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)', 'tokenizer.src_lang = hi', 'encoded_hi = tokenizer(hi_text, return_tensors=pt)', 'generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))', 'tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)'], 'performance': {'dataset': 'WMT', 'accuracy': 'Not provided'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.'}}

