{'code': '###Instruction: We are a group of teachers looking to create an automated summary based on the student\'s essays.\n###Output: <<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: BartModel.from_pretrained(\'facebook/bart-base\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary classes from the transformers package, in this case, BartTokenizer and BartModel.\n2. The pre-trained model \'facebook/bart-base\' will be loaded using BartModel.from_pretrained() method. This model can be used for text summarization tasks, which will be helpful in summarizing student essays.\n3. Use the BartTokenizer to convert the input text (student essay) into tokens compatible with the model.\n4. Pass the tokens into the BartModel to generate a summary of the essay.\n<<<code>>>: from transformers import BartTokenizer, BartModel\nessay = "Your student\'s essay here."\ntokenizer = BartTokenizer.from_pretrained(\'facebook/bart-base\')\nmodel = BartModel.from_pretrained(\'facebook/bart-base\')\ninputs = tokenizer(essay, return_tensors=\'pt\')\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n', 'api_call': "BartModel.from_pretrained('facebook/bart-base')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': "BartModel.from_pretrained('facebook/bart-base')", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmodel = BartModel.from_pretrained('facebook/bart-base')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).'}}

