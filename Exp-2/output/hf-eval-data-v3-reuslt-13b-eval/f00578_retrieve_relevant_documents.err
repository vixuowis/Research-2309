tokenizer_config.json:   0%|                                                         | 0.00/29.0 [00:00<?, ?B/s]tokenizer_config.json: 100%|█████████████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 8.46kB/s]
config.json:   0%|                                                                    | 0.00/473 [00:00<?, ?B/s]config.json: 100%|██████████████████████████████████████████████████████████████| 473/473 [00:00<00:00, 106kB/s]
vocab.txt:   0%|                                                                     | 0.00/213k [00:00<?, ?B/s]vocab.txt: 100%|██████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 249kB/s]vocab.txt: 100%|██████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 249kB/s]
tokenizer.json:   0%|                                                                | 0.00/436k [00:00<?, ?B/s]tokenizer.json: 100%|█████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 580kB/s]tokenizer.json: 100%|█████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 580kB/s]
model.safetensors:   0%|                                                             | 0.00/261M [00:00<?, ?B/s]model.safetensors:   4%|██                                                  | 10.5M/261M [00:04<01:37, 2.56MB/s]model.safetensors:   8%|████▏                                               | 21.0M/261M [00:07<01:20, 2.97MB/s]model.safetensors:  12%|██████▎                                             | 31.5M/261M [00:09<01:07, 3.38MB/s]model.safetensors:  16%|████████▎                                           | 41.9M/261M [00:12<01:01, 3.54MB/s]model.safetensors:  20%|██████████▍                                         | 52.4M/261M [00:17<01:15, 2.75MB/s]model.safetensors:  24%|████████████▌                                       | 62.9M/261M [00:26<01:41, 1.94MB/s]model.safetensors:  28%|██████████████▋                                     | 73.4M/261M [00:34<01:53, 1.66MB/s]model.safetensors:  32%|████████████████▋                                   | 83.9M/261M [00:45<02:09, 1.37MB/s]model.safetensors:  36%|██████████████████▊                                 | 94.4M/261M [00:52<02:01, 1.37MB/s]model.safetensors:  40%|█████████████████████▎                               | 105M/261M [01:00<01:55, 1.34MB/s]model.safetensors:  44%|███████████████████████▍                             | 115M/261M [01:08<01:48, 1.35MB/s]model.safetensors:  48%|█████████████████████████▌                           | 126M/261M [01:18<01:47, 1.25MB/s]model.safetensors:  52%|███████████████████████████▋                         | 136M/261M [01:26<01:36, 1.29MB/s]model.safetensors:  56%|█████████████████████████████▊                       | 147M/261M [01:35<01:33, 1.22MB/s]model.safetensors:  60%|███████████████████████████████▉                     | 157M/261M [01:45<01:27, 1.18MB/s]model.safetensors:  64%|██████████████████████████████████                   | 168M/261M [01:53<01:15, 1.22MB/s]model.safetensors:  68%|████████████████████████████████████▏                | 178M/261M [02:01<01:08, 1.21MB/s]model.safetensors:  72%|██████████████████████████████████████▎              | 189M/261M [02:08<00:56, 1.28MB/s]model.safetensors:  76%|████████████████████████████████████████▍            | 199M/261M [02:17<00:48, 1.27MB/s]model.safetensors:  80%|██████████████████████████████████████████▌          | 210M/261M [02:26<00:40, 1.25MB/s]model.safetensors:  84%|████████████████████████████████████████████▊        | 220M/261M [02:34<00:32, 1.23MB/s]model.safetensors:  88%|██████████████████████████████████████████████▉      | 231M/261M [02:43<00:24, 1.24MB/s]model.safetensors:  92%|█████████████████████████████████████████████████    | 241M/261M [02:51<00:15, 1.24MB/s]model.safetensors:  97%|███████████████████████████████████████████████████▏ | 252M/261M [03:02<00:08, 1.13MB/s]model.safetensors: 100%|█████████████████████████████████████████████████████| 261M/261M [03:11<00:00, 1.09MB/s]model.safetensors: 100%|█████████████████████████████████████████████████████| 261M/261M [03:11<00:00, 1.36MB/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "./f00578_retrieve_relevant_documents.py", line 62, in <module>
    test_retrieve_relevant_documents()
  File "./f00578_retrieve_relevant_documents.py", line 45, in test_retrieve_relevant_documents
    assert retrieve_relevant_documents(query, documents) == expected_output
  File "./f00578_retrieve_relevant_documents.py", line 25, in retrieve_relevant_documents
    inputs = tokenizer(query, documents, truncation=True, padding='max_length')
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2798, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2904, in _call_one
    return self.encode_plus(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2977, in encode_plus
    return self._encode_plus(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 576, in _encode_plus
    batched_output = self._batch_encode_plus(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 504, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
TypeError: TextInputSequence must be str
