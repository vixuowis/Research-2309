spiece.model:   0%|                                                                  | 0.00/792k [00:00<?, ?B/s]spiece.model: 100%|███████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 841kB/s]spiece.model: 100%|███████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 841kB/s]
tokenizer.json:   0%|                                                               | 0.00/1.39M [00:00<?, ?B/s]tokenizer.json: 100%|██████████████████████████████████████████████████████| 1.39M/1.39M [00:00<00:00, 2.59MB/s]tokenizer.json: 100%|██████████████████████████████████████████████████████| 1.39M/1.39M [00:00<00:00, 2.59MB/s]
config.json:   0%|                                                                  | 0.00/1.21k [00:00<?, ?B/s]config.json: 100%|█████████████████████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 2.05MB/s]
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
model.safetensors:   0%|                                                             | 0.00/892M [00:00<?, ?B/s]model.safetensors:   1%|▌                                                   | 10.5M/892M [00:04<06:19, 2.32MB/s]model.safetensors:   2%|█▏                                                  | 21.0M/892M [00:07<04:57, 2.92MB/s]model.safetensors:   4%|█▊                                                  | 31.5M/892M [00:10<04:22, 3.28MB/s]model.safetensors:   5%|██▍                                                 | 41.9M/892M [00:13<04:31, 3.13MB/s]model.safetensors:   6%|███                                                 | 52.4M/892M [00:17<04:39, 3.00MB/s]model.safetensors:   7%|███▋                                                | 62.9M/892M [00:20<04:18, 3.20MB/s]model.safetensors:   8%|████▎                                               | 73.4M/892M [00:23<04:13, 3.23MB/s]model.safetensors:   9%|████▉                                               | 83.9M/892M [00:26<04:03, 3.31MB/s]model.safetensors:  11%|█████▌                                              | 94.4M/892M [00:30<04:19, 3.07MB/s]model.safetensors:  12%|██████▏                                              | 105M/892M [00:33<03:54, 3.35MB/s]model.safetensors:  13%|██████▊                                              | 115M/892M [00:35<03:39, 3.53MB/s]model.safetensors:  14%|███████▍                                             | 126M/892M [00:38<03:33, 3.60MB/s]model.safetensors:  15%|████████                                             | 136M/892M [00:42<03:46, 3.34MB/s]model.safetensors:  16%|████████▋                                            | 147M/892M [00:45<03:39, 3.40MB/s]model.safetensors:  18%|█████████▎                                           | 157M/892M [00:48<03:48, 3.21MB/s]model.safetensors:  19%|█████████▉                                           | 168M/892M [00:51<03:37, 3.32MB/s]model.safetensors:  20%|██████████▌                                          | 178M/892M [00:56<04:03, 2.93MB/s]model.safetensors:  21%|███████████▏                                         | 189M/892M [01:00<04:06, 2.86MB/s]model.safetensors:  22%|███████████▊                                         | 199M/892M [01:03<03:56, 2.92MB/s]model.safetensors:  24%|████████████▍                                        | 210M/892M [01:07<04:08, 2.74MB/s]model.safetensors:  25%|█████████████                                        | 220M/892M [01:12<04:12, 2.66MB/s]model.safetensors:  26%|█████████████▋                                       | 231M/892M [01:16<04:19, 2.55MB/s]model.safetensors:  27%|██████████████▎                                      | 241M/892M [01:21<04:37, 2.35MB/s]model.safetensors:  28%|██████████████▉                                      | 252M/892M [01:26<04:29, 2.37MB/s]model.safetensors:  29%|███████████████▌                                     | 262M/892M [01:30<04:26, 2.36MB/s]model.safetensors:  31%|████████████████▏                                    | 273M/892M [01:35<04:22, 2.35MB/s]model.safetensors:  32%|████████████████▊                                    | 283M/892M [01:39<04:24, 2.30MB/s]model.safetensors:  33%|█████████████████▍                                   | 294M/892M [01:44<04:24, 2.26MB/s]model.safetensors:  34%|██████████████████                                   | 304M/892M [01:48<04:09, 2.36MB/s]model.safetensors:  35%|██████████████████▋                                  | 315M/892M [01:54<04:17, 2.24MB/s]model.safetensors:  36%|███████████████████▎                                 | 325M/892M [02:00<04:50, 1.95MB/s]model.safetensors:  38%|███████████████████▉                                 | 336M/892M [02:06<04:41, 1.98MB/s]model.safetensors:  39%|████████████████████▌                                | 346M/892M [02:12<04:44, 1.92MB/s]model.safetensors:  40%|█████████████████████▏                               | 357M/892M [02:16<04:17, 2.08MB/s]model.safetensors:  41%|█████████████████████▊                               | 367M/892M [02:19<03:54, 2.24MB/s]model.safetensors:  42%|██████████████████████▍                              | 377M/892M [02:23<03:36, 2.38MB/s]model.safetensors:  44%|███████████████████████                              | 388M/892M [02:26<03:14, 2.59MB/s]model.safetensors:  45%|███████████████████████▋                             | 398M/892M [02:30<03:06, 2.64MB/s]model.safetensors:  46%|████████████████████████▎                            | 409M/892M [02:34<02:54, 2.77MB/s]model.safetensors:  47%|████████████████████████▉                            | 419M/892M [02:39<03:15, 2.41MB/s]model.safetensors:  48%|█████████████████████████▌                           | 430M/892M [02:44<03:13, 2.39MB/s]model.safetensors:  49%|██████████████████████████▏                          | 440M/892M [02:48<03:10, 2.36MB/s]model.safetensors:  51%|██████████████████████████▊                          | 451M/892M [02:55<03:36, 2.04MB/s]model.safetensors:  52%|███████████████████████████▍                         | 461M/892M [03:03<04:04, 1.76MB/s]model.safetensors:  53%|████████████████████████████                         | 472M/892M [03:16<05:23, 1.30MB/s]model.safetensors:  54%|████████████████████████████▋                        | 482M/892M [03:25<05:29, 1.24MB/s]model.safetensors:  54%|████████████████████████████▋                        | 482M/892M [03:36<05:29, 1.24MB/s]model.safetensors:  55%|█████████████████████████████▎                       | 493M/892M [03:40<06:30, 1.02MB/s]model.safetensors:  56%|██████████████████████████████▍                       | 503M/892M [03:52<06:43, 962kB/s]model.safetensors:  56%|██████████████████████████████▍                       | 503M/892M [04:06<06:43, 962kB/s]model.safetensors:  58%|███████████████████████████████                       | 514M/892M [04:14<08:34, 735kB/s]model.safetensors:  58%|███████████████████████████████                       | 514M/892M [04:26<08:34, 735kB/s]model.safetensors:  59%|███████████████████████████████▊                      | 524M/892M [04:40<10:23, 590kB/s]model.safetensors:  59%|███████████████████████████████▊                      | 524M/892M [04:56<10:23, 590kB/s]model.safetensors:  60%|████████████████████████████████▍                     | 535M/892M [05:02<10:42, 556kB/s]model.safetensors:  60%|████████████████████████████████▍                     | 535M/892M [05:16<10:42, 556kB/s]model.safetensors:  61%|█████████████████████████████████                     | 545M/892M [05:32<12:20, 468kB/s]model.safetensors:  61%|█████████████████████████████████                     | 545M/892M [05:46<12:20, 468kB/s]model.safetensors:  62%|█████████████████████████████████▋                    | 556M/892M [05:49<10:59, 510kB/s]model.safetensors:  62%|█████████████████████████████████▋                    | 556M/892M [06:06<10:59, 510kB/s]model.safetensors:  64%|██████████████████████████████████▎                   | 566M/892M [06:35<14:43, 368kB/s]model.safetensors:  64%|██████████████████████████████████▎                   | 566M/892M [06:46<14:43, 368kB/s]model.safetensors:  65%|██████████████████████████████████▉                   | 577M/892M [07:14<15:43, 334kB/s]model.safetensors:  65%|██████████████████████████████████▉                   | 577M/892M [07:26<15:43, 334kB/s]model.safetensors:  66%|███████████████████████████████████▌                  | 587M/892M [08:15<19:28, 260kB/s]model.safetensors:  66%|███████████████████████████████████▌                  | 587M/892M [08:26<19:28, 260kB/s]model.safetensors:  67%|████████████████████████████████████▏                 | 598M/892M [08:28<15:01, 326kB/s]model.safetensors:  68%|████████████████████████████████████▊                 | 608M/892M [08:45<12:23, 381kB/s]model.safetensors:  69%|█████████████████████████████████████▍                | 619M/892M [08:56<09:51, 461kB/s]model.safetensors:  69%|█████████████████████████████████████▍                | 619M/892M [09:06<09:51, 461kB/s]model.safetensors:  71%|██████████████████████████████████████                | 629M/892M [09:26<10:20, 423kB/s]model.safetensors:  71%|██████████████████████████████████████                | 629M/892M [09:36<10:20, 423kB/s]model.safetensors:  72%|██████████████████████████████████████▋               | 640M/892M [09:45<09:18, 451kB/s]model.safetensors:  72%|██████████████████████████████████████▋               | 640M/892M [09:56<09:18, 451kB/s]model.safetensors:  73%|███████████████████████████████████████▎              | 650M/892M [10:05<08:31, 472kB/s]model.safetensors:  74%|████████████████████████████████████████              | 661M/892M [10:15<06:48, 565kB/s]model.safetensors:  75%|████████████████████████████████████████▋             | 671M/892M [10:25<05:33, 661kB/s]model.safetensors:  75%|████████████████████████████████████████▋             | 671M/892M [10:36<05:33, 661kB/s]model.safetensors:  76%|█████████████████████████████████████████▎            | 682M/892M [10:37<04:58, 704kB/s]model.safetensors:  78%|█████████████████████████████████████████▉            | 692M/892M [10:46<04:06, 811kB/s]model.safetensors:  79%|██████████████████████████████████████████▌           | 703M/892M [10:52<03:16, 961kB/s]model.safetensors:  80%|██████████████████████████████████████████▍          | 713M/892M [10:56<02:31, 1.18MB/s]model.safetensors:  81%|███████████████████████████████████████████          | 724M/892M [11:01<02:03, 1.36MB/s]model.safetensors:  82%|███████████████████████████████████████████▋         | 734M/892M [11:05<01:39, 1.58MB/s]model.safetensors:  83%|████████████████████████████████████████████▎        | 744M/892M [11:09<01:20, 1.83MB/s]model.safetensors:  85%|████████████████████████████████████████████▉        | 755M/892M [11:12<01:04, 2.13MB/s]model.safetensors:  86%|█████████████████████████████████████████████▍       | 765M/892M [11:16<00:54, 2.31MB/s]model.safetensors:  87%|██████████████████████████████████████████████       | 776M/892M [11:22<00:57, 2.02MB/s]model.safetensors:  87%|██████████████████████████████████████████████       | 776M/892M [11:36<00:57, 2.02MB/s]model.safetensors:  88%|██████████████████████████████████████████████▋      | 786M/892M [11:38<01:22, 1.27MB/s]model.safetensors:  89%|███████████████████████████████████████████████▎     | 797M/892M [11:51<01:29, 1.06MB/s]model.safetensors:  91%|████████████████████████████████████████████████▉     | 807M/892M [12:03<01:24, 994kB/s]model.safetensors:  92%|████████████████████████████████████████████████▌    | 818M/892M [12:13<01:12, 1.02MB/s]model.safetensors:  93%|█████████████████████████████████████████████████▏   | 828M/892M [12:21<00:57, 1.11MB/s]model.safetensors:  94%|█████████████████████████████████████████████████▊   | 839M/892M [12:31<00:48, 1.09MB/s]model.safetensors:  95%|██████████████████████████████████████████████████▍  | 849M/892M [12:42<00:40, 1.04MB/s]model.safetensors:  96%|███████████████████████████████████████████████████  | 860M/892M [12:50<00:28, 1.10MB/s]model.safetensors:  98%|███████████████████████████████████████████████████▋ | 870M/892M [12:59<00:19, 1.11MB/s]model.safetensors:  99%|█████████████████████████████████████████████████████▎| 881M/892M [13:15<00:11, 919kB/s]model.safetensors:  99%|█████████████████████████████████████████████████████▎| 881M/892M [13:26<00:11, 919kB/s]model.safetensors: 100%|█████████████████████████████████████████████████████▉| 891M/892M [13:28<00:00, 891kB/s]model.safetensors: 100%|██████████████████████████████████████████████████████| 892M/892M [13:28<00:00, 892kB/s]model.safetensors: 100%|█████████████████████████████████████████████████████| 892M/892M [13:28<00:00, 1.10MB/s]
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
Traceback (most recent call last):
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/ssl.py", line 1073, in _create
    self.do_handshake()
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/ssl.py", line 1342, in do_handshake
    self._sslobj.do_handshake()
socket.timeout: _ssl.c:1114: The handshake operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-base/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', timeout('_ssl.c:1114: The handshake operation timed out')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./f00146_translate_english_to_german.py", line 38, in <module>
    test_translate_english_to_german()
  File "./f00146_translate_english_to_german.py", line 31, in test_translate_english_to_german
    assert translate_english_to_german('How old are you?') != ''
  File "./f00146_translate_english_to_german.py", line 17, in translate_english_to_german
    tokenizer = T5Tokenizer.from_pretrained('t5-base')
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1947, in from_pretrained
    resolved_config_file = cached_file(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-base/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', timeout('_ssl.c:1114: The handshake operation timed out')))"), '(Request ID: b649d258-59ba-4afe-bd5d-61a714e2c7a1)')
