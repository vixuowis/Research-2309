Downloading (…)lve/main/config.json:   0%|                               | 0.00/1.51k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████| 1.51k/1.51k [00:00<00:00, 207kB/s]
You are using a model of type blenderbot-small to instantiate a model of type blenderbot. This is not supported for all configurations of models and can yield errors.
Downloading pytorch_model.bin:   0%|                                      | 0.00/350M [00:00<?, ?B/s]Downloading pytorch_model.bin:   3%|▊                            | 10.5M/350M [00:01<01:00, 5.63MB/s]Downloading pytorch_model.bin:   6%|█▋                           | 21.0M/350M [00:02<00:34, 9.53MB/s]Downloading pytorch_model.bin:   9%|██▌                          | 31.5M/350M [00:03<00:26, 11.8MB/s]Downloading pytorch_model.bin:  12%|███▍                         | 41.9M/350M [00:03<00:23, 13.2MB/s]Downloading pytorch_model.bin:  15%|████▎                        | 52.4M/350M [00:04<00:20, 14.7MB/s]Downloading pytorch_model.bin:  18%|█████▏                       | 62.9M/350M [00:04<00:19, 14.9MB/s]Downloading pytorch_model.bin:  21%|██████                       | 73.4M/350M [00:05<00:21, 13.1MB/s]Downloading pytorch_model.bin:  24%|██████▉                      | 83.9M/350M [00:06<00:20, 12.9MB/s]Downloading pytorch_model.bin:  27%|███████▊                     | 94.4M/350M [00:07<00:19, 13.2MB/s]Downloading pytorch_model.bin:  30%|████████▉                     | 105M/350M [00:08<00:19, 12.8MB/s]Downloading pytorch_model.bin:  33%|█████████▉                    | 115M/350M [00:09<00:21, 11.1MB/s]Downloading pytorch_model.bin:  36%|██████████▊                   | 126M/350M [00:11<00:25, 8.95MB/s]Downloading pytorch_model.bin:  39%|███████████▋                  | 136M/350M [00:13<00:28, 7.50MB/s]Downloading pytorch_model.bin:  42%|████████████▌                 | 147M/350M [00:14<00:28, 7.17MB/s]Downloading pytorch_model.bin:  45%|█████████████▍                | 157M/350M [00:16<00:28, 6.89MB/s]Downloading pytorch_model.bin:  48%|██████████████▎               | 168M/350M [00:18<00:28, 6.52MB/s]Downloading pytorch_model.bin:  51%|███████████████▎              | 178M/350M [00:20<00:28, 6.14MB/s]Downloading pytorch_model.bin:  54%|████████████████▏             | 189M/350M [00:23<00:32, 4.96MB/s]Downloading pytorch_model.bin:  57%|█████████████████             | 199M/350M [00:26<00:33, 4.51MB/s]Downloading pytorch_model.bin:  60%|█████████████████▉            | 210M/350M [00:28<00:32, 4.31MB/s]Downloading pytorch_model.bin:  63%|██████████████████▊           | 220M/350M [00:31<00:29, 4.39MB/s]Downloading pytorch_model.bin:  66%|███████████████████▊          | 231M/350M [00:33<00:26, 4.54MB/s]Downloading pytorch_model.bin:  69%|████████████████████▋         | 241M/350M [00:35<00:22, 4.80MB/s]Downloading pytorch_model.bin:  72%|█████████████████████▌        | 252M/350M [00:38<00:22, 4.29MB/s]Downloading pytorch_model.bin:  75%|██████████████████████▍       | 262M/350M [00:40<00:21, 4.12MB/s]Downloading pytorch_model.bin:  78%|███████████████████████▎      | 273M/350M [00:43<00:19, 4.08MB/s]Downloading pytorch_model.bin:  81%|████████████████████████▏     | 283M/350M [00:46<00:16, 3.98MB/s]Downloading pytorch_model.bin:  84%|█████████████████████████▏    | 294M/350M [00:48<00:13, 4.21MB/s]Downloading pytorch_model.bin:  87%|██████████████████████████    | 304M/350M [00:50<00:10, 4.33MB/s]Downloading pytorch_model.bin:  90%|██████████████████████████▉   | 315M/350M [00:52<00:07, 4.56MB/s]Downloading pytorch_model.bin:  93%|███████████████████████████▊  | 325M/350M [00:54<00:05, 4.82MB/s]Downloading pytorch_model.bin:  96%|████████████████████████████▋ | 336M/350M [00:56<00:02, 5.25MB/s]Downloading pytorch_model.bin:  99%|█████████████████████████████▋| 346M/350M [00:57<00:00, 5.58MB/s]Downloading pytorch_model.bin: 100%|██████████████████████████████| 350M/350M [00:58<00:00, 5.79MB/s]Downloading pytorch_model.bin: 100%|██████████████████████████████| 350M/350M [00:58<00:00, 5.99MB/s]
Some weights of BlenderbotForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['model.encoder.layer_norm.weight', 'model.decoder.layer_norm.weight', 'model.encoder.layer_norm.bias', 'model.decoder.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)neration_config.json:   0%|                                 | 0.00/311 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████████████████████| 311/311 [00:00<00:00, 55.2kB/s]
Downloading (…)olve/main/vocab.json:   0%|                                | 0.00/964k [00:00<?, ?B/s]Downloading (…)olve/main/vocab.json: 100%|█████████████████████████| 964k/964k [00:01<00:00, 734kB/s]Downloading (…)olve/main/vocab.json: 100%|█████████████████████████| 964k/964k [00:01<00:00, 734kB/s]
Downloading (…)olve/main/merges.txt:   0%|                                | 0.00/345k [00:00<?, ?B/s]Downloading (…)olve/main/merges.txt: 100%|████████████████████████| 345k/345k [00:00<00:00, 2.46MB/s]Downloading (…)olve/main/merges.txt: 100%|████████████████████████| 345k/345k [00:00<00:00, 2.45MB/s]
Downloading (…)okenizer_config.json:   0%|                                 | 0.00/205 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|███████████████████████████| 205/205 [00:00<00:00, 206kB/s]
Downloading (…)cial_tokens_map.json:   0%|                                | 0.00/99.0 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|█████████████████████████| 99.0/99.0 [00:00<00:00, 118kB/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BlenderbotSmallTokenizer'. 
The class this function is called from is 'BlenderbotTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )
  warnings.warn(
Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00403_get_chatbot_response.py", line 36, in <module>
    test_get_chatbot_response()
  File "output/hf-eval-data-v2/f00403_get_chatbot_response.py", line 31, in test_get_chatbot_response
    response = get_chatbot_response(input_text)
  File "output/hf-eval-data-v2/f00403_get_chatbot_response.py", line 21, in get_chatbot_response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3550, in decode
    return self._decode(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 956, in _decode
    sub_texts.append(self.convert_tokens_to_string(current_sub_text))
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/blenderbot/tokenization_blenderbot.py", line 308, in convert_tokens_to_string
    text = bytearray([self.byte_decoder[c] for c in text]).decode("utf-8", errors=self.errors)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/blenderbot/tokenization_blenderbot.py", line 308, in <listcomp>
    text = bytearray([self.byte_decoder[c] for c in text]).decode("utf-8", errors=self.errors)
KeyError: 'ū'
