Downloading (…)rocessor_config.json:   0%|                                 | 0.00/432 [00:00<?, ?B/s]Downloading (…)rocessor_config.json: 100%|██████████████████████████| 432/432 [00:00<00:00, 61.6kB/s]
Downloading (…)okenizer_config.json:   0%|                                 | 0.00/904 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|███████████████████████████| 904/904 [00:00<00:00, 163kB/s]
Downloading (…)/main/tokenizer.json:   0%|                               | 0.00/2.11M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:01<00:00, 1.62MB/s]Downloading (…)/main/tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:01<00:00, 1.62MB/s]
Downloading (…)cial_tokens_map.json:   0%|                                 | 0.00/548 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|███████████████████████████| 548/548 [00:00<00:00, 527kB/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. 
The class this function is called from is 'BertTokenizerFast'.
Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00262_get_ingredients_info.py", line 42, in <module>
    test_get_ingredients_info()
  File "output/hf-eval-data-v2/f00262_get_ingredients_info.py", line 38, in test_get_ingredients_info
    print(get_ingredients_info(img_url, question))
  File "output/hf-eval-data-v2/f00262_get_ingredients_info.py", line 21, in get_ingredients_info
    processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/processing_utils.py", line 226, in from_pretrained
    args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/processing_utils.py", line 270, in _get_arguments_from_pretrained
    args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1854, in from_pretrained
    return cls._from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2017, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert_fast.py", line 235, in __init__
    normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())
AttributeError: 'NoneType' object has no attribute '__getstate__'
