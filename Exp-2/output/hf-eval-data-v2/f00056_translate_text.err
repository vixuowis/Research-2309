Downloading (…)lve/main/config.json:   0%|                                 | 0.00/846 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████████| 846/846 [00:00<00:00, 184kB/s]
Downloading pytorch_model.bin:   0%|                                     | 0.00/2.46G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|                           | 10.5M/2.46G [00:19<1:16:05, 537kB/s]Downloading pytorch_model.bin:   0%|                           | 10.5M/2.46G [00:37<1:16:05, 537kB/s]Downloading pytorch_model.bin:   1%|▏                          | 21.0M/2.46G [00:57<1:57:05, 347kB/s]Downloading pytorch_model.bin:   1%|▏                          | 21.0M/2.46G [01:17<1:57:05, 347kB/s]Downloading pytorch_model.bin:   1%|▎                          | 31.5M/2.46G [02:07<3:07:34, 216kB/s]Downloading pytorch_model.bin:   1%|▎                          | 31.5M/2.46G [02:27<3:07:34, 216kB/s]Downloading pytorch_model.bin:   1%|▎                          | 31.5M/2.46G [02:28<3:11:10, 212kB/s]
Downloading pytorch_model.bin:   0%|                                     | 0.00/2.46G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|                            | 10.5M/2.46G [00:09<38:12, 1.07MB/s]Downloading pytorch_model.bin:   1%|▏                            | 21.0M/2.46G [00:22<43:53, 926kB/s]Downloading pytorch_model.bin:   1%|▏                            | 21.0M/2.46G [00:32<43:53, 926kB/s]Downloading pytorch_model.bin:   1%|▎                          | 31.5M/2.46G [00:45<1:05:41, 616kB/s]Downloading pytorch_model.bin:   1%|▎                          | 31.5M/2.46G [01:02<1:05:41, 616kB/s]Downloading pytorch_model.bin:   2%|▍                          | 41.9M/2.46G [01:51<2:19:41, 289kB/s]Downloading pytorch_model.bin:   2%|▍                          | 41.9M/2.46G [02:02<2:19:41, 289kB/s]Downloading pytorch_model.bin:   2%|▌                          | 52.4M/2.46G [03:56<4:20:39, 154kB/s]Downloading pytorch_model.bin:   2%|▌                          | 52.4M/2.46G [04:04<3:07:30, 214kB/s]
Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00056_translate_text.py", line 42, in <module>
    test_translate_text()
  File "output/hf-eval-data-v2/f00056_translate_text.py", line 36, in test_translate_text
    translated_text = translate_text(text, 'en', 'fr')
  File "output/hf-eval-data-v2/f00056_translate_text.py", line 25, in translate_text
    translator = pipeline(f'translation_{source_lang}_to_{target_lang}', model='facebook/nllb-200-distilled-600M')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 824, in pipeline
    framework, model = infer_framework_load_model(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 282, in infer_framework_load_model
    raise ValueError(
ValueError: Could not load model facebook/nllb-200-distilled-600M with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 710, in _error_catcher
    yield
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 835, in _raw_read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
urllib3.exceptions.IncompleteRead: IncompleteRead(33316056 bytes read, 2427141871 more expected)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 816, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 940, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 911, in read
    data = self._raw_read(amt)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 835, in _raw_read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File "/root/miniconda3/envs/py38/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 727, in _error_catcher
    raise ProtocolError(f"Connection broken: {e!r}", e) from e
urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(33316056 bytes read, 2427141871 more expected)', IncompleteRead(33316056 bytes read, 2427141871 more expected))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 551, in http_get
    for chunk in r.iter_content(chunk_size=10 * 1024 * 1024):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 818, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(33316056 bytes read, 2427141871 more expected)', IncompleteRead(33316056 bytes read, 2427141871 more expected))

while loading with M2M100ForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 710, in _error_catcher
    yield
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 835, in _raw_read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
urllib3.exceptions.IncompleteRead: IncompleteRead(52975671 bytes read, 2407482256 more expected)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 816, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 940, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 911, in read
    data = self._raw_read(amt)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 835, in _raw_read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File "/root/miniconda3/envs/py38/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 727, in _error_catcher
    raise ProtocolError(f"Connection broken: {e!r}", e) from e
urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(52975671 bytes read, 2407482256 more expected)', IncompleteRead(52975671 bytes read, 2407482256 more expected))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 551, in http_get
    for chunk in r.iter_content(chunk_size=10 * 1024 * 1024):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 818, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(52975671 bytes read, 2407482256 more expected)', IncompleteRead(52975671 bytes read, 2407482256 more expected))



