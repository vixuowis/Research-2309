Downloading (…)okenizer_config.json:   0%|                                 | 0.00/590 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|███████████████████████████| 590/590 [00:00<00:00, 119kB/s]
Downloading (…)solve/main/vocab.txt:   0%|                                | 0.00/232k [00:00<?, ?B/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 232k/232k [00:00<00:00, 361kB/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 232k/232k [00:00<00:00, 361kB/s]
Downloading (…)/main/tokenizer.json:   0%|                                | 0.00/466k [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 1.30MB/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 1.30MB/s]
Downloading (…)cial_tokens_map.json:   0%|                                 | 0.00/112 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|███████████████████████████| 112/112 [00:00<00:00, 129kB/s]
Downloading (…)lve/main/config.json:   0%|                               | 0.00/2.76k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████| 2.76k/2.76k [00:00<00:00, 491kB/s]
Downloading pytorch_model.bin:   0%|                                      | 0.00/802M [00:00<?, ?B/s]Downloading pytorch_model.bin:   1%|▎                          | 10.5M/802M [02:20<2:56:29, 74.8kB/s]Downloading pytorch_model.bin:   1%|▎                          | 10.5M/802M [02:37<2:56:29, 74.8kB/s]Downloading pytorch_model.bin:   3%|▋                          | 21.0M/802M [05:20<3:23:11, 64.1kB/s]Downloading pytorch_model.bin:   3%|▋                          | 21.0M/802M [05:37<3:23:11, 64.1kB/s]Downloading pytorch_model.bin:   4%|█                          | 31.5M/802M [08:03<3:20:13, 64.2kB/s]Downloading pytorch_model.bin:   4%|█                          | 31.5M/802M [08:17<3:20:13, 64.2kB/s]Downloading pytorch_model.bin:   5%|█▍                         | 41.9M/802M [10:27<3:08:15, 67.3kB/s]Downloading pytorch_model.bin:   5%|█▍                         | 41.9M/802M [10:37<3:08:15, 67.3kB/s]Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 710, in _error_catcher
    yield
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 835, in _raw_read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
urllib3.exceptions.IncompleteRead: IncompleteRead(42829253 bytes read, 759433942 more expected)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 816, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 940, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 911, in read
    data = self._raw_read(amt)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 835, in _raw_read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File "/root/miniconda3/envs/py38/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 727, in _error_catcher
    raise ProtocolError(f"Connection broken: {e!r}", e) from e
urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(42829253 bytes read, 759433942 more expected)', IncompleteRead(42829253 bytes read, 759433942 more expected))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00100_get_answer_from_document.py", line 42, in <module>
    test_get_answer_from_document()
  File "output/hf-eval-data-v2/f00100_get_answer_from_document.py", line 36, in test_get_answer_from_document
    answer = get_answer_from_document(document_text, question_text)
  File "output/hf-eval-data-v2/f00100_get_answer_from_document.py", line 19, in get_answer_from_document
    model = AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 551, in http_get
    for chunk in r.iter_content(chunk_size=10 * 1024 * 1024):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 818, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(42829253 bytes read, 759433942 more expected)', IncompleteRead(42829253 bytes read, 759433942 more expected))
Downloading pytorch_model.bin:   5%|█▍                         | 41.9M/802M [10:38<3:12:51, 65.7kB/s]