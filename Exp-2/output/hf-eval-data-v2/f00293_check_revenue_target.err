Downloading (…)lve/main/config.json:   0%|                               | 0.00/1.66k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████| 1.66k/1.66k [00:00<00:00, 287kB/s]
Downloading pytorch_model.bin:   0%|                                      | 0.00/117M [00:00<?, ?B/s]Downloading pytorch_model.bin:   9%|██▌                          | 10.5M/117M [00:02<00:24, 4.38MB/s]Downloading pytorch_model.bin:  18%|█████▏                       | 21.0M/117M [00:03<00:13, 6.99MB/s]Downloading pytorch_model.bin:  27%|███████▊                     | 31.5M/117M [00:04<00:09, 8.59MB/s]Downloading pytorch_model.bin:  36%|██████████▍                  | 41.9M/117M [00:04<00:07, 9.87MB/s]Downloading pytorch_model.bin:  45%|████████████▉                | 52.4M/117M [00:05<00:06, 10.6MB/s]Downloading pytorch_model.bin:  54%|███████████████▌             | 62.9M/117M [00:06<00:04, 11.7MB/s]Downloading pytorch_model.bin:  63%|██████████████████▏          | 73.4M/117M [00:07<00:03, 12.1MB/s]Downloading pytorch_model.bin:  72%|████████████████████▊        | 83.9M/117M [00:08<00:02, 11.6MB/s]Downloading pytorch_model.bin:  81%|███████████████████████▎     | 94.4M/117M [00:09<00:01, 12.0MB/s]Downloading pytorch_model.bin:  89%|██████████████████████████▊   | 105M/117M [00:09<00:00, 12.4MB/s]Downloading pytorch_model.bin:  98%|█████████████████████████████▌| 115M/117M [00:10<00:00, 12.4MB/s]Downloading pytorch_model.bin: 100%|██████████████████████████████| 117M/117M [00:10<00:00, 12.1MB/s]Downloading pytorch_model.bin: 100%|██████████████████████████████| 117M/117M [00:10<00:00, 10.7MB/s]
Downloading (…)solve/main/vocab.txt:   0%|                                | 0.00/262k [00:00<?, ?B/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 262k/262k [00:00<00:00, 431kB/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 262k/262k [00:00<00:00, 430kB/s]
Downloading (…)cial_tokens_map.json:   0%|                                 | 0.00/154 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|███████████████████████████| 154/154 [00:00<00:00, 140kB/s]
Downloading (…)okenizer_config.json:   0%|                                 | 0.00/490 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|███████████████████████████| 490/490 [00:00<00:00, 638kB/s]
Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00293_check_revenue_target.py", line 42, in <module>
    test_check_revenue_target()
  File "output/hf-eval-data-v2/f00293_check_revenue_target.py", line 36, in test_check_revenue_target
    predicted_answer_coordinates, predicted_aggregation_indices = check_revenue_target(table, query)
  File "output/hf-eval-data-v2/f00293_check_revenue_target.py", line 21, in check_revenue_target
    inputs = tokenizer(table=table, queries=query, return_tensors='pt')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/tapas/tokenization_tapas.py", line 630, in __call__
    assert isinstance(table, pd.DataFrame), "Table must be of type pd.DataFrame"
AssertionError: Table must be of type pd.DataFrame
