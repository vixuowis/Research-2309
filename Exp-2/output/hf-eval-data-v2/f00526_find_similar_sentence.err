Downloading (…)okenizer_config.json:   0%|                                 | 0.00/514 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████████████████████| 514/514 [00:00<00:00, 72.3kB/s]
Downloading (…)solve/main/vocab.txt:   0%|                                | 0.00/110k [00:00<?, ?B/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 110k/110k [00:00<00:00, 319kB/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 110k/110k [00:00<00:00, 318kB/s]
/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py:980: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.44 MB. The target location /root/autodl-tmp/.cache/huggingface/hub only has 0.27 MB free disk space.
  warnings.warn(
/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py:980: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.44 MB. The target location /root/autodl-tmp/.cache/huggingface/hub/models--GanymedeNil--text2vec-large-chinese/blobs only has 0.27 MB free disk space.
  warnings.warn(
Downloading (…)/main/tokenizer.json:   0%|                                | 0.00/439k [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████| 439k/439k [00:00<00:00, 1.04MB/s]Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00526_find_similar_sentence.py", line 44, in <module>
    test_find_similar_sentence()
  File "output/hf-eval-data-v2/f00526_find_similar_sentence.py", line 40, in test_find_similar_sentence
    assert find_similar_sentence(source_sentence, sentences_to_compare) == '我爱吃苹果'
  File "output/hf-eval-data-v2/f00526_find_similar_sentence.py", line 20, in find_similar_sentence
    tokenizer = AutoTokenizer.from_pretrained('GanymedeNil/text2vec-large-chinese')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 736, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1813, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 554, in http_get
    temp_file.write(chunk)
  File "/root/miniconda3/envs/py38/lib/python3.8/tempfile.py", line 473, in func_wrapper
    return func(*args, **kwargs)
OSError: [Errno 28] No space left on device
Downloading (…)/main/tokenizer.json: 100%|█████████████████████████| 439k/439k [00:00<00:00, 885kB/s]