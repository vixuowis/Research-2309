Downloading (…)lve/main/config.json:   0%|                                 | 0.00/860 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████████| 860/860 [00:00<00:00, 226kB/s]
Downloading pytorch_model.bin:   0%|                                      | 0.00/499M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|▌                            | 10.5M/499M [00:02<01:37, 5.01MB/s]Downloading pytorch_model.bin:   4%|█▏                           | 21.0M/499M [00:02<00:59, 7.98MB/s]Downloading pytorch_model.bin:   6%|█▊                           | 31.5M/499M [00:03<00:46, 10.1MB/s]Downloading pytorch_model.bin:   8%|██▍                          | 41.9M/499M [00:04<00:41, 11.1MB/s]Downloading pytorch_model.bin:  11%|███                          | 52.4M/499M [00:04<00:35, 12.6MB/s]Downloading pytorch_model.bin:  13%|███▋                         | 62.9M/499M [00:05<00:31, 14.0MB/s]Downloading pytorch_model.bin:  15%|████▎                        | 73.4M/499M [00:06<00:28, 14.7MB/s]Downloading pytorch_model.bin:  17%|████▉                        | 83.9M/499M [00:06<00:27, 15.2MB/s]Downloading pytorch_model.bin:  19%|█████▍                       | 94.4M/499M [00:07<00:25, 15.8MB/s]Downloading pytorch_model.bin:  21%|██████▎                       | 105M/499M [00:08<00:31, 12.7MB/s]Downloading pytorch_model.bin:  23%|██████▉                       | 115M/499M [00:09<00:34, 11.1MB/s]Downloading pytorch_model.bin:  25%|███████▌                      | 126M/499M [00:12<00:51, 7.25MB/s]Downloading pytorch_model.bin:  27%|████████▏                     | 136M/499M [00:14<00:56, 6.43MB/s]Downloading pytorch_model.bin:  29%|████████▊                     | 147M/499M [00:16<00:54, 6.44MB/s]Downloading pytorch_model.bin:  32%|█████████▍                    | 157M/499M [00:17<00:49, 6.92MB/s]Downloading pytorch_model.bin:  34%|██████████                    | 168M/499M [00:18<00:44, 7.42MB/s]Downloading pytorch_model.bin:  36%|██████████▋                   | 178M/499M [00:19<00:39, 8.10MB/s]Downloading pytorch_model.bin:  38%|███████████▎                  | 189M/499M [00:20<00:34, 8.93MB/s]Downloading pytorch_model.bin:  40%|███████████▉                  | 199M/499M [00:21<00:32, 9.14MB/s]Downloading pytorch_model.bin:  42%|████████████▌                 | 210M/499M [00:22<00:29, 9.94MB/s]Downloading pytorch_model.bin:  44%|█████████████▏                | 220M/499M [00:23<00:24, 11.2MB/s]Downloading pytorch_model.bin:  46%|█████████████▉                | 231M/499M [00:23<00:22, 12.0MB/s]Downloading pytorch_model.bin:  48%|██████████████▌               | 241M/499M [00:24<00:21, 12.2MB/s]Downloading pytorch_model.bin:  50%|███████████████▏              | 252M/499M [00:25<00:19, 12.9MB/s]Downloading pytorch_model.bin:  53%|███████████████▊              | 262M/499M [00:26<00:19, 11.9MB/s]Downloading pytorch_model.bin:  55%|████████████████▍             | 273M/499M [00:27<00:18, 12.2MB/s]Downloading pytorch_model.bin:  57%|█████████████████             | 283M/499M [00:28<00:19, 11.2MB/s]Downloading pytorch_model.bin:  59%|█████████████████▋            | 294M/499M [00:29<00:18, 11.0MB/s]Downloading pytorch_model.bin:  61%|██████████████████▎           | 304M/499M [00:30<00:17, 10.9MB/s]Downloading pytorch_model.bin:  63%|██████████████████▉           | 315M/499M [00:31<00:17, 10.6MB/s]Downloading pytorch_model.bin:  65%|███████████████████▌          | 325M/499M [00:32<00:15, 11.2MB/s]Downloading pytorch_model.bin:  67%|████████████████████▏         | 336M/499M [00:32<00:13, 11.7MB/s]Downloading pytorch_model.bin:  69%|████████████████████▊         | 346M/499M [00:33<00:13, 11.4MB/s]Downloading pytorch_model.bin:  71%|█████████████████████▍        | 357M/499M [00:34<00:11, 12.0MB/s]Downloading pytorch_model.bin:  74%|██████████████████████        | 367M/499M [00:35<00:11, 11.9MB/s]Downloading pytorch_model.bin:  76%|██████████████████████▋       | 377M/499M [00:36<00:11, 10.3MB/s]Downloading pytorch_model.bin:  78%|███████████████████████▎      | 388M/499M [00:37<00:10, 10.8MB/s]Downloading pytorch_model.bin:  80%|███████████████████████▉      | 398M/499M [00:38<00:08, 11.5MB/s]Downloading pytorch_model.bin:  82%|████████████████████████▌     | 409M/499M [00:39<00:07, 11.6MB/s]Downloading pytorch_model.bin:  84%|█████████████████████████▏    | 419M/499M [00:40<00:06, 11.6MB/s]Downloading pytorch_model.bin:  86%|█████████████████████████▊    | 430M/499M [00:41<00:05, 12.4MB/s]Downloading pytorch_model.bin:  88%|██████████████████████████▍   | 440M/499M [00:41<00:04, 12.3MB/s]Downloading pytorch_model.bin:  90%|███████████████████████████▏  | 451M/499M [00:42<00:03, 12.5MB/s]Downloading pytorch_model.bin:  93%|███████████████████████████▊  | 461M/499M [00:43<00:02, 12.8MB/s]Downloading pytorch_model.bin:  95%|████████████████████████████▍ | 472M/499M [00:44<00:02, 12.9MB/s]Downloading pytorch_model.bin:  97%|█████████████████████████████ | 482M/499M [00:45<00:01, 13.1MB/s]Downloading pytorch_model.bin:  99%|█████████████████████████████▋| 493M/499M [00:45<00:00, 13.8MB/s]Downloading pytorch_model.bin: 100%|██████████████████████████████| 499M/499M [00:46<00:00, 14.1MB/s]Downloading pytorch_model.bin: 100%|██████████████████████████████| 499M/499M [00:46<00:00, 10.8MB/s]
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 261, in hf_raise_for_status
    response.raise_for_status()
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/RobertaTokenizer/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1344, in hf_hub_download
    raise head_call_error
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1230, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1606, in get_hf_file_metadata
    hf_raise_for_status(r)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 293, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-654d9302-111136e228c0b343194d2a1b;f6031e2f-57b9-4c00-9e02-f2ae36475dca)

Repository Not Found for url: https://huggingface.co/RobertaTokenizer/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00381_stock_sentiment_analysis.py", line 38, in <module>
    test_stock_sentiment_analysis()
  File "output/hf-eval-data-v2/f00381_stock_sentiment_analysis.py", line 29, in test_stock_sentiment_analysis
    sentiment_results = stock_sentiment_analysis(stock_comments)
  File "output/hf-eval-data-v2/f00381_stock_sentiment_analysis.py", line 18, in stock_sentiment_analysis
    classifier = pipeline('text-classification', model='zhayunduo/roberta-base-stocktwits-finetuned', tokenizer='RobertaTokenizer')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 921, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 686, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 519, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 450, in cached_file
    raise EnvironmentError(
OSError: RobertaTokenizer is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
