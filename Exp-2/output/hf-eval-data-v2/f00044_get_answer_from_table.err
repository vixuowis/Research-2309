Downloading (…)lve/main/config.json:   0%|                               | 0.00/1.55k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████| 1.55k/1.55k [00:00<00:00, 148kB/s]
Downloading pytorch_model.bin:   0%|                                      | 0.00/117M [00:00<?, ?B/s]Downloading pytorch_model.bin:   9%|██▌                          | 10.5M/117M [00:02<00:23, 4.63MB/s]Downloading pytorch_model.bin:  18%|█████▏                       | 21.0M/117M [00:02<00:12, 7.76MB/s]Downloading pytorch_model.bin:  27%|███████▊                     | 31.5M/117M [00:03<00:08, 10.1MB/s]Downloading pytorch_model.bin:  36%|██████████▍                  | 41.9M/117M [00:04<00:06, 10.8MB/s]Downloading pytorch_model.bin:  45%|████████████▉                | 52.4M/117M [00:05<00:06, 10.4MB/s]Downloading pytorch_model.bin:  45%|████████████▉                | 52.4M/117M [00:16<00:06, 10.4MB/s]Downloading pytorch_model.bin:  54%|███████████████▌             | 62.9M/117M [00:22<00:33, 1.63MB/s]Downloading pytorch_model.bin:  54%|███████████████▌             | 62.9M/117M [00:36<00:33, 1.63MB/s]Downloading pytorch_model.bin:  63%|██████████████████▊           | 73.4M/117M [00:53<00:59, 732kB/s]Downloading pytorch_model.bin:  63%|██████████████████▊           | 73.4M/117M [01:06<00:59, 732kB/s]Downloading pytorch_model.bin:  72%|█████████████████████▍        | 83.9M/117M [01:31<01:10, 475kB/s]Downloading pytorch_model.bin:  81%|████████████████████████▏     | 94.4M/117M [01:43<00:40, 557kB/s]Downloading pytorch_model.bin:  81%|████████████████████████▏     | 94.4M/117M [01:56<00:40, 557kB/s]Downloading pytorch_model.bin:  89%|███████████████████████████▋   | 105M/117M [02:03<00:22, 550kB/s]Downloading pytorch_model.bin:  89%|███████████████████████████▋   | 105M/117M [02:16<00:22, 550kB/s]Downloading pytorch_model.bin:  98%|██████████████████████████████▌| 115M/117M [02:29<00:03, 490kB/s]Downloading pytorch_model.bin: 100%|███████████████████████████████| 117M/117M [02:35<00:00, 472kB/s]Downloading pytorch_model.bin: 100%|███████████████████████████████| 117M/117M [02:35<00:00, 754kB/s]
Downloading (…)okenizer_config.json:   0%|                                 | 0.00/490 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|███████████████████████████| 490/490 [00:00<00:00, 468kB/s]
Downloading (…)solve/main/vocab.txt:   0%|                                | 0.00/262k [00:00<?, ?B/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 262k/262k [00:00<00:00, 362kB/s]Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████| 262k/262k [00:00<00:00, 361kB/s]
Downloading (…)cial_tokens_map.json:   0%|                                 | 0.00/154 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|███████████████████████████| 154/154 [00:00<00:00, 153kB/s]
Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00044_get_answer_from_table.py", line 36, in <module>
    test_get_answer_from_table()
  File "output/hf-eval-data-v2/f00044_get_answer_from_table.py", line 31, in test_get_answer_from_table
    answer = get_answer_from_table(question, table_data)
  File "output/hf-eval-data-v2/f00044_get_answer_from_table.py", line 20, in get_answer_from_table
    answer = table_qa(question=question, table=table_data)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/table_question_answering.py", line 345, in __call__
    pipeline_inputs = self._args_parser(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/table_question_answering.py", line 70, in __call__
    raise ValueError(
ValueError: Invalid input. Keyword argument `table` should be either of type `dict` or `list`, but is <class 'dict'>)
