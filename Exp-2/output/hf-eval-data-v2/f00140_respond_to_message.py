# function_import --------------------

from transformers import AutoModelForCausalLM, AutoTokenizer

# function_code --------------------

def respond_to_message(input_message: str) -> str:
    """
    This function takes an input message as a string, processes it using the pre-trained
    'facebook/blenderbot-90M' model from Hugging Face Transformers, and returns a response
    generated by the model.

    Args:
        input_message (str): The input message that needs to be processed.

    Returns:
        str: The response generated by the model.
    """
    tokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')
    model = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')
    tokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')
    output = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)
    return response

# test_function_code --------------------

def test_respond_to_message():
    """
    This function tests the 'respond_to_message' function by providing a sample input message
    and checking if the output is a string.
    """
    input_message = 'Turn on the air conditioner.'
    response = respond_to_message(input_message)
    assert isinstance(response, str), 'The output should be a string.'

# call_test_function_code --------------------

test_respond_to_message()