{"path": "output/hf-eval-data-v3-valid/f00672_find_relevant_passage.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n# test_function_code --------------------\n\ndef test_find_relevant_passage():\n    \"\"\"\n    Test the function find_relevant_passage.\n    \"\"\"\n    question = 'How many people live in Berlin?'\n    candidate_passages = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert isinstance(find_relevant_passage(question, candidate_passages), str)\n    question = 'What is the capital of Germany?'\n    candidate_passages = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert find_relevant_passage(question, candidate_passages) == 'Berlin is the capital of Germany.'\n    question = 'Who won the world cup in 2014?'\n    candidate_passages = ['Germany won the world cup in 2014.', 'Brazil hosted the world cup in 2014.']\n    assert find_relevant_passage(question, candidate_passages) == 'Germany won the world cup in 2014.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_find_relevant_passage()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_find_relevant_passage():\n    \"\"\"\n    Test the function find_relevant_passage.\n    \"\"\"\n    question = 'How many people live in Berlin?'\n    candidate_passages = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert isinstance(find_relevant_passage(question, candidate_passages), str)\n    question = 'What is the capital of Germany?'\n    candidate_passages = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert find_relevant_passage(question, candidate_passages) == 'Berlin is the capital of Germany.'\n    question = 'Who won the world cup in 2014?'\n    candidate_passages = ['Germany won the world cup in 2014.', 'Brazil hosted the world cup in 2014.']\n    assert find_relevant_passage(question, candidate_passages) == 'Germany won the world cup in 2014.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_find_relevant_passage()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n", "prediction": "\n    \n    # Initialize our model and tokenizer\n    try:\n        \n        tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", return_tensors=\"pt\")\n        model = AutoModelForSequenceClassification.from_pretrained(\"deepset/roberta-base-squad2\")\n        \n    except OSError as ose:\n    \n        print(f\"Problem with loading the pre-trained model or tokenizer: {ose}\")\n        return \"\"\n            \n    # Tokenize our inputs and format them for PyTorch\n    question = [question] * len(candidate_passages)\n    inputs = tokenizer.encode_plus(question, candidate_passages, add_special_tokens=True, max_length=512, return_tensors='pt')\n    \n    # Get our input ids and attention masks\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n    \n    # Set the batch size to 1 so we can add more samples if needed (relevant for production)\n    batch_size = 1\n    \n    # Wrap our tensors in a PyTorch DataLoader\n    data = TensorDataset(input_ids, attention_mask)\n    sampler = SequentialSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n    \n    # Run through our model and get the predicted start/end token for the answer\n    all_results = []\n    for batch in tqdm(dataloader):\n        \n        input_ids, attention_mask = batch\n            \n            with torch.no_grad():\n                outputs = model(input_ids, attention_mask)\n                start_logits, end_logits = outputs[0], outputs[1]\n                    \n                # Convert the predicted token indices into text\n                tokens = input_ids[0].cpu().numpy()\n                \n                # Decode the output\n                all_tokens = tokenizer.convert_ids_to_tokens(tokens)"}