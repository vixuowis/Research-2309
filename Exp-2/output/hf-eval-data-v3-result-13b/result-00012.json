{"path": "output/hf-eval-data-v3-valid/f00060_summarize_text.py", "content": "# function_import --------------------\n\nfrom transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n\n# function_code --------------------\n\ndef summarize_text(text):\n    \"\"\"\n    Summarizes a given long text using BigBird Pegasus model.\n\n    Args:\n        text (str): The long text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-bigpatent')\n    model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')\n    inputs = tokenizer(text, return_tensors='pt')\n    prediction = model.generate(**inputs)\n    summary = tokenizer.batch_decode(prediction)[0]\n    return summary\n\n# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"\n    Tests the summarize_text function with some test cases.\n    \"\"\"\n    test_text1 = 'This is a long text that needs to be summarized. It contains many details that are not necessary for understanding the main idea.'\n    test_text2 = 'Another long text that needs summarization. It also contains many unnecessary details.'\n    assert len(summarize_text(test_text1)) < len(test_text1)\n    assert len(summarize_text(test_text2)) < len(test_text2)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_summarize_text()", "function_import": "# function_import --------------------\n\nfrom transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n\n", "function_code": "# function_code --------------------\n\ndef summarize_text(text):\n    \"\"\"\n    Summarizes a given long text using BigBird Pegasus model.\n\n    Args:\n        text (str): The long text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-bigpatent')\n    model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')\n    inputs = tokenizer(text, return_tensors='pt')\n    prediction = model.generate(**inputs)\n    summary = tokenizer.batch_decode(prediction)[0]\n    return summary\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"\n    Tests the summarize_text function with some test cases.\n    \"\"\"\n    test_text1 = 'This is a long text that needs to be summarized. It contains many details that are not necessary for understanding the main idea.'\n    test_text2 = 'Another long text that needs summarization. It also contains many unnecessary details.'\n    assert len(summarize_text(test_text1)) < len(test_text1)\n    assert len(summarize_text(test_text2)) < len(test_text2)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_summarize_text()", "instruct": "# function_import --------------------\n\nfrom transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n\n# function_code --------------------\n\ndef summarize_text(text):\n    \"\"\"\n    Summarizes a given long text using BigBird Pegasus model.\n\n    Args:\n        text (str): The long text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"", "answer": "\n    tokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-bigpatent')\n    model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')\n    inputs = tokenizer(text, return_tensors='pt')\n    prediction = model.generate(**inputs)\n    summary = tokenizer.batch_decode(prediction)[0]\n    return summary\n\n", "prediction": "    \n    # Load the model and tokenizer from Huggingface Hub.\n    print('Loading models...')\n    model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n    tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n    \n    # Tokenize the text with truncation and padding.\n    print('Tokenizing input...')\n    inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\",truncation=True,padding='max_length')\n\n    # Generate summarization using beam search decoding with alpha parameter of 0.95.\n    print('Generating summary...')\n    outputs = model.generate(\n        inputs['input_ids'], attention_mask=inputs['attention_mask'], num_beams=3, max_length=256, min_length=100, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True\n    )\n    \n    # De-tokenize the output to get human readable text and return it.\n    print('Converting to string...')\n    return tokenizer.batch_decode(outputs)[0]"}