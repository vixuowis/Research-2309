{"path": "output/hf-eval-data-v3-valid/f00143_generate_code.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# function_code --------------------\n\ndef generate_code(description: str) -> str:\n    '''\n    Generate code snippets based on natural language descriptions.\n\n    Args:\n        description (str): The natural language description.\n\n    Returns:\n        str: The generated code snippet.\n    '''\n    tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-2B-multi')\n    model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\n    input_ids = tokenizer(description, return_tensors='pt').input_ids\n    generated_ids = model.generate(input_ids, max_length=128)\n    generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    return generated_code\n\n# test_function_code --------------------\n\ndef test_generate_code():\n    '''\n    Test the generate_code function.\n    '''\n    description1 = 'Write a Python function to calculate the factorial of a number.'\n    description2 = 'Write a Python function to sort a list of numbers in ascending order.'\n    description3 = 'Write a Python function to reverse a string.'\n    assert isinstance(generate_code(description1), str)\n    assert isinstance(generate_code(description2), str)\n    assert isinstance(generate_code(description3), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_generate_code()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n", "function_code": "# function_code --------------------\n\ndef generate_code(description: str) -> str:\n    '''\n    Generate code snippets based on natural language descriptions.\n\n    Args:\n        description (str): The natural language description.\n\n    Returns:\n        str: The generated code snippet.\n    '''\n    tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-2B-multi')\n    model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\n    input_ids = tokenizer(description, return_tensors='pt').input_ids\n    generated_ids = model.generate(input_ids, max_length=128)\n    generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    return generated_code\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_code():\n    '''\n    Test the generate_code function.\n    '''\n    description1 = 'Write a Python function to calculate the factorial of a number.'\n    description2 = 'Write a Python function to sort a list of numbers in ascending order.'\n    description3 = 'Write a Python function to reverse a string.'\n    assert isinstance(generate_code(description1), str)\n    assert isinstance(generate_code(description2), str)\n    assert isinstance(generate_code(description3), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_code()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# function_code --------------------\n\ndef generate_code(description: str) -> str:\n    '''\n    Generate code snippets based on natural language descriptions.\n\n    Args:\n        description (str): The natural language description.\n\n    Returns:\n        str: The generated code snippet.\n    '''", "answer": "\n    tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-2B-multi')\n    model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\n    input_ids = tokenizer(description, return_tensors='pt').input_ids\n    generated_ids = model.generate(input_ids, max_length=128)\n    generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    return generated_code\n\n", "prediction": "\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n\n    # encode the new user input\n    chat_history_ids = tokenizer.encode(description, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids], dim=-1) if tokenizer is not None else None\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(bot_input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\n\n    # pretty print last ouput tokens from bot\n    return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)"}