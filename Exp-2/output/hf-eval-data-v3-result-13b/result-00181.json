{"path": "output/hf-eval-data-v3-valid/f00730_extract_features.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# function_code --------------------\n\ndef extract_features(source_code_text):\n    \"\"\"\n    Extracts features from the given source code text using the 'microsoft/unixcoder-base' model.\n\n    Args:\n        source_code_text (str): The source code text to extract features from.\n\n    Returns:\n        torch.Tensor: The feature matrix derived as a matrix of embeddings.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n    model = AutoModel.from_pretrained('microsoft/unixcoder-base')\n    inputs = tokenizer(source_code_text, return_tensors='pt')\n    outputs = model(**inputs)\n    feature_matrix = outputs.last_hidden_state\n    return feature_matrix\n\n# test_function_code --------------------\n\ndef test_extract_features():\n    \"\"\"\n    Tests the 'extract_features' function.\n    \"\"\"\n    source_code_text = '/* Your source code here */'\n    feature_matrix = extract_features(source_code_text)\n    assert isinstance(feature_matrix, torch.Tensor), 'The output should be a torch.Tensor.'\n    print('All Tests Passed')\n\n# call_test_function_code --------------------\n\nif __name__ == '__main__':\n    test_extract_features()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n", "function_code": "# function_code --------------------\n\ndef extract_features(source_code_text):\n    \"\"\"\n    Extracts features from the given source code text using the 'microsoft/unixcoder-base' model.\n\n    Args:\n        source_code_text (str): The source code text to extract features from.\n\n    Returns:\n        torch.Tensor: The feature matrix derived as a matrix of embeddings.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n    model = AutoModel.from_pretrained('microsoft/unixcoder-base')\n    inputs = tokenizer(source_code_text, return_tensors='pt')\n    outputs = model(**inputs)\n    feature_matrix = outputs.last_hidden_state\n    return feature_matrix\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_features():\n    \"\"\"\n    Tests the 'extract_features' function.\n    \"\"\"\n    source_code_text = '/* Your source code here */'\n    feature_matrix = extract_features(source_code_text)\n    assert isinstance(feature_matrix, torch.Tensor), 'The output should be a torch.Tensor.'\n    print('All Tests Passed')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nif __name__ == '__main__':\n    test_extract_features()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# function_code --------------------\n\ndef extract_features(source_code_text):\n    \"\"\"\n    Extracts features from the given source code text using the 'microsoft/unixcoder-base' model.\n\n    Args:\n        source_code_text (str): The source code text to extract features from.\n\n    Returns:\n        torch.Tensor: The feature matrix derived as a matrix of embeddings.\n    \"\"\"", "answer": "\n    tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n    model = AutoModel.from_pretrained('microsoft/unixcoder-base')\n    inputs = tokenizer(source_code_text, return_tensors='pt')\n    outputs = model(**inputs)\n    feature_matrix = outputs.last_hidden_state\n    return feature_matrix\n\n", "prediction": "\n    \n    # Get model tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n    model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\", return_dict=False)\n\n    # Tokenize the source code\n    tokens = tokenizer(source_code_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n    \n    with torch.no_grad():\n        # Compute model output and embeddings\n        outputs = model(**tokens)\n        embedding = outputs[1]\n        \n        # Get feature matrix (embedding matrix of the last hidden-state, averaged across token sequence)\n        feature_matrix = torch.mean(embedding, dim=1).squeeze()\n    \n    return feature_matrix"}