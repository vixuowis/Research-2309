{"path": "output/hf-eval-data-v3-valid/f00826_encode_sentences.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import BertModel, BertTokenizerFast\n\n# function_code --------------------\n\ndef encode_sentences(sentences):\n    \"\"\"\n    Encode sentences using the pre-trained LaBSE model.\n\n    Args:\n        sentences (list): A list of sentences to be encoded.\n\n    Returns:\n        torch.Tensor: The encoded sentences.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\n    model = BertModel.from_pretrained('setu4993/LaBSE')\n    model = model.eval()\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.pooler_output\n    return embeddings\n\n# test_function_code --------------------\n\ndef test_encode_sentences():\n    \"\"\"\n    Test the encode_sentences function.\n    \"\"\"\n    sentences = [\n        'dog',\n        'Cuccioli sono carini.',\n        '\u72ac\u3068\u4e00\u7dd2\u306b\u30d3\u30fc\u30c1\u3092\u6563\u6b69\u3059\u308b\u306e\u304c\u597d\u304d',\n    ]\n    embeddings = encode_sentences(sentences)\n    assert embeddings.shape[0] == len(sentences), 'The number of embeddings should be equal to the number of sentences.'\n    assert embeddings.shape[1] == 768, 'The dimension of each embedding should be 768.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_encode_sentences()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import BertModel, BertTokenizerFast\n\n", "function_code": "# function_code --------------------\n\ndef encode_sentences(sentences):\n    \"\"\"\n    Encode sentences using the pre-trained LaBSE model.\n\n    Args:\n        sentences (list): A list of sentences to be encoded.\n\n    Returns:\n        torch.Tensor: The encoded sentences.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\n    model = BertModel.from_pretrained('setu4993/LaBSE')\n    model = model.eval()\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.pooler_output\n    return embeddings\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_encode_sentences():\n    \"\"\"\n    Test the encode_sentences function.\n    \"\"\"\n    sentences = [\n        'dog',\n        'Cuccioli sono carini.',\n        '\u72ac\u3068\u4e00\u7dd2\u306b\u30d3\u30fc\u30c1\u3092\u6563\u6b69\u3059\u308b\u306e\u304c\u597d\u304d',\n    ]\n    embeddings = encode_sentences(sentences)\n    assert embeddings.shape[0] == len(sentences), 'The number of embeddings should be equal to the number of sentences.'\n    assert embeddings.shape[1] == 768, 'The dimension of each embedding should be 768.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_encode_sentences()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import BertModel, BertTokenizerFast\n\n# function_code --------------------\n\ndef encode_sentences(sentences):\n    \"\"\"\n    Encode sentences using the pre-trained LaBSE model.\n\n    Args:\n        sentences (list): A list of sentences to be encoded.\n\n    Returns:\n        torch.Tensor: The encoded sentences.\n    \"\"\"", "answer": "\n    tokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\n    model = BertModel.from_pretrained('setu4993/LaBSE')\n    model = model.eval()\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.pooler_output\n    return embeddings\n\n", "prediction": "\n    tokenizer = BertTokenizerFast.from_pretrained(\"sentence-transformers/LaBSE\")\n    model = BertModel.from_pretrained(\n            \"sentence-transformers/LaBSE\", \n            return_dict=True)\n\n    encoding = tokenizer(sentences, padding='max_length', max_length=512, truncation=True)\n    \n    input_ids = torch.LongTensor([encoding['input_ids']]).squeeze()\n    attention_mask = torch.LongTensor([encoding['attention_mask']]).squeeze()\n\n    with torch.no_grad():\n        output, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n    return output"}