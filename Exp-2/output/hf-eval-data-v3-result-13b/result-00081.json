{"path": "output/hf-eval-data-v3-valid/f00349_generate_image_caption.py", "content": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n# test_function_code --------------------\n\ndef test_generate_image_caption():\n    \"\"\"\n    Test the function generate_image_caption.\n    \"\"\"\n    assert isinstance(generate_image_caption('test_image.jpg', 'product photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'landscape photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'portrait photography'), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_generate_image_caption())", "function_import": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n", "function_code": "# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_image_caption():\n    \"\"\"\n    Test the function generate_image_caption.\n    \"\"\"\n    assert isinstance(generate_image_caption('test_image.jpg', 'product photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'landscape photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'portrait photography'), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_generate_image_caption())", "instruct": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"", "answer": "\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n", "prediction": "\n    try:\n      img = Image.open(image_path)\n    except FileNotFoundError as e:\n      raise Exception(\"Image file not found.\") from e\n    except AttributeError as e:\n      raise TypeError('Invalid image path') from e \n      \n    # Load the blip processor and model for CAPTION generation.\n    processor = BlipProcessor.from_pretrained(\"models/blip-tokenizer\")\n    \n    inputs = processor(\n        text=[text],\n        images=img,\n        return_tensors=\"pt\",\n        padding='max_length',\n        max_length=128,\n        truncation=True,\n    )\n\n    # Load the blip model for CAPTION generation.\n    model = BlipForConditionalGeneration.from_pretrained(\"models/blip\")\n    \n    # Generate the caption using the model.\n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        images=inputs.pixel_values,\n        max_length=128,\n        num_beams=4,\n    )    \n\n    return processor.batch_decode(outputs)[0] "}