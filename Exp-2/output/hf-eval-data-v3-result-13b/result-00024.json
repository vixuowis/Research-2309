{"path": "output/hf-eval-data-v3-valid/f00126_get_best_answer.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef get_best_answer(query: str, passages: list) -> str:\n    \"\"\"\n    Given a query and a list of passages, this function returns the passage that best answers the query.\n    \n    Args:\n        query (str): The question to be answered.\n        passages (list): A list of possible answer passages.\n    \n    Returns:\n        str: The passage that best answers the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    features = tokenizer([query]*len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_passages = [passages[idx] for idx in scores.argsort(descending=True)]\n    best_passage = sorted_passages[0]\n    return best_passage\n\n# test_function_code --------------------\n\ndef test_get_best_answer():\n    \"\"\"\n    Test the function get_best_answer.\n    \"\"\"\n    query = 'What is the capital of France?'\n    passages = ['Paris is the capital of France.', 'London is the capital of England.', 'Berlin is the capital of Germany.']\n    assert get_best_answer(query, passages) == 'Paris is the capital of France.'\n    \n    query = 'Who won the world cup in 2018?'\n    passages = ['France won the world cup in 2018.', 'Germany won the world cup in 2014.', 'Brazil won the world cup in 2002.']\n    assert get_best_answer(query, passages) == 'France won the world cup in 2018.'\n    \n    query = 'Who is the CEO of Tesla?'\n    passages = ['Elon Musk is the CEO of Tesla.', 'Bill Gates is the CEO of Microsoft.', 'Jeff Bezos is the CEO of Amazon.']\n    assert get_best_answer(query, passages) == 'Elon Musk is the CEO of Tesla.'\n    \n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_get_best_answer()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef get_best_answer(query: str, passages: list) -> str:\n    \"\"\"\n    Given a query and a list of passages, this function returns the passage that best answers the query.\n    \n    Args:\n        query (str): The question to be answered.\n        passages (list): A list of possible answer passages.\n    \n    Returns:\n        str: The passage that best answers the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    features = tokenizer([query]*len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_passages = [passages[idx] for idx in scores.argsort(descending=True)]\n    best_passage = sorted_passages[0]\n    return best_passage\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_get_best_answer():\n    \"\"\"\n    Test the function get_best_answer.\n    \"\"\"\n    query = 'What is the capital of France?'\n    passages = ['Paris is the capital of France.', 'London is the capital of England.', 'Berlin is the capital of Germany.']\n    assert get_best_answer(query, passages) == 'Paris is the capital of France.'\n    \n    query = 'Who won the world cup in 2018?'\n    passages = ['France won the world cup in 2018.', 'Germany won the world cup in 2014.', 'Brazil won the world cup in 2002.']\n    assert get_best_answer(query, passages) == 'France won the world cup in 2018.'\n    \n    query = 'Who is the CEO of Tesla?'\n    passages = ['Elon Musk is the CEO of Tesla.', 'Bill Gates is the CEO of Microsoft.', 'Jeff Bezos is the CEO of Amazon.']\n    assert get_best_answer(query, passages) == 'Elon Musk is the CEO of Tesla.'\n    \n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_get_best_answer()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef get_best_answer(query: str, passages: list) -> str:\n    \"\"\"\n    Given a query and a list of passages, this function returns the passage that best answers the query.\n    \n    Args:\n        query (str): The question to be answered.\n        passages (list): A list of possible answer passages.\n    \n    Returns:\n        str: The passage that best answers the query.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    features = tokenizer([query]*len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_passages = [passages[idx] for idx in scores.argsort(descending=True)]\n    best_passage = sorted_passages[0]\n    return best_passage\n\n", "prediction": "\n    \n    # Initialize the model and tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-answering-squadv2\")\n    model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/t5-base-finetuned-question-answering-squadv2\").to(0)\n    \n    # Tokenize the query and all passages\n    inputs = tokenizer([[query + ' ' + passage for passage in passages]], return_tensors='pt', max_length=384, padding=True).to(0)\n    outputs = model(**inputs)[0][:, 1]\n    \n    # Get the index of the best answer\n    index = torch.argmax(outputs)\n    \n    return passages[index]"}