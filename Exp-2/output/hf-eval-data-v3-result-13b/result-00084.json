{"path": "output/hf-eval-data-v3-valid/f00364_detect_kitchen_objects.py", "content": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n# test_function_code --------------------\n\ndef test_detect_kitchen_objects():\n    \"\"\"\n    Tests the detect_kitchen_objects function.\n    \"\"\"\n    try:\n        detect_kitchen_objects('test_image.jpg', 0.1)\n        print('Test passed')\n    except Exception as e:\n        print(f'Test failed: {str(e)}')\n\n# call_test_function_code --------------------\n\ntest_detect_kitchen_objects()", "function_import": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n", "function_code": "# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_detect_kitchen_objects():\n    \"\"\"\n    Tests the detect_kitchen_objects function.\n    \"\"\"\n    try:\n        detect_kitchen_objects('test_image.jpg', 0.1)\n        print('Test passed')\n    except Exception as e:\n        print(f'Test failed: {str(e)}')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_detect_kitchen_objects()", "instruct": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"", "answer": "\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "prediction": "\n    # Load the pretrained model from Hugging Face.\n    try:\n        processor = OwlViTProcessor.from_pretrained(\"dandelin/owlvit-object-classification\")\n        model = OwlViTForObjectDetection.from_pretrained(\"dandelin/owlvit-object-classification\")\n    except:\n        raise RuntimeError(\"Could not load the pretrained model.\")\n\n    # Load and process the image file.\n    if os.path.isfile(image_path):\n        try:\n            pil_img = Image.open(image_path).convert('RGB')\n        except:\n            raise RuntimeError(\"Could not load the image file.\")\n    else:\n        raise FileNotFoundError(\"Image file does not exist\")\n    \n    # Make the prediction and print results to console.\n    pixel_values = processor(pil_img, return_tensors=\"pt\").pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits[0][\"logits\"]\n    \n    if torch.numel(logits) == 1:\n        print(\"No objects detected.\")\n    else:\n        print(\"Objects found:\\n\")\n        \n        for i in range(torch.numel(logits)):\n            score = logits[i]\n            \n            if float(score) > score_threshold:\n                class_name = processor.id2label[int(i / 80)]\n                \n                print(\"{}: {}\".format(class_name, round((float(score)*100), 3)))"}