{"path": "output/hf-eval-data-v3-valid/f00575_location_recommendation.py", "content": "# function_import --------------------\n\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\n# function_code --------------------\n\ndef location_recommendation(image_url: str, choices: list):\n    \"\"\"\n    This function uses the StreetCLIP model to generate probabilities for various cities based on images from potential locations.\n    It identifies possible locations for new stores.\n\n    Args:\n        image_url (str): The URL of the image of the potential location.\n        choices (list): A list of city options to classify images.\n\n    Returns:\n        dict: A dictionary with city names as keys and their corresponding probabilities as values.\n    \"\"\"\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    probs = probs.tolist()[0]\n    return {city: prob for city, prob in zip(choices, probs)}\n\n# test_function_code --------------------\n\ndef test_location_recommendation():\n    \"\"\"\n    This function tests the location_recommendation function with different test cases.\n    \"\"\"\n    test_case_1 = ('https://placekitten.com/200/300', ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'])\n    result_1 = location_recommendation(*test_case_1)\n    assert isinstance(result_1, dict), 'The result should be a dictionary.'\n    assert len(result_1) == len(test_case_1[1]), 'The number of cities in the result should be equal to the number of choices.'\n\n    test_case_2 = ('https://placekitten.com/200/300', ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco'])\n    result_2 = location_recommendation(*test_case_2)\n    assert isinstance(result_2, dict), 'The result should be a dictionary.'\n    assert len(result_2) == len(test_case_2[1]), 'The number of cities in the result should be equal to the number of choices.'\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_location_recommendation()", "function_import": "# function_import --------------------\n\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\n", "function_code": "# function_code --------------------\n\ndef location_recommendation(image_url: str, choices: list):\n    \"\"\"\n    This function uses the StreetCLIP model to generate probabilities for various cities based on images from potential locations.\n    It identifies possible locations for new stores.\n\n    Args:\n        image_url (str): The URL of the image of the potential location.\n        choices (list): A list of city options to classify images.\n\n    Returns:\n        dict: A dictionary with city names as keys and their corresponding probabilities as values.\n    \"\"\"\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    probs = probs.tolist()[0]\n    return {city: prob for city, prob in zip(choices, probs)}\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_location_recommendation():\n    \"\"\"\n    This function tests the location_recommendation function with different test cases.\n    \"\"\"\n    test_case_1 = ('https://placekitten.com/200/300', ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'])\n    result_1 = location_recommendation(*test_case_1)\n    assert isinstance(result_1, dict), 'The result should be a dictionary.'\n    assert len(result_1) == len(test_case_1[1]), 'The number of cities in the result should be equal to the number of choices.'\n\n    test_case_2 = ('https://placekitten.com/200/300', ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco'])\n    result_2 = location_recommendation(*test_case_2)\n    assert isinstance(result_2, dict), 'The result should be a dictionary.'\n    assert len(result_2) == len(test_case_2[1]), 'The number of cities in the result should be equal to the number of choices.'\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_location_recommendation()", "instruct": "# function_import --------------------\n\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\n# function_code --------------------\n\ndef location_recommendation(image_url: str, choices: list):\n    \"\"\"\n    This function uses the StreetCLIP model to generate probabilities for various cities based on images from potential locations.\n    It identifies possible locations for new stores.\n\n    Args:\n        image_url (str): The URL of the image of the potential location.\n        choices (list): A list of city options to classify images.\n\n    Returns:\n        dict: A dictionary with city names as keys and their corresponding probabilities as values.\n    \"\"\"", "answer": "\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    probs = probs.tolist()[0]\n    return {city: prob for city, prob in zip(choices, probs)}\n\n", "prediction": "\n\n    # Initialize the StreetCLIP model using CLIP's built-in pretrained weights for a 768 dimension image embedding.\n    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n    # Create a new processor to process the images.\n    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n    # Download the image from the URL and resize it to 256 x 192 px.\n    response = requests.get(image_url)\n    resized_image = Image.open(response.raw).resize((256, 192))\n    \n    # Process the image using the CLIP processor and get the embedding vector.\n    image_features = clip_processor([resized_image], return_tensors=\"pt\", padding=True)\n    image_embedding = clip_model.get_image_features(**image_features).squeeze()\n    \n    # Get the text description for each city from the list of choices and process it using the CLIP processor.\n    city_text_descriptions, probabilities = [], []\n    for choice in choices:\n        text_inputs = clip_processor(text=[choice], return_tensors=\"pt\", padding=True)\n        text_embedding = clip_model.get_text_features(**text_inputs).squeeze()\n        \n        # Calculate the cosine similarity between image and text embeddings to get their probabilities.\n        probability = Image.cosine_similarity(image_embedding, text_embedding)\n        city_text_descriptions.append(choice), probabilities.append(probability[0])\n    \n    # Create a dictionary with the city names as keys and their corresponding probabilities as values.\n    return dict(zip(city_text_descriptions, probabilities))\n"}