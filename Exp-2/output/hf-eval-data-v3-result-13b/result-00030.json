{"path": "output/hf-eval-data-v3-valid/f00138_summarize_text.py", "content": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"Test the function summarize_text.\"\"\"\n    input_text1 = 'This is a long article about the history of the world. It covers many different topics and periods.'\n    input_text2 = 'This is another long article, this time about the future of technology. It discusses many potential advancements and challenges.'\n    assert isinstance(summarize_text(input_text1), str)\n    assert isinstance(summarize_text(input_text2), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_summarize_text()", "function_import": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n", "function_code": "# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"Test the function summarize_text.\"\"\"\n    input_text1 = 'This is a long article about the history of the world. It covers many different topics and periods.'\n    input_text2 = 'This is another long article, this time about the future of technology. It discusses many potential advancements and challenges.'\n    assert isinstance(summarize_text(input_text1), str)\n    assert isinstance(summarize_text(input_text2), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_summarize_text()", "instruct": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"", "answer": "\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n", "prediction": "\n\n    try:\n         # Load the tokenizer and model from HuggingFace Hub\n         tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n         model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6').to(\"cpu\")\n\n    except requests.exceptions.ChunkedEncodingError:\n        raise requests.exceptions.ChunkedEncodingError(\n            \"Could not download the model 'sshleifer/distilbart-cnn-12-6'.\\n\" +\n             \"Please try again.\"\n         )\n    \n    # Preprocessing the text to remove the unnecessary characters and lowercase the words.\n    input_text = str(input_text).replace('\\\"','').replace('.','.\\n ').lower().strip()\n\n    max_length = 128\n    min_length = 35\n\n    # Tokenize the input text.\n    encoded_input = tokenizer([input_text], max_length=max_length, truncation=True, return_tensors=\"pt\")\n    encoded_inputs = encoded_input[\"input_ids\"]\n\n    # Perform summarization of the text using HuggingFace model.\n    output = model.generate(encoded_inputs, min_length=min_length, max_length=max_length, length_penalty=2)\n    \n    summary_text = tokenizer.batch_decode(output)[0]\n\n    return summary_text"}