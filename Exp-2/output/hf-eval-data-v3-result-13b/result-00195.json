{"path": "output/hf-eval-data-v3-valid/f00762_video_action_recognition.py", "content": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n# test_function_code --------------------\n\ndef test_video_action_recognition():\n    '''\n    Function to test the video_action_recognition function.\n    '''\n    file_path = hf_hub_download('archery.mp4')\n    assert isinstance(video_action_recognition(file_path), str), 'The function should return a string.'\n    assert video_action_recognition(file_path) != '', 'The function should not return an empty string.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_video_action_recognition()", "function_import": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n", "function_code": "# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_video_action_recognition():\n    '''\n    Function to test the video_action_recognition function.\n    '''\n    file_path = hf_hub_download('archery.mp4')\n    assert isinstance(video_action_recognition(file_path), str), 'The function should return a string.'\n    assert video_action_recognition(file_path) != '', 'The function should not return an empty string.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_video_action_recognition()", "instruct": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''", "answer": "\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n", "prediction": "\n    \n    # load the pretrained model\n    model = VideoMAEForVideoClassification.from_pretrained(\"hf-internal-testing/tiny-vit-14x14-mae\")\n    feature_extractor = VideoMAEFeatureExtractor()\n    \n    # read the video file\n    reader = VideoReader(file_path, ctx=cpu(0))\n    \n    # get the number of frames in the clip\n    num_frames = len(reader)\n    \n    # calculate the frame index step size for given parameters\n    sampling_step_size = int((num_frames - 16 / frame_sample_rate) // (clip_len-1))\n    \n    # extract the features using VideoMAEFeatureExtractor\n    features = feature_extractor(reader, sampling_strategy='random', num_sampling_steps=int(num_frames/sampling_step_size+1), \n                                 device=\"cpu\", stack_dir=\"row\")\n    \n    # convert the extracted features to tensor\n    inputs = torch.tensor(features)\n    \n    # predict and return the predicted class\n    preds = model(inputs).logits\n    \n    return model.config.id2label[np.argmax(preds)]"}