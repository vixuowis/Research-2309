{"path": "output/hf-eval-data-v3-valid/f00018_estimate_image_depth.py", "content": "# function_import --------------------\n\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\n\n# function_code --------------------\n\ndef estimate_image_depth(image_url):\n    '''\n    Estimate the depth of an image using a pretrained model from Hugging Face Transformers.\n\n    Args:\n        image_url (str): The URL of the image to be processed.\n\n    Returns:\n        depth (PIL.Image): The depth estimation of the image.\n\n    Raises:\n        requests.exceptions.RequestException: If the image cannot be loaded from the provided URL.\n        RuntimeError: If there is a problem loading the pretrained model.\n    '''\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    processor = DPTImageProcessor.from_pretrained('Intel/dpt-large')\n    model = DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\n    inputs = processor(images=image, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode='bicubic', align_corners=False)\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype('uint8')\n    depth = Image.fromarray(formatted)\n    return depth\n\n# test_function_code --------------------\n\ndef test_estimate_image_depth():\n    '''\n    Test the estimate_image_depth function with different test cases.\n    '''\n    test_image_url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    result = estimate_image_depth(test_image_url)\n    assert isinstance(result, Image.Image), 'The result should be a PIL Image.'\n    test_image_url = 'https://placekitten.com/200/300'\n    result = estimate_image_depth(test_image_url)\n    assert isinstance(result, Image.Image), 'The result should be a PIL Image.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_estimate_image_depth()", "function_import": "# function_import --------------------\n\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\n\n", "function_code": "# function_code --------------------\n\ndef estimate_image_depth(image_url):\n    '''\n    Estimate the depth of an image using a pretrained model from Hugging Face Transformers.\n\n    Args:\n        image_url (str): The URL of the image to be processed.\n\n    Returns:\n        depth (PIL.Image): The depth estimation of the image.\n\n    Raises:\n        requests.exceptions.RequestException: If the image cannot be loaded from the provided URL.\n        RuntimeError: If there is a problem loading the pretrained model.\n    '''\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    processor = DPTImageProcessor.from_pretrained('Intel/dpt-large')\n    model = DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\n    inputs = processor(images=image, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode='bicubic', align_corners=False)\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype('uint8')\n    depth = Image.fromarray(formatted)\n    return depth\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_estimate_image_depth():\n    '''\n    Test the estimate_image_depth function with different test cases.\n    '''\n    test_image_url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    result = estimate_image_depth(test_image_url)\n    assert isinstance(result, Image.Image), 'The result should be a PIL Image.'\n    test_image_url = 'https://placekitten.com/200/300'\n    result = estimate_image_depth(test_image_url)\n    assert isinstance(result, Image.Image), 'The result should be a PIL Image.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_estimate_image_depth()", "instruct": "# function_import --------------------\n\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\n\n# function_code --------------------\n\ndef estimate_image_depth(image_url):\n    '''\n    Estimate the depth of an image using a pretrained model from Hugging Face Transformers.\n\n    Args:\n        image_url (str): The URL of the image to be processed.\n\n    Returns:\n        depth (PIL.Image): The depth estimation of the image.\n\n    Raises:\n        requests.exceptions.RequestException: If the image cannot be loaded from the provided URL.\n        RuntimeError: If there is a problem loading the pretrained model.\n    '''", "answer": "\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    processor = DPTImageProcessor.from_pretrained('Intel/dpt-large')\n    model = DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\n    inputs = processor(images=image, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode='bicubic', align_corners=False)\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype('uint8')\n    depth = Image.fromarray(formatted)\n    return depth\n\n", "prediction": "\n\n    try:\n        # Load the image and convert it to NumPy format:\n        response = requests.get(image_url)\n        image = Image.open(response.content).convert('RGB')\n        image = np.asarray(image)\n\n        # Create a processor instance that will perform all of the necessary preprocessing steps and batching for the model:\n        try:\n            processor = DPTImageProcessor.from_pretrained(\"IntelPicasso/dpt-large\")\n            processor.default_image_size = 384\n            #print('Using large model')\n        except Exception as e:\n            print(e)\n            processor = DPTImageProcessor.from_pretrained(\"IntelPicasso/dpt-small\")\n            processor.default_image_size = 384\n            #print('Could not find large model, using small instead')\n        input_height = input_width = processor.default_image_size\n\n        # Create a PyTorch tensor with the properly processed image:\n        img_tensor = processor(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n        batch_img_tensor = torch.unsqueeze(img_tensor, 0)\n\n        # Load the pretrained model from Hugging Face:\n        try:\n            model = DPTForDepthEstimation.from_pretrained(\"IntelPicasso/dpt-large\")\n            input_height = input_width = processor.default_image_size\n            #print('Using large model')\n        except Exception as e:\n            print(e)\n            model = DPTForDepthEstimation.from_pretrained(\"IntelPicasso/dpt-small\")\n            input_height = input_width = processor.default_image_size\n            #print('Could not find large model, using small instead')\n        model.eval()\n\n        # Run the image through the network and convert it to NumPy array:\n        with torch.no_grad():\n            outputs = model(batch_img_tensor)\n\n        depth_estimation = outputs[0][\"pred_depth\"].numpy"}