{"path": "output/hf-eval-data-v3-valid/f00642_extract_captions.py", "content": "# function_import --------------------\n\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n# function_code --------------------\n\ndef extract_captions(image_url):\n    \"\"\"\n    Extract captions from an image using a pre-trained model from Hugging Face.\n\n    Args:\n        image_url (str): URL of the image to extract captions from.\n\n    Returns:\n        list: A list of generated captions.\n\n    Raises:\n        OSError: If there is an error in loading the image or the pre-trained model.\n    \"\"\"\n    loc = 'ydshieh/vit-gpt2-coco-en'\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    with Image.open(requests.get(image_url, stream=True).raw) as image:\n        pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n    return preds\n\n# test_function_code --------------------\n\ndef test_extract_captions():\n    \"\"\"\n    Test the extract_captions function with a few test cases.\n    \"\"\"\n    test_cases = [\n        'http://images.cocodataset.org/val2017/000000039769.jpg',\n        'https://placekitten.com/200/300',\n        'https://placekitten.com/400/600'\n    ]\n    for url in test_cases:\n        captions = extract_captions(url)\n        assert isinstance(captions, list), 'The output should be a list.'\n        assert all(isinstance(caption, str) for caption in captions), 'All elements in the output list should be strings.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_extract_captions())", "function_import": "# function_import --------------------\n\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n", "function_code": "# function_code --------------------\n\ndef extract_captions(image_url):\n    \"\"\"\n    Extract captions from an image using a pre-trained model from Hugging Face.\n\n    Args:\n        image_url (str): URL of the image to extract captions from.\n\n    Returns:\n        list: A list of generated captions.\n\n    Raises:\n        OSError: If there is an error in loading the image or the pre-trained model.\n    \"\"\"\n    loc = 'ydshieh/vit-gpt2-coco-en'\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    with Image.open(requests.get(image_url, stream=True).raw) as image:\n        pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n    return preds\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_captions():\n    \"\"\"\n    Test the extract_captions function with a few test cases.\n    \"\"\"\n    test_cases = [\n        'http://images.cocodataset.org/val2017/000000039769.jpg',\n        'https://placekitten.com/200/300',\n        'https://placekitten.com/400/600'\n    ]\n    for url in test_cases:\n        captions = extract_captions(url)\n        assert isinstance(captions, list), 'The output should be a list.'\n        assert all(isinstance(caption, str) for caption in captions), 'All elements in the output list should be strings.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_extract_captions())", "instruct": "# function_import --------------------\n\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n# function_code --------------------\n\ndef extract_captions(image_url):\n    \"\"\"\n    Extract captions from an image using a pre-trained model from Hugging Face.\n\n    Args:\n        image_url (str): URL of the image to extract captions from.\n\n    Returns:\n        list: A list of generated captions.\n\n    Raises:\n        OSError: If there is an error in loading the image or the pre-trained model.\n    \"\"\"", "answer": "\n    loc = 'ydshieh/vit-gpt2-coco-en'\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    with Image.open(requests.get(image_url, stream=True).raw) as image:\n        pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n    return preds\n\n", "prediction": "\n\n    try:\n        # load images and tokenizer\n        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n        tokenizer = AutoTokenizer.from_pretrained('allenai/led-large-16384-arxiv')\n\n        # load model\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model = VisionEncoderDecoderModel.from_pretrained('allenai/led-large-16384-arxiv').to(device)\n    except OSError as e:\n        print(\"OSError in load pre-trained model or tokenizer.\")\n        print(e)\n        raise e\n\n    # load image from url, resize the image and convert to tensor\n    try:\n        response = requests.get(image_url)\n        img = Image.open(BytesIO(response.content))\n        resized_img = feature_extractor(images=img, return_tensors=\"pt\")['pixel_values']\n    except OSError as e:\n        print(\"OSError in load or resize the image.\")\n        print(e)\n        raise e\n    \n    # generate captions for the images using the pre-trained model\n    try:\n        input = resized_img.to(device=device, dtype=torch.float32)\n        output_ids = model.generate(input, max_length=50, do_sample=True, top_p=.95, num_return_sequences=1)[0]\n    except RuntimeError as e:\n        print(\"Runtime Error in generating captions.\")\n        print(e)\n        raise e\n    \n    # decode the generated tokens into human-readable caption\n    return tokenizer.decode(output_ids, skip_special_tokens=True)"}