{"path": "output/hf-eval-data-v3-valid/f00578_retrieve_relevant_documents.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef retrieve_relevant_documents(query: str, documents: list) -> list:\n    \"\"\"\n    Retrieve relevant documents based on a user's query using Hugging Face Transformers.\n\n    Args:\n        query (str): The user's query.\n        documents (list): A list of documents to retrieve information from.\n\n    Returns:\n        list: A list of documents sorted based on their relevance to the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n\n    features = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors='pt')\n\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n    return sorted_docs\n\n# test_function_code --------------------\n\ndef test_retrieve_relevant_documents():\n    query = 'How many people live in Berlin?'\n    documents = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    expected_output = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the capital of Germany?'\n    documents = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    expected_output = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the population of New York City?'\n    documents = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    expected_output = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_retrieve_relevant_documents()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef retrieve_relevant_documents(query: str, documents: list) -> list:\n    \"\"\"\n    Retrieve relevant documents based on a user's query using Hugging Face Transformers.\n\n    Args:\n        query (str): The user's query.\n        documents (list): A list of documents to retrieve information from.\n\n    Returns:\n        list: A list of documents sorted based on their relevance to the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n\n    features = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors='pt')\n\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n    return sorted_docs\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_retrieve_relevant_documents():\n    query = 'How many people live in Berlin?'\n    documents = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    expected_output = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the capital of Germany?'\n    documents = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    expected_output = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the population of New York City?'\n    documents = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    expected_output = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_retrieve_relevant_documents()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef retrieve_relevant_documents(query: str, documents: list) -> list:\n    \"\"\"\n    Retrieve relevant documents based on a user's query using Hugging Face Transformers.\n\n    Args:\n        query (str): The user's query.\n        documents (list): A list of documents to retrieve information from.\n\n    Returns:\n        list: A list of documents sorted based on their relevance to the query.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n\n    features = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors='pt')\n\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n    return sorted_docs\n\n", "prediction": "\n\n    # Load Hugging Face model and tokenizer.\n    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased-distilled-squad', return_dict=True)\n\n    # Tokenize the query and the documents.\n    inputs = tokenizer(query, documents, truncation=True, padding='max_length')\n    features = {key: torch.as_tensor([val]) for key, val in inputs.items()}\n\n    # Get predictions on the query/documents pair.\n    with torch.no_grad():\n        output = model(**features)\n    \n    scores = [item.logits for item in output]\n    scores = [scores[0][i].item() for i in range(len(documents))]\n\n    # Sort the documents by their relevance to the query.\n    sorted_documents, _ = list(zip(*sorted(list(zip(documents, scores)), key=lambda x: -x[1])))\n    return list(sorted_documents)"}