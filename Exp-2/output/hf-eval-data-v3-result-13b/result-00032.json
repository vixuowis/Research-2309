{"path": "output/hf-eval-data-v3-valid/f00141_generate_story.py", "content": "# function_import --------------------\n\nfrom transformers import pipeline\n\n# function_code --------------------\n\ndef generate_story(prompt: str) -> str:\n    \"\"\"\n    Generate a short story based on a given prompt using the LLaMA-7B language model.\n\n    Args:\n        prompt (str): The initial prompt to base the story on.\n\n    Returns:\n        str: The generated story.\n\n    Raises:\n        OSError: If the specified model is not found.\n    \"\"\"\n    try:\n        story_generator = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n        story = story_generator(prompt)\n        return story[0]['generated_text']\n    except OSError:\n        raise OSError('Model not found. Please make sure the model name is correct.')\n\n# test_function_code --------------------\n\ndef test_generate_story():\n    \"\"\"\n    Test the generate_story function.\n    \"\"\"\n    try:\n        # Test with a simple prompt\n        prompt = 'Once upon a time in a small village...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with a different prompt\n        prompt = 'In a galaxy far, far away...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with an empty prompt\n        prompt = ''\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        print('All Tests Passed')\n    except OSError:\n        print('Model not found. Skipping tests.')\n\n# call_test_function_code --------------------\n\ntest_generate_story()", "function_import": "# function_import --------------------\n\nfrom transformers import pipeline\n\n", "function_code": "# function_code --------------------\n\ndef generate_story(prompt: str) -> str:\n    \"\"\"\n    Generate a short story based on a given prompt using the LLaMA-7B language model.\n\n    Args:\n        prompt (str): The initial prompt to base the story on.\n\n    Returns:\n        str: The generated story.\n\n    Raises:\n        OSError: If the specified model is not found.\n    \"\"\"\n    try:\n        story_generator = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n        story = story_generator(prompt)\n        return story[0]['generated_text']\n    except OSError:\n        raise OSError('Model not found. Please make sure the model name is correct.')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_story():\n    \"\"\"\n    Test the generate_story function.\n    \"\"\"\n    try:\n        # Test with a simple prompt\n        prompt = 'Once upon a time in a small village...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with a different prompt\n        prompt = 'In a galaxy far, far away...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with an empty prompt\n        prompt = ''\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        print('All Tests Passed')\n    except OSError:\n        print('Model not found. Skipping tests.')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_story()", "instruct": "# function_import --------------------\n\nfrom transformers import pipeline\n\n# function_code --------------------\n\ndef generate_story(prompt: str) -> str:\n    \"\"\"\n    Generate a short story based on a given prompt using the LLaMA-7B language model.\n\n    Args:\n        prompt (str): The initial prompt to base the story on.\n\n    Returns:\n        str: The generated story.\n\n    Raises:\n        OSError: If the specified model is not found.\n    \"\"\"", "answer": "\n    try:\n        story_generator = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n        story = story_generator(prompt)\n        return story[0]['generated_text']\n    except OSError:\n        raise OSError('Model not found. Please make sure the model name is correct.')\n\n", "prediction": "\n\n    try:\n        # Load pipeline\n        nlp = pipeline(task=\"text-generation\", model='./lama')\n\n        # Generate story\n        generated = nlp(prompt)[0]['generated_text']\n        \n        return generated.strip()\n\n    except OSError as e:\n        raise OSError(\"Model not found. Please ensure that it has been downloaded from S3.\") from e"}