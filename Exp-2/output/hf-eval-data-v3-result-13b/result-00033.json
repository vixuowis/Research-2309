{"path": "output/hf-eval-data-v3-valid/f00142_generate_conversation.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n# test_function_code --------------------\n\ndef test_generate_conversation():\n    \"\"\"\n    Test the generate_conversation function.\n    \"\"\"\n    situation = \"Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\"\n    instruction = \"You are Cosmo and you are talking to a friend.\"\n    conversation = [\"Hey, how was your trip to Abu Dhabi?\"]\n    response = generate_conversation(situation, instruction, conversation)\n    assert isinstance(response, str)\n    assert len(response) > 0\n    print('All Tests Passed')\n\n# call_test_function_code --------------------\n\ntest_generate_conversation()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n", "function_code": "# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_conversation():\n    \"\"\"\n    Test the generate_conversation function.\n    \"\"\"\n    situation = \"Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\"\n    instruction = \"You are Cosmo and you are talking to a friend.\"\n    conversation = [\"Hey, how was your trip to Abu Dhabi?\"]\n    response = generate_conversation(situation, instruction, conversation)\n    assert isinstance(response, str)\n    assert len(response) > 0\n    print('All Tests Passed')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_conversation()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"", "answer": "\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n", "prediction": "\n\n    if not isinstance(situation_narrative, str) or \\\n            not isinstance(role_instruction, str) or \\\n            not isinstance(conversation_history, list):\n        raise ValueError('Invalid input type.')\n    \n    # Set up model and tokenizer\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    tokenizer = AutoTokenizer.from_pretrained('/work/qfw1729-mcd1830/checkpoints/conversational-bart')\n    model = AutoModelForSeq2SeqLM.from_pretrained('/work/qfw1729-mcd1830/checkpoints/conversational-bart').to(device)\n\n    # Tokenize input and add EOS token to situation narrative, role instruction, and conversation history\n    input_ids = tokenizer.batch_encode_plus([situation_narrative + ' ' + \\\n                    role_instruction + ' ' + ' '.join(conversation_history)])['input_ids'][0]  # List of integers\n    \n    # Set up decoding parameters and generate conversation using model.generate()\n    eos_token_id = tokenizer.eos_token_id\n    max_length = 40 + len(conversation_history)\n    repetition_penalty = 1.3\n\n    output_ids = model.generate(torch.tensor([input_ids]).to(device), \\\n                                max_length=max_length, \\\n                                eos_token_id=eos_token_id, \\\n                                repetition_penalty=repetition_penalty)\n    conversation = tokenizer.decode(output_ids[0])  # str\n    \n    # Remove EOS token and return result\n    conversation = conversation[:conversation.rfind('<EOS>')]\n    if '<BOS>' in conversation:\n        conversation = conversation[conversation.find('<BOS>'):][5:]\n        \n    return conversation"}