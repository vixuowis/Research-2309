{"path": "output/hf-eval-data-v3-valid/f00823_extract_features.py", "content": "# function_import --------------------\n\nfrom transformers import BertTokenizer, AutoModel\nimport torch\n\n# function_code --------------------\n\ndef extract_features(input_text):\n    \"\"\"\n    Extracts contextual representation of the input text using IndoBERT model.\n\n    Args:\n        input_text (str): The input text in Indonesian language.\n\n    Returns:\n        torch.Tensor: The contextual representation of the input text.\n\n    Raises:\n        OSError: If there is a problem in loading the pretrained model or tokenizer.\n    \"\"\"\n    tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n    model = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\n    encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n    contextual_representation = model(encoded_input)[0]\n    return contextual_representation\n\n# test_function_code --------------------\n\ndef test_extract_features():\n    \"\"\"\n    Tests the function 'extract_features'.\n    \"\"\"\n    sample_text = 'Saya suka makan nasi goreng'\n    output = extract_features(sample_text)\n    assert isinstance(output, torch.Tensor), 'Output is not a torch.Tensor'\n    assert output.shape[0] == 1, 'Output shape is not correct'\n    print('All Tests Passed')\n\n# call_test_function_code --------------------\n\ntest_extract_features()", "function_import": "# function_import --------------------\n\nfrom transformers import BertTokenizer, AutoModel\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef extract_features(input_text):\n    \"\"\"\n    Extracts contextual representation of the input text using IndoBERT model.\n\n    Args:\n        input_text (str): The input text in Indonesian language.\n\n    Returns:\n        torch.Tensor: The contextual representation of the input text.\n\n    Raises:\n        OSError: If there is a problem in loading the pretrained model or tokenizer.\n    \"\"\"\n    tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n    model = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\n    encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n    contextual_representation = model(encoded_input)[0]\n    return contextual_representation\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_features():\n    \"\"\"\n    Tests the function 'extract_features'.\n    \"\"\"\n    sample_text = 'Saya suka makan nasi goreng'\n    output = extract_features(sample_text)\n    assert isinstance(output, torch.Tensor), 'Output is not a torch.Tensor'\n    assert output.shape[0] == 1, 'Output shape is not correct'\n    print('All Tests Passed')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_features()", "instruct": "# function_import --------------------\n\nfrom transformers import BertTokenizer, AutoModel\nimport torch\n\n# function_code --------------------\n\ndef extract_features(input_text):\n    \"\"\"\n    Extracts contextual representation of the input text using IndoBERT model.\n\n    Args:\n        input_text (str): The input text in Indonesian language.\n\n    Returns:\n        torch.Tensor: The contextual representation of the input text.\n\n    Raises:\n        OSError: If there is a problem in loading the pretrained model or tokenizer.\n    \"\"\"", "answer": "\n    tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n    model = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\n    encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n    contextual_representation = model(encoded_input)[0]\n    return contextual_representation\n\n", "prediction": "\n    \n    try:\n        # Loading pretrained BertTokenizer and IndoBERT model using HuggingFace Transformers library.\n        indobert_tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n        \n        indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n    except OSError as error:\n        print(f\"OSErrro: {error}\")\n    \n    # Converting text to token id.\n    input_text_tokenized = indobert_tokenizer([input_text], return_tensors=\"pt\", truncation=True, max_length=512)[\"input_ids\"]\n\n    # Getting features from IndoBERT model.\n    with torch.no_grad():\n        output = indobert_model(input_text_tokenized)[0]\n    \n    return (output[0, 0], input_text_tokenized)"}