{"path": "output/hf-eval-data-v3-valid/f00580_extract_entities.py", "content": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n# test_function_code --------------------\n\ndef test_extract_entities():\n    \"\"\"\n    Test the extract_entities function.\n    \"\"\"\n    sentence1 = \"Apple's CEO is Tim Cook and Microsoft's CEO is Satya Nadella\"\n    sentence2 = \"Google's CEO is Sundar Pichai\"\n    sentence3 = \"Amazon's CEO is Andy Jassy\"\n    assert isinstance(extract_entities(sentence1), dict)\n    assert isinstance(extract_entities(sentence2), dict)\n    assert isinstance(extract_entities(sentence3), dict)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_extract_entities()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_entities():\n    \"\"\"\n    Test the extract_entities function.\n    \"\"\"\n    sentence1 = \"Apple's CEO is Tim Cook and Microsoft's CEO is Satya Nadella\"\n    sentence2 = \"Google's CEO is Sundar Pichai\"\n    sentence3 = \"Amazon's CEO is Andy Jassy\"\n    assert isinstance(extract_entities(sentence1), dict)\n    assert isinstance(extract_entities(sentence2), dict)\n    assert isinstance(extract_entities(sentence3), dict)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_entities()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"", "answer": "\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n", "prediction": "\n    \n    model = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    \n    tokens = tokenizer.tokenize(sentence)\n    inputs = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)])\n    outputs = model(inputs)[0]\n    \n    predictions = torch.argmax(outputs, dim=2)\n    tokens = tokenizer.convert_ids_to_tokens(inputs.squeeze().tolist())\n    predictions = predictions.squeeze().tolist()\n    \n    entities = {}\n    for prediction in zip(predictions, tokens):\n        # print(prediction)\n        \n        pred, tok = prediction\n        if str(pred) == '0':  # O - Outside of an entity.\n            continue\n                            \n        if str(pred) not in entities:\n            entities[str(pred)] = []\n        entities[str(pred)].append(tok)\n    \n    return entities"}