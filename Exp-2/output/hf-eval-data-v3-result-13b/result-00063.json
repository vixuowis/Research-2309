{"path": "output/hf-eval-data-v3-valid/f00228_korean_text_summarization.py", "content": "# function_import --------------------\n\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\n# function_code --------------------\n\ndef korean_text_summarization(input_text: str) -> str:\n    \"\"\"\n    Summarize the input Korean text using the pretrained model 'kykim/bertshared-kor-base'.\n\n    Args:\n        input_text (str): The input Korean text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    summary_tokens = model.generate(input_tokens)\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n    return summary_text\n\n# test_function_code --------------------\n\ndef test_korean_text_summarization():\n    assert isinstance(korean_text_summarization('\uace0\uac1d\uc774 \uc785\ub825\ud55c \ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8\ub97c \uc694\uc57d\uc73c\ub85c \ubcc0\ud658\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\uc774\uac83\uc740 \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc785\ub2c8\ub2e4. \uc774 \ubb38\uc7a5\uc740 \uc694\uc57d\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8 \uc694\uc57d \uc54c\uace0\ub9ac\uc998\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4.'), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_korean_text_summarization()", "function_import": "# function_import --------------------\n\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\n", "function_code": "# function_code --------------------\n\ndef korean_text_summarization(input_text: str) -> str:\n    \"\"\"\n    Summarize the input Korean text using the pretrained model 'kykim/bertshared-kor-base'.\n\n    Args:\n        input_text (str): The input Korean text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    summary_tokens = model.generate(input_tokens)\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n    return summary_text\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_korean_text_summarization():\n    assert isinstance(korean_text_summarization('\uace0\uac1d\uc774 \uc785\ub825\ud55c \ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8\ub97c \uc694\uc57d\uc73c\ub85c \ubcc0\ud658\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\uc774\uac83\uc740 \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc785\ub2c8\ub2e4. \uc774 \ubb38\uc7a5\uc740 \uc694\uc57d\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8 \uc694\uc57d \uc54c\uace0\ub9ac\uc998\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4.'), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_korean_text_summarization()", "instruct": "# function_import --------------------\n\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\n# function_code --------------------\n\ndef korean_text_summarization(input_text: str) -> str:\n    \"\"\"\n    Summarize the input Korean text using the pretrained model 'kykim/bertshared-kor-base'.\n\n    Args:\n        input_text (str): The input Korean text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"", "answer": "\n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    summary_tokens = model.generate(input_tokens)\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n    return summary_text\n\n", "prediction": " \n    \n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    \n    tokens = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n\n    summary_ids = model.generate(**tokens, \n                                 num_beams=4, \n                                 length_penalty=0.6, \n                                 early_stopping=True, \n                                 min_length=32, \n                                 max_length=512)\n    output = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0]\n    \n    return output\n"}