{"path": "output/hf-eval-data-v3-valid/f00217_determine_logical_relationship.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# function_code --------------------\n\ndef determine_logical_relationship(text1: str, text2: str) -> dict:\n    \"\"\"\n    Determine the logical relationship between two given sentences.\n\n    Args:\n        text1 (str): The first sentence.\n        text2 (str): The second sentence.\n\n    Returns:\n        dict: A dictionary containing the probabilities of each logical relationship (entailment, contradiction, or neutral).\n    \"\"\"\n    model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    with torch.inference_mode():\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n\n    result = {v: proba[k] for k, v in model.config.id2label.items()}\n    return result\n\n# test_function_code --------------------\n\ndef test_determine_logical_relationship():\n    \"\"\"\n    Test the function determine_logical_relationship.\n    \"\"\"\n    text1 = 'The cat is on the mat.'\n    text2 = 'There is a cat on the mat.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'It is raining.'\n    text2 = 'The weather is sunny.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'He is a boy.'\n    text2 = 'She is a girl.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_determine_logical_relationship())", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n", "function_code": "# function_code --------------------\n\ndef determine_logical_relationship(text1: str, text2: str) -> dict:\n    \"\"\"\n    Determine the logical relationship between two given sentences.\n\n    Args:\n        text1 (str): The first sentence.\n        text2 (str): The second sentence.\n\n    Returns:\n        dict: A dictionary containing the probabilities of each logical relationship (entailment, contradiction, or neutral).\n    \"\"\"\n    model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    with torch.inference_mode():\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n\n    result = {v: proba[k] for k, v in model.config.id2label.items()}\n    return result\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_determine_logical_relationship():\n    \"\"\"\n    Test the function determine_logical_relationship.\n    \"\"\"\n    text1 = 'The cat is on the mat.'\n    text2 = 'There is a cat on the mat.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'It is raining.'\n    text2 = 'The weather is sunny.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'He is a boy.'\n    text2 = 'She is a girl.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_determine_logical_relationship())", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# function_code --------------------\n\ndef determine_logical_relationship(text1: str, text2: str) -> dict:\n    \"\"\"\n    Determine the logical relationship between two given sentences.\n\n    Args:\n        text1 (str): The first sentence.\n        text2 (str): The second sentence.\n\n    Returns:\n        dict: A dictionary containing the probabilities of each logical relationship (entailment, contradiction, or neutral).\n    \"\"\"", "answer": "\n    model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    with torch.inference_mode():\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n\n    result = {v: proba[k] for k, v in model.config.id2label.items()}\n    return result\n\n", "prediction": "\n    # Load tokenizer and model from pre-trained checkpoint. \n    model_name = \"albert-xxlarge-v2\"\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    \n    # Tokenize and encode the two sentences. \n    inputs1 = tokenizer([text1], return_tensors=\"pt\", padding=True, truncation=True)\n    inputs2 = tokenizer([text2], return_tensors=\"pt\", padding=True, truncation=True)\n    \n    # Move model and inputs to the device (CPU or GPU). \n    model.to(device)\n    inputs1[\"input_ids\"] = inputs1[\"input_ids\"].to(device)\n    inputs1[\"attention_mask\"] = inputs1[\"attention_mask\"].to(device)\n    inputs2[\"input_ids\"] = inputs2[\"input_ids\"].to(device)\n    inputs2[\"attention_mask\"] = inputs2[\"attention_mask\"].to(device)\n    \n    # Make a prediction. \n    model.eval()\n    with torch.no_grad():\n        outputs1 = model(**inputs1)\n        outputs2 = model(**inputs2)\n        \n    return {\n        \"entailment\": (outputs1[0][0] + outputs2[0][0]) / 2, \n        \"contradiction\": (outputs1[0][1] + outputs2[0][1]) / 2, \n        \"neutral\": (outputs1[0][2] + outputs2[0][2]) / 2\n    }"}