{"path": "output/hf-eval-data-v3-valid/f00142_generate_conversation.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n# test_function_code --------------------\n\ndef test_generate_conversation():\n    \"\"\"\n    Test the generate_conversation function.\n    \"\"\"\n    situation = \"Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\"\n    instruction = \"You are Cosmo and you are talking to a friend.\"\n    conversation = [\"Hey, how was your trip to Abu Dhabi?\"]\n    response = generate_conversation(situation, instruction, conversation)\n    assert isinstance(response, str)\n    assert len(response) > 0\n    print('All Tests Passed')\n\n# call_test_function_code --------------------\n\ntest_generate_conversation()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n", "function_code": "# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_conversation():\n    \"\"\"\n    Test the generate_conversation function.\n    \"\"\"\n    situation = \"Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\"\n    instruction = \"You are Cosmo and you are talking to a friend.\"\n    conversation = [\"Hey, how was your trip to Abu Dhabi?\"]\n    response = generate_conversation(situation, instruction, conversation)\n    assert isinstance(response, str)\n    assert len(response) > 0\n    print('All Tests Passed')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_conversation()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"", "answer": "\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n", "prediction": "\n    model = AutoModelForSeq2SeqLM.from_pretrained('tuner007/pegasus_paraphrase')\n    tokenizer = AutoTokenizer.from_pretrained('tuner007/pegasus_paraphrase')\n\n    input_text = situation_narrative + ' ' + role_instruction + ' ' + '<newline>'.join(conversation_history)\n    \n    # Max model length: 512 tokens. Divide the input into 512-token segments and feed into the model as a batch.\n    if len(tokenizer.encode(input_text)) <= 512:\n        outputs = model.generate(input_text, do_sample=True, temperature=0.7)\n    else:\n        input_ids = tokenizer.batch_encode_plus([input_text], return_tensors='pt')['input_ids']\n        \n        n_chunks = torch.ceil(torch.tensor(len(input_ids[0]) / 512)).to(torch.int32)\n        split_lengths = torch.ceil(torch.tensor(len(input_ids[0])) / n_chunks).to(torch.int32).repeat(n_chunks)\n        \n        # Ensure last chunk is less than 512 tokens in length (otherwise will throw error)\n        split_lengths[-1] = len(input_ids[0]) - sum(split_lengths[:-1])\n        \n        chunks = torch.tensor([torch.cat([input_ids[0][i:i+l] for i in range(0, len(input_ids[0]), l)], dim=0).unsqueeze(0) \\\n                                  for l in split_lengths])[:n_chunks-1]\n        \n        outputs = model.generate(chunks, pad_token_id=50256, do_sample=True, temperature=0.7)    \n            \n    conversation = tokenizer.batch_decode(outputs, skip_special"}