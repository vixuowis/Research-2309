{"path": "output/hf-eval-data-v3-valid/f00672_find_relevant_passage.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n# test_function_code --------------------\n\ndef test_find_relevant_passage():\n    \"\"\"\n    Test the function find_relevant_passage.\n    \"\"\"\n    question = 'How many people live in Berlin?'\n    candidate_passages = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert isinstance(find_relevant_passage(question, candidate_passages), str)\n    question = 'What is the capital of Germany?'\n    candidate_passages = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert find_relevant_passage(question, candidate_passages) == 'Berlin is the capital of Germany.'\n    question = 'Who won the world cup in 2014?'\n    candidate_passages = ['Germany won the world cup in 2014.', 'Brazil hosted the world cup in 2014.']\n    assert find_relevant_passage(question, candidate_passages) == 'Germany won the world cup in 2014.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_find_relevant_passage()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_find_relevant_passage():\n    \"\"\"\n    Test the function find_relevant_passage.\n    \"\"\"\n    question = 'How many people live in Berlin?'\n    candidate_passages = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert isinstance(find_relevant_passage(question, candidate_passages), str)\n    question = 'What is the capital of Germany?'\n    candidate_passages = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert find_relevant_passage(question, candidate_passages) == 'Berlin is the capital of Germany.'\n    question = 'Who won the world cup in 2014?'\n    candidate_passages = ['Germany won the world cup in 2014.', 'Brazil hosted the world cup in 2014.']\n    assert find_relevant_passage(question, candidate_passages) == 'Germany won the world cup in 2014.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_find_relevant_passage()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n", "prediction": "\n    \n    # Initialize a BERT model and a tokenizer\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n        )\n        model = AutoModelForSequenceClassification.from_pretrained(\n            \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n        )\n    except OSError:\n        return \"OSError: There was a problem with loading the pre-trained model or tokenizer.\"\n    \n    # Split the input into a question and candidate passages using the given delimiters.\n    \n    try:\n        question, *candidate_passages = [s.strip() for s in question.split(\"###\")]\n    except ValueError as e:\n        return f\"ValueError: {e} Please make sure you separate the question and candidate passages using '###'.\"\n    \n    # Find the most relevant passage by scoring each one and returning the highest-scored one.\n    \n    best_score, best_passage = 0.0, \"\"\n    \n    for passage in candidate_passages:\n        \n        input_text = f\"[CLS] {question} [SEP] {passage} [SEP]\"\n        encoding = tokenizer(input_text, return_tensors=\"pt\")\n        input_ids = encoding[\"input_ids\"].to(model.device)\n        \n        with torch.no_grad():\n            outputs = model(**encoding, return_dict=True)\n            score = outputs.logits[0][0].item()\n            \n        if score > best_score:\n            best_score = score\n            best_passage = passage\n    \n    # Return the best-scoring passage.\n    \n    return best_passage"}