{"path": "output/hf-eval-data-v3-valid/f00226_generate_text.py", "content": "# function_import --------------------\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nimport torch\n\n# function_code --------------------\n\ndef generate_text(prompt: str, num_return_sequences: int = 5, max_length: int = 10):\n    \"\"\"\n    Generate text based on a given prompt using the pretrained model 'facebook/opt-66b'.\n\n    Args:\n        prompt (str): The initial text to start the generation from.\n        num_return_sequences (int, optional): The number of different response sequences to generate. Defaults to 5.\n        max_length (int, optional): The maximum length of each response. Defaults to 10.\n\n    Returns:\n        List[str]: A list of generated text sequences.\n    \"\"\"\n    model = AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16).cuda()\n    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-66b', use_fast=False)\n    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n    set_seed(32)\n    generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=num_return_sequences, max_length=max_length)\n    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    return responses\n\n# test_function_code --------------------\n\ndef test_generate_text():\n    \"\"\"\n    Test the function generate_text.\n    \"\"\"\n    responses = generate_text('Hello, I am conscious and', 5, 10)\n    assert isinstance(responses, list), 'The return type should be a list.'\n    assert len(responses) == 5, 'The length of the list should be equal to num_return_sequences.'\n    for response in responses:\n        assert isinstance(response, str), 'Each element in the list should be a string.'\n        assert len(response.split()) <= 10, 'The length of each response should be less than or equal to max_length.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_generate_text()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef generate_text(prompt: str, num_return_sequences: int = 5, max_length: int = 10):\n    \"\"\"\n    Generate text based on a given prompt using the pretrained model 'facebook/opt-66b'.\n\n    Args:\n        prompt (str): The initial text to start the generation from.\n        num_return_sequences (int, optional): The number of different response sequences to generate. Defaults to 5.\n        max_length (int, optional): The maximum length of each response. Defaults to 10.\n\n    Returns:\n        List[str]: A list of generated text sequences.\n    \"\"\"\n    model = AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16).cuda()\n    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-66b', use_fast=False)\n    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n    set_seed(32)\n    generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=num_return_sequences, max_length=max_length)\n    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    return responses\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_text():\n    \"\"\"\n    Test the function generate_text.\n    \"\"\"\n    responses = generate_text('Hello, I am conscious and', 5, 10)\n    assert isinstance(responses, list), 'The return type should be a list.'\n    assert len(responses) == 5, 'The length of the list should be equal to num_return_sequences.'\n    for response in responses:\n        assert isinstance(response, str), 'Each element in the list should be a string.'\n        assert len(response.split()) <= 10, 'The length of each response should be less than or equal to max_length.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_text()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nimport torch\n\n# function_code --------------------\n\ndef generate_text(prompt: str, num_return_sequences: int = 5, max_length: int = 10):\n    \"\"\"\n    Generate text based on a given prompt using the pretrained model 'facebook/opt-66b'.\n\n    Args:\n        prompt (str): The initial text to start the generation from.\n        num_return_sequences (int, optional): The number of different response sequences to generate. Defaults to 5.\n        max_length (int, optional): The maximum length of each response. Defaults to 10.\n\n    Returns:\n        List[str]: A list of generated text sequences.\n    \"\"\"", "answer": "\n    model = AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16).cuda()\n    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-66b', use_fast=False)\n    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n    set_seed(32)\n    generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=num_return_sequences, max_length=max_length)\n    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    return responses\n\n", "prediction": "\n    # Set the seed for reproducibility.\n    set_seed(42)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-66b\")\n    model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\").to(device)\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.repeat_interleave(num_return_sequences, dim=0).to(device)\n    output = model.generate(\n        input_ids, \n        max_length=max_length,\n        do_sample=True,  \n        top_k=50, \n    )\n    \n    generated_texts = tokenizer.batch_decode(output)\n    \n    return generated_texts"}