{"path": "output/hf-eval-data-v3-valid/f00364_detect_kitchen_objects.py", "content": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n# test_function_code --------------------\n\ndef test_detect_kitchen_objects():\n    \"\"\"\n    Tests the detect_kitchen_objects function.\n    \"\"\"\n    try:\n        detect_kitchen_objects('test_image.jpg', 0.1)\n        print('Test passed')\n    except Exception as e:\n        print(f'Test failed: {str(e)}')\n\n# call_test_function_code --------------------\n\ntest_detect_kitchen_objects()", "function_import": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n", "function_code": "# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_detect_kitchen_objects():\n    \"\"\"\n    Tests the detect_kitchen_objects function.\n    \"\"\"\n    try:\n        detect_kitchen_objects('test_image.jpg', 0.1)\n        print('Test passed')\n    except Exception as e:\n        print(f'Test failed: {str(e)}')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_detect_kitchen_objects()", "instruct": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"", "answer": "\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "prediction": "\n    # Verify that the image exists\n    try:\n        image = Image.open(image_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The image file was not found.\")\n    \n    # Load the model and processor\n    processor = OwlViTProcessor.from_pretrained(\"vovk/owlvit-base-1m-sampling\")\n    model = OwlViTForObjectDetection.from_pretrained(\"vovk/owlvit-base-1m-sampling\", num_labels=20)\n    \n    # Process the image\n    try:\n        processed_image = processor(image, return_tensors=\"pt\")\n    except RuntimeError as error:\n        raise RuntimeError(\"There was a problem loading the model.\") from error\n    \n    # Perform object detection\n    outputs = model(**processed_image)\n    \n    # Print the detected objects and their locations\n    if len(outputs.logits[0]) > 0:\n        print(\"Detected objects:\")\n        \n        labels = outputs.logits[0].softmax(-1)[0]\n        scores, classes = labels.detach().topk(labels.shape[-1])\n                    \n        for score, class_index in zip(scores, classes):\n            if score > score_threshold:\n                print(f\"- {processor.decode_label(int(class_index)).title()}, confidence score: {score}\")\n    else:\n        print(\"No objects detected.\")"}