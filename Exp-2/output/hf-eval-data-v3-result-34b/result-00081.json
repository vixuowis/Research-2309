{"path": "output/hf-eval-data-v3-valid/f00349_generate_image_caption.py", "content": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n# test_function_code --------------------\n\ndef test_generate_image_caption():\n    \"\"\"\n    Test the function generate_image_caption.\n    \"\"\"\n    assert isinstance(generate_image_caption('test_image.jpg', 'product photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'landscape photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'portrait photography'), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_generate_image_caption())", "function_import": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n", "function_code": "# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_image_caption():\n    \"\"\"\n    Test the function generate_image_caption.\n    \"\"\"\n    assert isinstance(generate_image_caption('test_image.jpg', 'product photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'landscape photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'portrait photography'), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_generate_image_caption())", "instruct": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"", "answer": "\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n", "prediction": "\n    \n    # Load the model and tokenizer from pretrained BLIP.\n    model = BlipForConditionalGeneration.from_pretrained(\"KuangHW/blip-vizwiki-small\")\n    processor = BlipProcessor.from_pretrained(\"KuangHW/blip-vizwiki-small\")\n\n    # Load the input image.\n    PIL_image = Image.open(image_path)\n\n    # Process and prepare the inputs for model.\n    inputs = processor(PIL_image, text, return_tensors='pt', padding='max_length')\n\n    # Generate an caption.\n    gen_caption = model.generate(**inputs)\n    \n    # Return a single caption in the batch.\n    return [processor.tokenizer.decode(g, skip_special_tokens=True) for g in gen_caption][0]\n"}