{"path": "output/hf-eval-data-v3-valid/f00762_video_action_recognition.py", "content": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n# test_function_code --------------------\n\ndef test_video_action_recognition():\n    '''\n    Function to test the video_action_recognition function.\n    '''\n    file_path = hf_hub_download('archery.mp4')\n    assert isinstance(video_action_recognition(file_path), str), 'The function should return a string.'\n    assert video_action_recognition(file_path) != '', 'The function should not return an empty string.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_video_action_recognition()", "function_import": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n", "function_code": "# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_video_action_recognition():\n    '''\n    Function to test the video_action_recognition function.\n    '''\n    file_path = hf_hub_download('archery.mp4')\n    assert isinstance(video_action_recognition(file_path), str), 'The function should return a string.'\n    assert video_action_recognition(file_path) != '', 'The function should not return an empty string.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_video_action_recognition()", "instruct": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''", "answer": "\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n", "prediction": "\n\n    # Load feature extractor and model.\n    print('Loading...')\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('facebook/video-mae-base-kinetics-600h')\n    model = VideoMAEForVideoClassification.from_pretrained('facebook/video-mae-base-kinetics-600h', num_labels=400)\n\n    # Load and preprocess frames from video file.\n    with open(file_path, 'rb') as f:\n        full = np.frombuffer(f.read(), dtype=np.uint8)\n    frames = VideoReader(full, ctx=cpu(0))  # change context to gpu if available.\n    \n    # Sample frames based on frame sample rate and preprocess frames using feature extractor.\n    step_size = clip_len * (frame_sample_rate - 1) + 1\n    frames_select = frames[::step_size][:clip_len]\n    video = []\n    \n    # Decord is used to decode videos into numerical array representation for preprocessing.\n    for i in range(0, clip_len):\n        img = frames_select[i].asnumpy()\n        img = img[:, :, ::-1]  # RGB to BGR format\n        video.append(img)\n    \n    # Preprocess selected frames using feature extractor and convert list to numpy array.\n    inputs = feature_extractor(images=video, return_tensors='pt')['pixel_values']\n    print('Loaded...')\n\n    # Prediction on video.\n    with torch.no_grad():\n        prediction = model(inputs)\n    \n    # Get label from prediction.\n    predicted_class_idx = prediction.logits.argmax(-1).item()\n    le = hf_hub_download('facebook/video-mae-base-kinetics-600h', filename='label_map.txt')\n    \n    with open(le) as f:\n        lbl_list = [ll.split()[1][:-1] for ll in f."}