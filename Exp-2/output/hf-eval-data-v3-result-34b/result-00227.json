{"path": "output/hf-eval-data-v3-valid/f00862_rank_search_results.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef rank_search_results(query: str, passages: list) -> list:\n    \"\"\"\n    Ranks the given passages based on their relevance to the given query using a pretrained model.\n\n    Args:\n        query (str): The search query.\n        passages (list): The list of passages to be ranked.\n\n    Returns:\n        list: The list of passages ranked in descending order of relevance.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    features = tokenizer([query] * len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n\n    sorted_passages = sorted(zip(passages, scores.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n    return sorted_passages\n\n# test_function_code --------------------\n\ndef test_rank_search_results():\n    \"\"\"\n    Tests the rank_search_results function with some test cases.\n    \"\"\"\n    query = 'How many people live in Berlin?'\n    passages = [\n        'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.',\n        'New York City is famous for the Metropolitan Museum of Art.',\n        'Berlin is the capital of Germany and one of the 16 states of Germany.',\n        'Berlin is known for its festivals, diverse architecture, nightlife, contemporary arts, and a high quality of living.'\n    ]\n    result = rank_search_results(query, passages)\n    assert isinstance(result, list), 'The result should be a list.'\n    assert len(result) == len(passages), 'The result should have the same length as the input passages.'\n    assert all(isinstance(item, tuple) and len(item) == 2 for item in result), 'Each item in the result should be a tuple with two elements.'\n    assert all(isinstance(item[0], str) and isinstance(item[1], float) for item in result), 'Each item in the result should be a tuple with a string and a float.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_rank_search_results()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef rank_search_results(query: str, passages: list) -> list:\n    \"\"\"\n    Ranks the given passages based on their relevance to the given query using a pretrained model.\n\n    Args:\n        query (str): The search query.\n        passages (list): The list of passages to be ranked.\n\n    Returns:\n        list: The list of passages ranked in descending order of relevance.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    features = tokenizer([query] * len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n\n    sorted_passages = sorted(zip(passages, scores.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n    return sorted_passages\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_rank_search_results():\n    \"\"\"\n    Tests the rank_search_results function with some test cases.\n    \"\"\"\n    query = 'How many people live in Berlin?'\n    passages = [\n        'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.',\n        'New York City is famous for the Metropolitan Museum of Art.',\n        'Berlin is the capital of Germany and one of the 16 states of Germany.',\n        'Berlin is known for its festivals, diverse architecture, nightlife, contemporary arts, and a high quality of living.'\n    ]\n    result = rank_search_results(query, passages)\n    assert isinstance(result, list), 'The result should be a list.'\n    assert len(result) == len(passages), 'The result should have the same length as the input passages.'\n    assert all(isinstance(item, tuple) and len(item) == 2 for item in result), 'Each item in the result should be a tuple with two elements.'\n    assert all(isinstance(item[0], str) and isinstance(item[1], float) for item in result), 'Each item in the result should be a tuple with a string and a float.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_rank_search_results()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef rank_search_results(query: str, passages: list) -> list:\n    \"\"\"\n    Ranks the given passages based on their relevance to the given query using a pretrained model.\n\n    Args:\n        query (str): The search query.\n        passages (list): The list of passages to be ranked.\n\n    Returns:\n        list: The list of passages ranked in descending order of relevance.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    features = tokenizer([query] * len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n\n    sorted_passages = sorted(zip(passages, scores.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n    return sorted_passages\n\n", "prediction": "\n\n    # Load tokenizer and model from Huggingface hub\n    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"mrm8488/bert-mini-finetuned-age_news-classification\"\n    )\n\n    # Pair each passage with the query to be fed into the model\n    inputs = [[passage, query] for passage in passages]\n\n    # Tokenize the pair of passage and query\n    tokenized_inputs = [tokenizer(pair) for pair in inputs]\n\n    # Extract input ids from the tokenized inputs and convert into a PyTorch tensor\n    input_ids = torch.tensor([pair[\"input_ids\"] for pair in tokenized_inputs])\n\n    # Feed passage-query pairs to the model and get their corresponding probabilities of being relevant or not\n    outputs = model(input_ids)\n    logits = outputs[0]\n    relevance_prob = torch.softmax(logits[:, 1], dim=0)\n    relevance_prob = list(relevance_prob.detach().numpy())\n\n    # Sort the passages in descending order of their relevance probabilities to the query\n    ranked_passages = [\n        passage for _, passage in sorted(zip(relevance_prob, passages), reverse=True)\n    ]\n\n    return ranked_passages"}