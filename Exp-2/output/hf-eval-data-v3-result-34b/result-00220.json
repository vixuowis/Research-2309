{"path": "output/hf-eval-data-v3-valid/f00845_detect_objects.py", "content": "# function_import --------------------\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_objects(url: str, texts: list, model_name: str = 'google/owlvit-large-patch14', score_threshold: float = 0.1):\n    \"\"\"\n    Detect objects in an image based on specific text phrases using the OwlViT model.\n\n    Args:\n        url (str): The URL of the image.\n        texts (list): A list of text descriptions.\n        model_name (str, optional): The name of the OwlViT model. Defaults to 'google/owlvit-large-patch14'.\n        score_threshold (float, optional): The score threshold for filtering detections. Defaults to 0.1.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and bounding box locations.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n    for i, result in enumerate(results):\n        boxes, scores, labels = result['boxes'], result['scores'], result['labels']\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[label]} with confidence {round(score.item(), 3)} at location {box}')\n\n# test_function_code --------------------\n\ndef test_detect_objects():\n    \"\"\"\n    Test the detect_objects function.\n    \"\"\"\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    texts = ['a photo of a cat', 'a photo of a dog']\n    try:\n        detect_objects(url, texts)\n        print('Test passed.')\n    except Exception as e:\n        print('Test failed. Error: ', e)\n\n# call_test_function_code --------------------\n\ntest_detect_objects()", "function_import": "# function_import --------------------\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n", "function_code": "# function_code --------------------\n\ndef detect_objects(url: str, texts: list, model_name: str = 'google/owlvit-large-patch14', score_threshold: float = 0.1):\n    \"\"\"\n    Detect objects in an image based on specific text phrases using the OwlViT model.\n\n    Args:\n        url (str): The URL of the image.\n        texts (list): A list of text descriptions.\n        model_name (str, optional): The name of the OwlViT model. Defaults to 'google/owlvit-large-patch14'.\n        score_threshold (float, optional): The score threshold for filtering detections. Defaults to 0.1.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and bounding box locations.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n    for i, result in enumerate(results):\n        boxes, scores, labels = result['boxes'], result['scores'], result['labels']\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_detect_objects():\n    \"\"\"\n    Test the detect_objects function.\n    \"\"\"\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    texts = ['a photo of a cat', 'a photo of a dog']\n    try:\n        detect_objects(url, texts)\n        print('Test passed.')\n    except Exception as e:\n        print('Test failed. Error: ', e)\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_detect_objects()", "instruct": "# function_import --------------------\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_objects(url: str, texts: list, model_name: str = 'google/owlvit-large-patch14', score_threshold: float = 0.1):\n    \"\"\"\n    Detect objects in an image based on specific text phrases using the OwlViT model.\n\n    Args:\n        url (str): The URL of the image.\n        texts (list): A list of text descriptions.\n        model_name (str, optional): The name of the OwlViT model. Defaults to 'google/owlvit-large-patch14'.\n        score_threshold (float, optional): The score threshold for filtering detections. Defaults to 0.1.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and bounding box locations.\n    \"\"\"", "answer": "\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n    for i, result in enumerate(results):\n        boxes, scores, labels = result['boxes'], result['scores'], result['labels']\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "prediction": "\n    \n    # Load OwlViT model\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    \n    # Create batch of inputs to process\n    with urllib.request.urlopen(url) as image_file:\n        input_data = [{'url': url, 'texts': texts}]\n        \n    # Preprocess inputs\n    processed_input = processor(images=image_file, args=input_data)\n    \n    # Run through model and post-process outputs to get object detections\n    with torch.no_grad():\n        outputs = model(**processed_input)\n        \n    # Print object detection results\n    print(f\"Detected objects (with text phrases):\")\n    for box in outputs.object_boxes[0]:\n        if outputs.object_confidences[0][outputs.object_labels[0].index(max(outputs.object_labels[0]))] >= score_threshold:\n            print(f\"{processor.tokenizer.decode(outputs.text_seq[0])} - Confidence: {round(100 * outputs.object_confidences[0][outputs.object_labels[0].index(max(outputs.object_labels[0]))], 2)}%\", end=' ')\n            print({'x': round(box[0]), 'y': round(box[1]), 'width': round(box[2] - box[0]), 'height': round(box[3] - box[1])})\n"}