{"path": "output/hf-eval-data-v3-valid/f00635_extract_code_syntax_and_entities.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# function_code --------------------\n\ndef extract_code_syntax_and_entities(text):\n    \"\"\"\n    Extracts code syntax and named entities from a text taken from StackOverflow.\n\n    Args:\n        text (str): The text from which to extract code syntax and named entities.\n\n    Returns:\n        dict: A dictionary containing the classified tokens and their corresponding labels.\n\n    Raises:\n        OSError: If there is an error in loading the pre-trained model or tokenizer.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n    model = AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    predictions = outputs.logits.argmax(-1)\n    return {'tokens': tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), 'labels': predictions.tolist()}\n\n# test_function_code --------------------\n\ndef test_extract_code_syntax_and_entities():\n    \"\"\"\n    Tests the function extract_code_syntax_and_entities.\n    \"\"\"\n    test_text = 'How to use the AutoModelForTokenClassification from Hugging Face Transformers?'\n    result = extract_code_syntax_and_entities(test_text)\n    assert isinstance(result, dict), 'The result should be a dictionary.'\n    assert 'tokens' in result, 'The result dictionary should have a key named tokens.'\n    assert 'labels' in result, 'The result dictionary should have a key named labels.'\n    assert isinstance(result['tokens'], list), 'The tokens should be a list.'\n    assert isinstance(result['labels'], list), 'The labels should be a list.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_extract_code_syntax_and_entities()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n", "function_code": "# function_code --------------------\n\ndef extract_code_syntax_and_entities(text):\n    \"\"\"\n    Extracts code syntax and named entities from a text taken from StackOverflow.\n\n    Args:\n        text (str): The text from which to extract code syntax and named entities.\n\n    Returns:\n        dict: A dictionary containing the classified tokens and their corresponding labels.\n\n    Raises:\n        OSError: If there is an error in loading the pre-trained model or tokenizer.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n    model = AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    predictions = outputs.logits.argmax(-1)\n    return {'tokens': tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), 'labels': predictions.tolist()}\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_code_syntax_and_entities():\n    \"\"\"\n    Tests the function extract_code_syntax_and_entities.\n    \"\"\"\n    test_text = 'How to use the AutoModelForTokenClassification from Hugging Face Transformers?'\n    result = extract_code_syntax_and_entities(test_text)\n    assert isinstance(result, dict), 'The result should be a dictionary.'\n    assert 'tokens' in result, 'The result dictionary should have a key named tokens.'\n    assert 'labels' in result, 'The result dictionary should have a key named labels.'\n    assert isinstance(result['tokens'], list), 'The tokens should be a list.'\n    assert isinstance(result['labels'], list), 'The labels should be a list.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_code_syntax_and_entities()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# function_code --------------------\n\ndef extract_code_syntax_and_entities(text):\n    \"\"\"\n    Extracts code syntax and named entities from a text taken from StackOverflow.\n\n    Args:\n        text (str): The text from which to extract code syntax and named entities.\n\n    Returns:\n        dict: A dictionary containing the classified tokens and their corresponding labels.\n\n    Raises:\n        OSError: If there is an error in loading the pre-trained model or tokenizer.\n    \"\"\"", "answer": "\n    tokenizer = AutoTokenizer.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n    model = AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    predictions = outputs.logits.argmax(-1)\n    return {'tokens': tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), 'labels': predictions.tolist()}\n\n", "prediction": "    \n\n    try:\n        # load tokenizer and model from local disk if not cached\n        tokenizer = AutoTokenizer.from_pretrained('./syntax_entity_code/tokenizer', do_lower_case=False)\n        model = AutoModelForTokenClassification.from_pretrained('./syntax_entity_code/model')\n    except OSError:\n        raise Exception(\"Error loading syntax and entity code tokenizer or model!\")\n\n    # classify the input text\n    tokens = tokenizer.encode(text)\n    output = model(torch.tensor([tokens]))[0]\n    predictions = torch.argmax(output, dim=2)\n    \n    return {i:j for i, j in zip(tokenizer.convert_ids_to_tokens(tokens), [ENTITY_CLASSES[prediction].split('-')[1] for prediction in predictions[0]])}"}