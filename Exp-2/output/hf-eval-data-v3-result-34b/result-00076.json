{"path": "output/hf-eval-data-v3-valid/f00310_generate_query.py", "content": "# function_import --------------------\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# function_code --------------------\n\ndef generate_query(document):\n    \"\"\"\n    Generate a query from a given document using a pre-trained T5 model.\n\n    Args:\n        document (str): The document from which to generate the query.\n\n    Returns:\n        str: The generated query.\n\n    Raises:\n        ValueError: If the document is not a string or is empty.\n    \"\"\"\n    if not isinstance(document, str) or not document:\n        raise ValueError('Document must be a non-empty string.')\n\n    tokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\n    model = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\n\n    inputs = tokenizer.encode('generate query: ' + document, return_tensors='pt', max_length=512, truncation=True)\n    outputs = model.generate(inputs, num_return_sequences=1, max_length=40)\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# test_function_code --------------------\n\ndef test_generate_query():\n    \"\"\"\n    Test the generate_query function.\n    \"\"\"\n    # Test with a valid document\n    document = 'This is a test document.'\n    query = generate_query(document)\n    assert isinstance(query, str), 'Query must be a string.'\n\n    # Test with an empty document\n    try:\n        generate_query('')\n    except ValueError as e:\n        assert str(e) == 'Document must be a non-empty string.', 'Exception message must be correct.'\n\n    # Test with a non-string document\n    try:\n        generate_query(None)\n    except ValueError as e:\n        assert str(e) == 'Document must be a non-empty string.', 'Exception message must be correct.'\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_generate_query()", "function_import": "# function_import --------------------\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n", "function_code": "# function_code --------------------\n\ndef generate_query(document):\n    \"\"\"\n    Generate a query from a given document using a pre-trained T5 model.\n\n    Args:\n        document (str): The document from which to generate the query.\n\n    Returns:\n        str: The generated query.\n\n    Raises:\n        ValueError: If the document is not a string or is empty.\n    \"\"\"\n    if not isinstance(document, str) or not document:\n        raise ValueError('Document must be a non-empty string.')\n\n    tokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\n    model = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\n\n    inputs = tokenizer.encode('generate query: ' + document, return_tensors='pt', max_length=512, truncation=True)\n    outputs = model.generate(inputs, num_return_sequences=1, max_length=40)\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_query():\n    \"\"\"\n    Test the generate_query function.\n    \"\"\"\n    # Test with a valid document\n    document = 'This is a test document.'\n    query = generate_query(document)\n    assert isinstance(query, str), 'Query must be a string.'\n\n    # Test with an empty document\n    try:\n        generate_query('')\n    except ValueError as e:\n        assert str(e) == 'Document must be a non-empty string.', 'Exception message must be correct.'\n\n    # Test with a non-string document\n    try:\n        generate_query(None)\n    except ValueError as e:\n        assert str(e) == 'Document must be a non-empty string.', 'Exception message must be correct.'\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_query()", "instruct": "# function_import --------------------\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# function_code --------------------\n\ndef generate_query(document):\n    \"\"\"\n    Generate a query from a given document using a pre-trained T5 model.\n\n    Args:\n        document (str): The document from which to generate the query.\n\n    Returns:\n        str: The generated query.\n\n    Raises:\n        ValueError: If the document is not a string or is empty.\n    \"\"\"", "answer": "\n    if not isinstance(document, str) or not document:\n        raise ValueError('Document must be a non-empty string.')\n\n    tokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\n    model = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\n\n    inputs = tokenizer.encode('generate query: ' + document, return_tensors='pt', max_length=512, truncation=True)\n    outputs = model.generate(inputs, num_return_sequences=1, max_length=40)\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n", "prediction": "\n    if type(document) != str:\n        raise TypeError('The document should be passed as a single string.')\n    \n    # load tokenizer and model (should be fast if using T5 model from HuggingFace Hub with local cache)\n    \n    print('Loading tokenizer and model...')\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n    \n    # process input\n    \n    text = \"summarize: \"+document\n    encoding = tokenizer(text, padding=\"max_length\", max_length=200, truncation=True, return_tensors=\"pt\")\n    input_ids = encoding['input_ids']\n    attention_mask = encoding[\"attention_mask\"]\n    \n    # generate output\n    \n    summary_ids = model.generate(input_ids, \n                              attention_mask=attention_mask,\n                              num_beams=4,\n                              length_penalty=2.0,\n                              min_length=15,\n                              max_length=60,\n                              no_repeat_ngram_size=3)\n    \n    # decode and return output (trim first token because it is always the same)\n    \n    return tokenizer.decode(summary_ids[0][1:], skip_special_tokens=True)"}