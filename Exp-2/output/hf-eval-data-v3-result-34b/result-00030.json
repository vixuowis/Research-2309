{"path": "output/hf-eval-data-v3-valid/f00138_summarize_text.py", "content": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"Test the function summarize_text.\"\"\"\n    input_text1 = 'This is a long article about the history of the world. It covers many different topics and periods.'\n    input_text2 = 'This is another long article, this time about the future of technology. It discusses many potential advancements and challenges.'\n    assert isinstance(summarize_text(input_text1), str)\n    assert isinstance(summarize_text(input_text2), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_summarize_text()", "function_import": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n", "function_code": "# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"Test the function summarize_text.\"\"\"\n    input_text1 = 'This is a long article about the history of the world. It covers many different topics and periods.'\n    input_text2 = 'This is another long article, this time about the future of technology. It discusses many potential advancements and challenges.'\n    assert isinstance(summarize_text(input_text1), str)\n    assert isinstance(summarize_text(input_text2), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_summarize_text()", "instruct": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"", "answer": "\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n", "prediction": "\n\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    \n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6').to(\"cuda\")\n\n    input_text = input_text.replace('\\n', ' ')\n        \n    input_ids = tokenizer(input_text, return_tensors='pt')['input_ids']\n    \n    output_summary = model.generate(\n        input_ids.to(\"cuda\"), \n        max_length=250, \n        num_beams=6, \n        early_stopping=True\n    )\n    \n    summary_text = tokenizer.decode(output_summary[0], skip_special_tokens=True)\n        \n    return summary_text"}