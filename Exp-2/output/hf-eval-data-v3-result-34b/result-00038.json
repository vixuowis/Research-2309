{"path": "output/hf-eval-data-v3-valid/f00173_generate_sentence_embeddings.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# function_code --------------------\n\ndef generate_sentence_embeddings(sentences):\n    \"\"\"\n    Generate sentence embeddings for the given sentences using a pre-trained model.\n\n    Args:\n        sentences (list): A list of sentences for which to generate embeddings.\n\n    Returns:\n        torch.Tensor: A tensor containing the sentence embeddings.\n    \"\"\"\n    def mean_pooling(model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n\n    tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n    model = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n    return sentence_embeddings\n\n# test_function_code --------------------\n\ndef test_generate_sentence_embeddings():\n    \"\"\"\n    Test the generate_sentence_embeddings function.\n    \"\"\"\n    sentences = ['\u0410\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u0433\u0430\u0437\u0435\u0442\u044b', '\u042d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f', '\u041c\u044b \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439']\n    embeddings = generate_sentence_embeddings(sentences)\n    assert embeddings.shape[0] == len(sentences), 'Number of embeddings does not match number of sentences'\n    assert embeddings.shape[1] == 1024, 'Embedding size does not match expected size'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_generate_sentence_embeddings()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef generate_sentence_embeddings(sentences):\n    \"\"\"\n    Generate sentence embeddings for the given sentences using a pre-trained model.\n\n    Args:\n        sentences (list): A list of sentences for which to generate embeddings.\n\n    Returns:\n        torch.Tensor: A tensor containing the sentence embeddings.\n    \"\"\"\n    def mean_pooling(model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n\n    tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n    model = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n    return sentence_embeddings\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_sentence_embeddings():\n    \"\"\"\n    Test the generate_sentence_embeddings function.\n    \"\"\"\n    sentences = ['\u0410\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u0433\u0430\u0437\u0435\u0442\u044b', '\u042d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f', '\u041c\u044b \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439']\n    embeddings = generate_sentence_embeddings(sentences)\n    assert embeddings.shape[0] == len(sentences), 'Number of embeddings does not match number of sentences'\n    assert embeddings.shape[1] == 1024, 'Embedding size does not match expected size'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_sentence_embeddings()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# function_code --------------------\n\ndef generate_sentence_embeddings(sentences):\n    \"\"\"\n    Generate sentence embeddings for the given sentences using a pre-trained model.\n\n    Args:\n        sentences (list): A list of sentences for which to generate embeddings.\n\n    Returns:\n        torch.Tensor: A tensor containing the sentence embeddings.\n    \"\"\"", "answer": "\n    def mean_pooling(model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n\n    tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n    model = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n    return sentence_embeddings\n\n", "prediction": "\n    \n    # Load the tokenizer and model that we will use to generate the sentence embeddings.\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else None\n    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens', do_lower_case=True, output_hidden_states=False)\n    model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n    \n    if device is not None:\n        # We can put the model on the GPU to make things faster.\n        model = model.cuda()\n\n    sentence_embeddings = []\n\n    for sent in sentences:\n        tokenized_sentence = torch.tensor(tokenizer.encode(str(sent)), device=device)\n        output = model(tokenized_sentence)\n        \n        # take the first output embedding and add to sentence embeddings\n        sentence_embeddings.append(output[0][0].detach().cpu().numpy())\n    \n    return torch.Tensor(sentence_embeddings).cpu()"}