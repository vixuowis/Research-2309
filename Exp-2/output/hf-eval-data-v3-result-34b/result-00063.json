{"path": "output/hf-eval-data-v3-valid/f00228_korean_text_summarization.py", "content": "# function_import --------------------\n\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\n# function_code --------------------\n\ndef korean_text_summarization(input_text: str) -> str:\n    \"\"\"\n    Summarize the input Korean text using the pretrained model 'kykim/bertshared-kor-base'.\n\n    Args:\n        input_text (str): The input Korean text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    summary_tokens = model.generate(input_tokens)\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n    return summary_text\n\n# test_function_code --------------------\n\ndef test_korean_text_summarization():\n    assert isinstance(korean_text_summarization('\uace0\uac1d\uc774 \uc785\ub825\ud55c \ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8\ub97c \uc694\uc57d\uc73c\ub85c \ubcc0\ud658\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\uc774\uac83\uc740 \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc785\ub2c8\ub2e4. \uc774 \ubb38\uc7a5\uc740 \uc694\uc57d\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8 \uc694\uc57d \uc54c\uace0\ub9ac\uc998\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4.'), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_korean_text_summarization()", "function_import": "# function_import --------------------\n\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\n", "function_code": "# function_code --------------------\n\ndef korean_text_summarization(input_text: str) -> str:\n    \"\"\"\n    Summarize the input Korean text using the pretrained model 'kykim/bertshared-kor-base'.\n\n    Args:\n        input_text (str): The input Korean text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    summary_tokens = model.generate(input_tokens)\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n    return summary_text\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_korean_text_summarization():\n    assert isinstance(korean_text_summarization('\uace0\uac1d\uc774 \uc785\ub825\ud55c \ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8\ub97c \uc694\uc57d\uc73c\ub85c \ubcc0\ud658\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\uc774\uac83\uc740 \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc785\ub2c8\ub2e4. \uc774 \ubb38\uc7a5\uc740 \uc694\uc57d\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4.'), str)\n    assert isinstance(korean_text_summarization('\ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8 \uc694\uc57d \uc54c\uace0\ub9ac\uc998\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4.'), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_korean_text_summarization()", "instruct": "# function_import --------------------\n\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\n# function_code --------------------\n\ndef korean_text_summarization(input_text: str) -> str:\n    \"\"\"\n    Summarize the input Korean text using the pretrained model 'kykim/bertshared-kor-base'.\n\n    Args:\n        input_text (str): The input Korean text to be summarized.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"", "answer": "\n    tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    summary_tokens = model.generate(input_tokens)\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n    return summary_text\n\n", "prediction": "\n\n    # Load tokenizer and encoder-decoder model --------------------\n    fast_tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\n    model = EncoderDecoderModel.from_encoder_decoder_pretrained('kykim/bertshared-kor-base', 'kykim/bertshared-kor-base')\n\n    # Prepare input_text --------------------\n    tokenized_input_text = fast_tokenizer(input_text, return_tensors='pt')['input_ids']\n\n    # Summarize input_text --------------------\n    summarized_text = model.generate(**tokenized_input_text)\n\n    # Decode summarized text and return it --------------------\n    return fast_tokenizer.decode(summarized_text[0], skip_special_tokens=True)\n"}