{"path": "output/hf-eval-data-v3-valid/f00075_transcribe_audio.py", "content": "# function_import --------------------\n\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# function_code --------------------\n\ndef transcribe_audio(audio_sample):\n    '''\n    Transcribe audio using the openai/whisper-tiny model.\n\n    Args:\n        audio_sample (dict): A dictionary containing 'array' and 'sampling_rate' of the audio.\n\n    Returns:\n        str: The transcribed text.\n    '''\n    processor = WhisperProcessor.from_pretrained('openai/whisper-tiny')\n    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\n    input_features = processor(audio_sample['array'], sampling_rate=audio_sample['sampling_rate'], return_tensors='pt').input_features\n    predicted_ids = model.generate(input_features)\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    return transcription[0]\n\n# test_function_code --------------------\n\ndef test_transcribe_audio():\n    '''\n    Test the transcribe_audio function.\n    '''\n    ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\n    sample = ds[0]['audio']\n    transcription = transcribe_audio(sample)\n    assert isinstance(transcription, str), 'The result should be a string.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_transcribe_audio()", "function_import": "# function_import --------------------\n\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n", "function_code": "# function_code --------------------\n\ndef transcribe_audio(audio_sample):\n    '''\n    Transcribe audio using the openai/whisper-tiny model.\n\n    Args:\n        audio_sample (dict): A dictionary containing 'array' and 'sampling_rate' of the audio.\n\n    Returns:\n        str: The transcribed text.\n    '''\n    processor = WhisperProcessor.from_pretrained('openai/whisper-tiny')\n    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\n    input_features = processor(audio_sample['array'], sampling_rate=audio_sample['sampling_rate'], return_tensors='pt').input_features\n    predicted_ids = model.generate(input_features)\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    return transcription[0]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_transcribe_audio():\n    '''\n    Test the transcribe_audio function.\n    '''\n    ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\n    sample = ds[0]['audio']\n    transcription = transcribe_audio(sample)\n    assert isinstance(transcription, str), 'The result should be a string.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_transcribe_audio()", "instruct": "# function_import --------------------\n\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# function_code --------------------\n\ndef transcribe_audio(audio_sample):\n    '''\n    Transcribe audio using the openai/whisper-tiny model.\n\n    Args:\n        audio_sample (dict): A dictionary containing 'array' and 'sampling_rate' of the audio.\n\n    Returns:\n        str: The transcribed text.\n    '''", "answer": "\n    processor = WhisperProcessor.from_pretrained('openai/whisper-tiny')\n    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\n    input_features = processor(audio_sample['array'], sampling_rate=audio_sample['sampling_rate'], return_tensors='pt').input_features\n    predicted_ids = model.generate(input_features)\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    return transcription[0]\n\n", "prediction": "\n\n     # Load processor\n    processor = WhisperProcessor.from_pretrained(\"cacos/whisper-tiny\")\n\n    # Reload model discarding model weights, use for inference only!\n    model = WhisperForConditionalGeneration.from_pretrained('facebook/wav2vec2-base-960h', from_tf=False)\n\n    # Preprocess audio dict to torch tensor -> ValueError: Audio array not of int16 dtype\n    features = processor(audio_sample, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n    \n    # Generate prediction\n    logits = model(features.input_values, attention_mask=features.attention_mask).logits\n    idx = logits.argmax(-1).flatten().tolist()[0]\n    \n    # Decode transcription\n    transcript = processor.decode(idx)\n\n    return transcript\n\n# function call --------------------\n\nif __name__ == '__main__':\n\n    ds_iter = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n    audio_sample = next(ds_iter)['audio']\n    \n    transcription = transcribe_audio(audio_sample)\n    print('Transcribed text: ', transcription)"}