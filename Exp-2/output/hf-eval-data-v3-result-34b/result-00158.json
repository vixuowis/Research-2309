{"path": "output/hf-eval-data-v3-valid/f00642_extract_captions.py", "content": "# function_import --------------------\n\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n# function_code --------------------\n\ndef extract_captions(image_url):\n    \"\"\"\n    Extract captions from an image using a pre-trained model from Hugging Face.\n\n    Args:\n        image_url (str): URL of the image to extract captions from.\n\n    Returns:\n        list: A list of generated captions.\n\n    Raises:\n        OSError: If there is an error in loading the image or the pre-trained model.\n    \"\"\"\n    loc = 'ydshieh/vit-gpt2-coco-en'\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    with Image.open(requests.get(image_url, stream=True).raw) as image:\n        pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n    return preds\n\n# test_function_code --------------------\n\ndef test_extract_captions():\n    \"\"\"\n    Test the extract_captions function with a few test cases.\n    \"\"\"\n    test_cases = [\n        'http://images.cocodataset.org/val2017/000000039769.jpg',\n        'https://placekitten.com/200/300',\n        'https://placekitten.com/400/600'\n    ]\n    for url in test_cases:\n        captions = extract_captions(url)\n        assert isinstance(captions, list), 'The output should be a list.'\n        assert all(isinstance(caption, str) for caption in captions), 'All elements in the output list should be strings.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_extract_captions())", "function_import": "# function_import --------------------\n\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n", "function_code": "# function_code --------------------\n\ndef extract_captions(image_url):\n    \"\"\"\n    Extract captions from an image using a pre-trained model from Hugging Face.\n\n    Args:\n        image_url (str): URL of the image to extract captions from.\n\n    Returns:\n        list: A list of generated captions.\n\n    Raises:\n        OSError: If there is an error in loading the image or the pre-trained model.\n    \"\"\"\n    loc = 'ydshieh/vit-gpt2-coco-en'\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    with Image.open(requests.get(image_url, stream=True).raw) as image:\n        pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n    return preds\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_captions():\n    \"\"\"\n    Test the extract_captions function with a few test cases.\n    \"\"\"\n    test_cases = [\n        'http://images.cocodataset.org/val2017/000000039769.jpg',\n        'https://placekitten.com/200/300',\n        'https://placekitten.com/400/600'\n    ]\n    for url in test_cases:\n        captions = extract_captions(url)\n        assert isinstance(captions, list), 'The output should be a list.'\n        assert all(isinstance(caption, str) for caption in captions), 'All elements in the output list should be strings.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_extract_captions())", "instruct": "# function_import --------------------\n\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n# function_code --------------------\n\ndef extract_captions(image_url):\n    \"\"\"\n    Extract captions from an image using a pre-trained model from Hugging Face.\n\n    Args:\n        image_url (str): URL of the image to extract captions from.\n\n    Returns:\n        list: A list of generated captions.\n\n    Raises:\n        OSError: If there is an error in loading the image or the pre-trained model.\n    \"\"\"", "answer": "\n    loc = 'ydshieh/vit-gpt2-coco-en'\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    with Image.open(requests.get(image_url, stream=True).raw) as image:\n        pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n    return preds\n\n", "prediction": "\n    \n    # Download the image and load it into PIL\n    try:\n        img = Image.open(requests.get(image_url, stream=True).raw)\n        \n    except OSError as e:\n        print(\"There was an error loading the image.\")\n        raise\n    \n    # Load the pre-trained model and feature extractor\n    try:\n        tokenizer = AutoTokenizer.from_pretrained('google/vit-base-patch16-224')\n        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224', do_resize=False)\n        \n    except OSError as e:\n        print(\"There was an error loading the pre-trained model.\")\n        raise \n    \n    # Encode and tokenize the image using the feature extractor\n    encoding = feature_extractor(images=img, return_tensors=\"pt\")\n    pixel_values = encoding['pixel_values']\n        \n    # Create a caption list and add it to our list of generated captions\n    decoder = VisionEncoderDecoderModel.from_pretrained('google/vit-base-patch16-224')\n    \n    return [decoder.generate(pixel_values, max_length=50, num_beams=5, do_sample=True)[0]]"}