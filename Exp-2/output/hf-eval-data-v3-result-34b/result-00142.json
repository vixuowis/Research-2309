{"path": "output/hf-eval-data-v3-valid/f00580_extract_entities.py", "content": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n# test_function_code --------------------\n\ndef test_extract_entities():\n    \"\"\"\n    Test the extract_entities function.\n    \"\"\"\n    sentence1 = \"Apple's CEO is Tim Cook and Microsoft's CEO is Satya Nadella\"\n    sentence2 = \"Google's CEO is Sundar Pichai\"\n    sentence3 = \"Amazon's CEO is Andy Jassy\"\n    assert isinstance(extract_entities(sentence1), dict)\n    assert isinstance(extract_entities(sentence2), dict)\n    assert isinstance(extract_entities(sentence3), dict)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_extract_entities()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_entities():\n    \"\"\"\n    Test the extract_entities function.\n    \"\"\"\n    sentence1 = \"Apple's CEO is Tim Cook and Microsoft's CEO is Satya Nadella\"\n    sentence2 = \"Google's CEO is Sundar Pichai\"\n    sentence3 = \"Amazon's CEO is Andy Jassy\"\n    assert isinstance(extract_entities(sentence1), dict)\n    assert isinstance(extract_entities(sentence2), dict)\n    assert isinstance(extract_entities(sentence3), dict)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_entities()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"", "answer": "\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n", "prediction": "\n\n    \n    # Load trained model for entity extraction\n    tokenizer = AutoTokenizer.from_pretrained(\"./model/tokenizer\")\n    model = AutoModelForTokenClassification.from_pretrained(\"./model\", return_dict=True)\n\n    # Prepare input\n    input = tokenizer(sentence, return_tensors=\"pt\")\n    \n    # Extract entities using the pre-trained entity extraction model\n    output = model(**input)\n    predictions = output.logits[0].argmax(-1).numpy()\n    predicted_labels = [model.config.id2label[x] for x in predictions][1:-1] # Remove special tokens\n    \n    # Extract entities from labels and map them to their types (e.g. B-ORG -> ORG)\n    entities = []\n    last_type = None\n    start_idx = -1\n    end_idx   = -1\n    \n    for i, label in enumerate(predicted_labels):\n        \n        if last_type is not None and (\"O\" == label or \"B\" in label):\n            entities.append({'entity': sentence[start_idx:end_idx], 'type': last_type})\n            \n        if label != \"O\":\n            if \"B\" in label:\n                start_idx = i\n            else:\n                end_idx = i\n            last_type = label[-3:] # B-ORG -> ORG, I-PER -> PER\n    \n    return entities"}