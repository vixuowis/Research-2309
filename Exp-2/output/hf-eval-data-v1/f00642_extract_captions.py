import torch
import requests
from PIL import Image
from transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel

# Pretrained model location
loc = 'ydshieh/vit-gpt2-coco-en'

# Load the feature extractor, tokenizer and model
feature_extractor = ViTFeatureExtractor.from_pretrained(loc)
tokenizer = AutoTokenizer.from_pretrained(loc)
model = VisionEncoderDecoderModel.from_pretrained(loc)
model.eval()


def extract_captions(image):
    """
    This function takes an image as input and returns the caption generated by the pretrained model.
    
    Parameters:
    image (PIL.Image): The input image.
    
    Returns:
    list: The generated captions.
    """
    # Extract pixel values from the image
    pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values
    
    # Generate captions
    with torch.no_grad():
        output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences
    
    # Decode the generated captions
    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    preds = [pred.strip() for pred in preds]
    
    return preds