2023-12-01 00:39:28.757282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-01 00:39:29.527377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
config.json:   0%|                                                                    | 0.00/624 [00:00<?, ?B/s]config.json: 100%|██████████████████████████████████████████████████████████████| 624/624 [00:00<00:00, 206kB/s]
model.safetensors:   0%|                                                             | 0.00/412M [00:00<?, ?B/s]model.safetensors:   5%|██▋                                                  | 21.0M/412M [00:00<00:02, 160MB/s]model.safetensors:  13%|██████▊                                              | 52.4M/412M [00:00<00:01, 190MB/s]model.safetensors:  18%|█████████▍                                           | 73.4M/412M [00:00<00:01, 196MB/s]model.safetensors:  25%|█████████████▊                                        | 105M/412M [00:00<00:01, 204MB/s]model.safetensors:  33%|█████████████████▉                                    | 136M/412M [00:00<00:01, 202MB/s]model.safetensors:  38%|████████████████████▋                                 | 157M/412M [00:00<00:01, 202MB/s]model.safetensors:  43%|███████████████████████▍                              | 178M/412M [00:00<00:01, 200MB/s]model.safetensors:  51%|███████████████████████████▌                          | 210M/412M [00:01<00:00, 205MB/s]model.safetensors:  56%|██████████████████████████████▎                       | 231M/412M [00:01<00:00, 205MB/s]model.safetensors:  61%|█████████████████████████████████                     | 252M/412M [00:01<00:00, 206MB/s]model.safetensors:  66%|███████████████████████████████████▊                  | 273M/412M [00:01<00:00, 201MB/s]model.safetensors:  74%|███████████████████████████████████████▉              | 304M/412M [00:01<00:00, 206MB/s]model.safetensors:  82%|████████████████████████████████████████████          | 336M/412M [00:01<00:00, 207MB/s]model.safetensors:  87%|██████████████████████████████████████████████▊       | 357M/412M [00:01<00:00, 207MB/s]model.safetensors:  92%|█████████████████████████████████████████████████▌    | 377M/412M [00:01<00:00, 205MB/s]model.safetensors:  97%|████████████████████████████████████████████████████▎ | 398M/412M [00:01<00:00, 200MB/s]model.safetensors: 100%|██████████████████████████████████████████████████████| 412M/412M [00:02<00:00, 202MB/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
tokenizer_config.json:   0%|                                                         | 0.00/29.0 [00:00<?, ?B/s]tokenizer_config.json: 100%|█████████████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 5.79kB/s]
vocab.txt:   0%|                                                                     | 0.00/110k [00:00<?, ?B/s]vocab.txt: 100%|█████████████████████████████████████████████████████████████| 110k/110k [00:00<00:00, 43.5MB/s]
tokenizer.json:   0%|                                                                | 0.00/269k [00:00<?, ?B/s]tokenizer.json: 100%|█████████████████████████████████████████████████████████| 269k/269k [00:00<00:00, 180MB/s]
Traceback (most recent call last):
  File "./f00892_fill_mask_chinese.py", line 64, in <module>
    test_fill_mask_chinese()
  File "./f00892_fill_mask_chinese.py", line 42, in test_fill_mask_chinese
    result1 = fill_mask_chinese(text1)
  File "./f00892_fill_mask_chinese.py", line 29, in fill_mask_chinese
    raise PipelineException(f"The input string '{text}' does not contain a mask token ([MASK]). Please try again with a different input.")
NameError: name 'PipelineException' is not defined
