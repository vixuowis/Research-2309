2023-11-30 15:39:59.115070: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-30 15:40:00.003835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
config.json:   0%|                                                                    | 0.00/841 [00:00<?, ?B/s]config.json: 100%|██████████████████████████████████████████████████████████████| 841/841 [00:00<00:00, 124kB/s]
pytorch_model.bin:   0%|                                                            | 0.00/1.11G [00:00<?, ?B/s]pytorch_model.bin:   1%|▍                                                   | 10.5M/1.11G [00:14<24:33, 748kB/s]pytorch_model.bin:   1%|▍                                                   | 10.5M/1.11G [00:27<24:33, 748kB/s]pytorch_model.bin:   2%|▉                                                   | 21.0M/1.11G [00:32<29:10, 623kB/s]pytorch_model.bin:   2%|▉                                                   | 21.0M/1.11G [00:47<29:10, 623kB/s]pytorch_model.bin:   3%|█▍                                                  | 31.5M/1.11G [00:51<30:16, 595kB/s]pytorch_model.bin:   3%|█▍                                                  | 31.5M/1.11G [01:07<30:16, 595kB/s]pytorch_model.bin:   4%|█▉                                                  | 41.9M/1.11G [01:15<34:07, 523kB/s]pytorch_model.bin:   4%|█▉                                                  | 41.9M/1.11G [01:27<34:07, 523kB/s]pytorch_model.bin:   5%|██▍                                                 | 52.4M/1.11G [01:33<32:48, 538kB/s]pytorch_model.bin:   5%|██▍                                                 | 52.4M/1.11G [01:47<32:48, 538kB/s]pytorch_model.bin:   6%|██▉                                                 | 62.9M/1.11G [01:55<33:44, 518kB/s]pytorch_model.bin:   6%|██▉                                                 | 62.9M/1.11G [02:07<33:44, 518kB/s]pytorch_model.bin:   7%|███▍                                                | 73.4M/1.11G [02:14<33:05, 523kB/s]pytorch_model.bin:   7%|███▍                                                | 73.4M/1.11G [02:27<33:05, 523kB/s]pytorch_model.bin:   8%|███▉                                                | 83.9M/1.11G [02:31<31:02, 552kB/s]pytorch_model.bin:   8%|███▉                                                | 83.9M/1.11G [02:47<31:02, 552kB/s]pytorch_model.bin:   8%|████▍                                               | 94.4M/1.11G [02:52<31:37, 536kB/s]pytorch_model.bin:   8%|████▍                                               | 94.4M/1.11G [03:07<31:37, 536kB/s]pytorch_model.bin:   9%|████▉                                                | 105M/1.11G [03:19<35:13, 477kB/s]pytorch_model.bin:   9%|████▉                                                | 105M/1.11G [03:37<35:13, 477kB/s]pytorch_model.bin:  10%|█████▍                                               | 115M/1.11G [03:54<40:45, 408kB/s]pytorch_model.bin:  10%|█████▍                                               | 115M/1.11G [04:02<34:57, 475kB/s]
Traceback (most recent call last):
  File "./f00035_sentiment_analysis.py", line 48, in <module>
    test_sentiment_analysis()
  File "./f00035_sentiment_analysis.py", line 32, in test_sentiment_analysis
    result = sentiment_analysis('I am really frustrated with the service')
  File "./f00035_sentiment_analysis.py", line 20, in sentiment_analysis
    classifier = pipeline("sentiment-analysis", model="cardiffnlp/twitter-xlm-roberta-base-sentiment")
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 870, in pipeline
    framework, model = infer_framework_load_model(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3057, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 541, in http_get
    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/requests/models.py", line 816, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 934, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 877, in read
    data = self._raw_read(amt)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 812, in _raw_read
    data = self._fp_read(amt) if not fp_closed else b""
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 797, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/http/client.py", line 459, in read
    n = self.readinto(b)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/http/client.py", line 503, in readinto
    n = self.fp.readinto(b)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/root/autodl-tmp/conda-envs/py38/lib/python3.8/ssl.py", line 1132, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
