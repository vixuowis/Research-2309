vocab.json:   0%|                                                                   | 0.00/1.04M [00:00<?, ?B/s]vocab.json: 100%|██████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 3.73MB/s]vocab.json: 100%|██████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 3.71MB/s]
merges.txt:   0%|                                                                    | 0.00/456k [00:00<?, ?B/s]merges.txt: 100%|████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.70MB/s]merges.txt: 100%|████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.68MB/s]
tokenizer.json:   0%|                                                               | 0.00/1.36M [00:00<?, ?B/s]tokenizer.json: 100%|██████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 3.08MB/s]tokenizer.json: 100%|██████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 3.07MB/s]
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1509: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  warnings.warn(
model.safetensors:   0%|                                                             | 0.00/548M [00:00<?, ?B/s]model.safetensors:   2%|▉                                                   | 10.5M/548M [00:05<04:20, 2.07MB/s]model.safetensors:   4%|█▉                                                  | 21.0M/548M [00:12<05:17, 1.66MB/s]model.safetensors:   6%|██▉                                                 | 31.5M/548M [00:16<04:19, 1.99MB/s]model.safetensors:   8%|███▉                                                | 41.9M/548M [00:18<03:18, 2.55MB/s]model.safetensors:  10%|████▉                                               | 52.4M/548M [00:20<02:31, 3.27MB/s]model.safetensors:  11%|█████▉                                              | 62.9M/548M [00:22<02:12, 3.66MB/s]model.safetensors:  13%|██████▉                                             | 73.4M/548M [00:24<02:04, 3.80MB/s]model.safetensors:  15%|███████▉                                            | 83.9M/548M [00:27<02:01, 3.83MB/s]model.safetensors:  17%|████████▉                                           | 94.4M/548M [00:30<01:58, 3.84MB/s]model.safetensors:  19%|██████████▏                                          | 105M/548M [00:32<01:49, 4.03MB/s]model.safetensors:  21%|███████████▏                                         | 115M/548M [00:37<02:19, 3.09MB/s]model.safetensors:  23%|████████████▏                                        | 126M/548M [00:40<02:06, 3.33MB/s]model.safetensors:  25%|█████████████▏                                       | 136M/548M [00:43<01:57, 3.51MB/s]model.safetensors:  27%|██████████████▏                                      | 147M/548M [00:45<01:49, 3.68MB/s]model.safetensors:  29%|███████████████▏                                     | 157M/548M [00:49<02:00, 3.25MB/s]model.safetensors:  31%|████████████████▏                                    | 168M/548M [00:56<02:38, 2.40MB/s]model.safetensors:  33%|█████████████████▏                                   | 178M/548M [01:01<02:34, 2.40MB/s]model.safetensors:  34%|██████████████████▎                                  | 189M/548M [01:06<02:41, 2.22MB/s]model.safetensors:  36%|███████████████████▎                                 | 199M/548M [01:11<02:33, 2.27MB/s]model.safetensors:  38%|████████████████████▎                                | 210M/548M [01:15<02:26, 2.31MB/s]model.safetensors:  40%|█████████████████████▎                               | 220M/548M [01:18<02:11, 2.50MB/s]model.safetensors:  42%|██████████████████████▎                              | 231M/548M [01:21<01:54, 2.77MB/s]model.safetensors:  44%|███████████████████████▎                             | 241M/548M [01:24<01:43, 2.96MB/s]model.safetensors:  46%|████████████████████████▎                            | 252M/548M [01:30<01:58, 2.51MB/s]model.safetensors:  48%|█████████████████████████▎                           | 262M/548M [01:37<02:20, 2.04MB/s]model.safetensors:  50%|██████████████████████████▎                          | 273M/548M [01:44<02:28, 1.85MB/s]model.safetensors:  52%|███████████████████████████▍                         | 283M/548M [01:54<02:53, 1.52MB/s]model.safetensors:  52%|███████████████████████████▍                         | 283M/548M [02:04<02:53, 1.52MB/s]model.safetensors:  54%|████████████████████████████▍                        | 294M/548M [02:04<03:14, 1.31MB/s]model.safetensors:  55%|█████████████████████████████▍                       | 304M/548M [02:12<03:05, 1.31MB/s]model.safetensors:  57%|██████████████████████████████▍                      | 315M/548M [02:23<03:18, 1.17MB/s]model.safetensors:  59%|███████████████████████████████▍                     | 325M/548M [02:31<03:01, 1.23MB/s]model.safetensors:  61%|████████████████████████████████▍                    | 336M/548M [02:39<02:46, 1.27MB/s]model.safetensors:  63%|█████████████████████████████████▍                   | 346M/548M [02:49<02:51, 1.18MB/s]model.safetensors:  65%|██████████████████████████████████▍                  | 357M/548M [03:02<03:03, 1.05MB/s]model.safetensors:  67%|████████████████████████████████████▏                 | 367M/548M [03:14<03:04, 982kB/s]model.safetensors:  69%|████████████████████████████████████▌                | 377M/548M [03:23<02:45, 1.03MB/s]model.safetensors:  71%|█████████████████████████████████████▌               | 388M/548M [03:33<02:32, 1.05MB/s]model.safetensors:  73%|██████████████████████████████████████▌              | 398M/548M [03:41<02:17, 1.08MB/s]model.safetensors:  73%|██████████████████████████████████████▌              | 398M/548M [03:54<02:17, 1.08MB/s]model.safetensors:  75%|████████████████████████████████████████▎             | 409M/548M [03:56<02:27, 944kB/s]model.safetensors:  77%|█████████████████████████████████████████▎            | 419M/548M [04:08<02:18, 928kB/s]model.safetensors:  78%|██████████████████████████████████████████▎           | 430M/548M [04:20<02:11, 896kB/s]model.safetensors:  80%|███████████████████████████████████████████▍          | 440M/548M [04:30<01:55, 933kB/s]model.safetensors:  80%|███████████████████████████████████████████▍          | 440M/548M [04:44<01:55, 933kB/s]model.safetensors:  82%|████████████████████████████████████████████▍         | 451M/548M [04:45<01:52, 862kB/s]model.safetensors:  84%|█████████████████████████████████████████████▍        | 461M/548M [04:56<01:37, 891kB/s]model.safetensors:  86%|█████████████████████████████████████████████▋       | 472M/548M [05:02<01:13, 1.03MB/s]model.safetensors:  88%|██████████████████████████████████████████████▋      | 482M/548M [05:10<01:00, 1.09MB/s]model.safetensors:  90%|███████████████████████████████████████████████▋     | 493M/548M [05:20<00:49, 1.11MB/s]model.safetensors:  92%|████████████████████████████████████████████████▋    | 503M/548M [05:31<00:43, 1.03MB/s]model.safetensors:  94%|██████████████████████████████████████████████████▌   | 514M/548M [05:43<00:34, 990kB/s]model.safetensors:  94%|██████████████████████████████████████████████████▌   | 514M/548M [05:54<00:34, 990kB/s]model.safetensors:  96%|███████████████████████████████████████████████████▋  | 524M/548M [05:56<00:25, 936kB/s]model.safetensors:  98%|████████████████████████████████████████████████████▋ | 535M/548M [06:08<00:14, 900kB/s]model.safetensors:  99%|█████████████████████████████████████████████████████▋| 545M/548M [06:19<00:03, 926kB/s]model.safetensors: 100%|██████████████████████████████████████████████████████| 548M/548M [06:23<00:00, 904kB/s]model.safetensors: 100%|█████████████████████████████████████████████████████| 548M/548M [06:23<00:00, 1.43MB/s]
generation_config.json:   0%|                                                         | 0.00/124 [00:00<?, ?B/s]generation_config.json: 100%|██████████████████████████████████████████████████| 124/124 [00:00<00:00, 23.4kB/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 80, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1509: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 94, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/root/autodl-tmp/conda-envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 97, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
