{'code': "###Instruction: We are developing a solution where we can visually detect the medical conditions of patients. Can you write code to determine the condition based on the image provided?\n###Output: <<<domain>>>: Multimodal Visual Question Answering\n<<<api_call>>>: AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import necessary libraries: AutoModelForSeq2SeqLM and AutoTokenizer from the 'transformers' library.\n2. Load the pretrained 'microsoft/git-large-textvqa' using the AutoModelForSeq2SeqLM.from_pretrained() function. This model is fine-tuned on TextVQA and can be used for multimodal tasks like visual question answering.\n3. Load the tokenizer using AutoTokenizer.from_pretrained() function with the same model name.\n4. Prepare the input: Use the tokenizer to encode the image and the question together. The question should specifically ask the model about the medical condition in the provided image.\n5. Run the model on the encoded input and decode the predicted tokens back into text to get the medical condition as output.\n<<<code>>>: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\ntokenizer = AutoTokenizer.from_pretrained('microsoft/git-large-textvqa')\nencoded_input = tokenizer('What medical condition is present in the image?', image, return_tensors='pt')\ngenerated_tokens = model.generate(**encoded_input)\ndetected_medical_condition = tokenizer.decode(generated_tokens[0])\n", 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00267_detect_medical_condition.py", line 38, in <module>
    test_detect_medical_condition()
  File "output/hf-eval-data-v2/f00267_detect_medical_condition.py", line 33, in test_detect_medical_condition
    detected_condition = detect_medical_condition(sample_image, sample_question)
  File "output/hf-eval-data-v2/f00267_detect_medical_condition.py", line 18, in detect_medical_condition
    model = AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.git.configuration_git.GitConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.
Model type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.
