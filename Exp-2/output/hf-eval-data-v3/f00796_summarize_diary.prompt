{'code': '###Instruction: I am an astronaut in space, writing my diary every day. I need a summary of my diary before sharing it with my family.\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: LEDForConditionalGeneration.from_pretrained(\'MingZhong/DialogLED-base-16384\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries from the transformers package. This includes LEDForConditionalGeneration and AutoTokenizer.\n2. Load the pre-trained model \'MingZhong/DialogLED-base-16384\' for text summarization using the LEDForConditionalGeneration.from_pretrained function.\n3. Create an AutoTokenizer instance of the model to tokenize the input text correctly.\n4. Tokenize the text of your diary entry and use the model to generate a summary by providing the input tokens to the model.\n5. Use the tokenizer to decode the summary tokens into text.\n<<<code>>>: from transformers import LEDForConditionalGeneration, AutoTokenizer\nmodel = LEDForConditionalGeneration.from_pretrained(\'MingZhong/DialogLED-base-16384\')\ntokenizer = AutoTokenizer.from_pretrained(\'MingZhong/DialogLED-base-16384\')\n\ndiary_entry = "Your diary entry goes here..."\ninput_tokens = tokenizer(diary_entry, return_tensors=\'pt\')\nsummary_output = model.generate(**input_tokens)\nsummary_text = tokenizer.decode(summary_output[0])\n', 'api_call': "LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'DialogLED-base-16384', 'api_call': "LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')", 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'arxiv', 'accuracy': '2109.02492'}, 'description': 'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.'}}

