{'code': "###Instruction: We have an international audience for our website and require our website content to be translated into multiple languages for better understanding.\n###Output: <<<domain>>>: Natural Language Processing Translation\n<<<api_call>>>: pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\n<<<api_provider>>>: PyTorch Transformers\n<<<explanation>>>: 1. Import the necessary libraries from transformers, specifically the 'pipeline' function.\n2. Use the 'pipeline' function to create a translation model using the 'facebook/nllb-200-distilled-600M' model.\n3. This model is capable of translating text between 200 languages, making it a suitable choice for the task of translating website content.\n4. To translate a specific piece of text, simply pass the text as an argument to the generated translation pipeline. The model will automatically detect the input language and translate it accordingly.\n<<<code>>>: from transformers import pipeline\ntranslator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\ntranslated_text = translator('This is the content of the website.')", 'api_call': "pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'PyTorch Transformers', 'functionality': 'text2text-generation', 'api_name': 'facebook/nllb-200-distilled-600M', 'api_call': "pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')", 'api_arguments': ['model', 'text'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': "from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')", 'performance': {'dataset': 'Flores-200', 'accuracy': 'BLEU, spBLEU, chrF++'}, 'description': 'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.'}}

  File "output/hf-eval-data-v2/f00502_translate_text.py", line 28, in test_translate_text
    translated_text = translate_text(sample_text)
  File "output/hf-eval-data-v2/f00502_translate_text.py", line 17, in translate_text
    translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 824, in pipeline
    framework, model = infer_framework_load_model(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 282, in infer_framework_load_model
    raise ValueError(
ValueError: Could not load model facebook/nllb-200-distilled-600M with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 554, in http_get
    temp_file.write(chunk)
  File "/root/miniconda3/envs/py38/lib/python3.8/tempfile.py", line 473, in func_wrapper
    return func(*args, **kwargs)
OSError: [Errno 28] No space left on device

while loading with M2M100ForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 554, in http_get
    temp_file.write(chunk)
  File "/root/miniconda3/envs/py38/lib/python3.8/tempfile.py", line 473, in func_wrapper
    return func(*args, **kwargs)
OSError: [Errno 28] No space left on device



