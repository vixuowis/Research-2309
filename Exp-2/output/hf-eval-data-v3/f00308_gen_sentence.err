Downloading (…)okenizer_config.json:   0%|                                                                          | 0.00/25.0 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████████████████████████████| 25.0/25.0 [00:00<00:00, 3.16kB/s]
Downloading (…)lve/main/config.json:   0%|                                                                         | 0.00/1.27k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████| 1.27k/1.27k [00:00<00:00, 109kB/s]
Downloading (…)ve/main/spiece.model:   0%|                                                                          | 0.00/792k [00:00<?, ?B/s]Downloading (…)ve/main/spiece.model: 100%|███████████████████████████████████████████████████████████████████| 792k/792k [00:01<00:00, 474kB/s]Downloading (…)ve/main/spiece.model: 100%|███████████████████████████████████████████████████████████████████| 792k/792k [00:01<00:00, 474kB/s]
Downloading (…)cial_tokens_map.json:   0%|                                                                         | 0.00/1.79k [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████| 1.79k/1.79k [00:00<00:00, 725kB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "./f00308_gen_sentence.py", line 43, in <module>
    test_gen_sentence()
  File "./f00308_gen_sentence.py", line 32, in test_gen_sentence
    sentence = gen_sentence(words)
  File "./f00308_gen_sentence.py", line 19, in gen_sentence
    model = AutoModelForCausalLM.from_pretrained('mrm8488/t5-base-finetuned-common_gen')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.
