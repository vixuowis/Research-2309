Downloading (…)lve/main/config.json:   0%|                                                                           | 0.00/462 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████████████| 462/462 [00:00<00:00, 45.6kB/s]2023-11-11 19:57:46.040314: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-11 19:57:46.092917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-11 19:57:47.015430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

Downloading pytorch_model.bin:   0%|                                                                                | 0.00/433M [00:00<?, ?B/s]Downloading pytorch_model.bin:   5%|███▍                                                                    | 21.0M/433M [00:00<00:03, 134MB/s]Downloading pytorch_model.bin:  10%|██████▉                                                                 | 41.9M/433M [00:00<00:02, 152MB/s]Downloading pytorch_model.bin:  15%|██████████▍                                                             | 62.9M/433M [00:00<00:02, 160MB/s]Downloading pytorch_model.bin:  19%|█████████████▉                                                          | 83.9M/433M [00:00<00:02, 163MB/s]Downloading pytorch_model.bin:  24%|█████████████████▋                                                       | 105M/433M [00:00<00:01, 165MB/s]Downloading pytorch_model.bin:  29%|█████████████████████▏                                                   | 126M/433M [00:00<00:01, 164MB/s]Downloading pytorch_model.bin:  34%|████████████████████████▋                                                | 147M/433M [00:00<00:01, 158MB/s]Downloading pytorch_model.bin:  39%|████████████████████████████▎                                            | 168M/433M [00:01<00:01, 155MB/s]Downloading pytorch_model.bin:  44%|███████████████████████████████▊                                         | 189M/433M [00:01<00:01, 149MB/s]Downloading pytorch_model.bin:  48%|███████████████████████████████████▎                                     | 210M/433M [00:01<00:01, 153MB/s]Downloading pytorch_model.bin:  53%|██████████████████████████████████████▊                                  | 231M/433M [00:01<00:01, 154MB/s]Downloading pytorch_model.bin:  58%|██████████████████████████████████████████▍                              | 252M/433M [00:01<00:01, 155MB/s]Downloading pytorch_model.bin:  63%|█████████████████████████████████████████████▉                           | 273M/433M [00:01<00:01, 145MB/s]Downloading pytorch_model.bin:  68%|█████████████████████████████████████████████████▍                       | 294M/433M [00:01<00:00, 148MB/s]Downloading pytorch_model.bin:  73%|████████████████████████████████████████████████████▉                    | 315M/433M [00:02<00:00, 149MB/s]Downloading pytorch_model.bin:  77%|████████████████████████████████████████████████████████▌                | 336M/433M [00:02<00:00, 153MB/s]Downloading pytorch_model.bin:  82%|████████████████████████████████████████████████████████████             | 357M/433M [00:02<00:00, 151MB/s]Downloading pytorch_model.bin:  87%|███████████████████████████████████████████████████████████████▌         | 377M/433M [00:02<00:00, 151MB/s]Downloading pytorch_model.bin:  92%|███████████████████████████████████████████████████████████████████▏     | 398M/433M [00:02<00:00, 154MB/s]Downloading pytorch_model.bin:  97%|██████████████████████████████████████████████████████████████████████▋  | 419M/433M [00:02<00:00, 158MB/s]Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████| 433M/433M [00:02<00:00, 155MB/s]
Downloading (…)okenizer_config.json:   0%|                                                                          | 0.00/49.0 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████████████████████████████| 49.0/49.0 [00:00<00:00, 25.5kB/s]
Downloading (…)solve/main/vocab.txt:   0%|                                                                          | 0.00/213k [00:00<?, ?B/s]Downloading (…)solve/main/vocab.txt: 100%|██████████████████████████████████████████████████████████████████| 213k/213k [00:02<00:00, 93.2kB/s]Downloading (…)solve/main/vocab.txt: 100%|██████████████████████████████████████████████████████████████████| 213k/213k [00:02<00:00, 93.1kB/s]
Downloading (…)cial_tokens_map.json:   0%|                                                                           | 0.00/112 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 92.2kB/s]
Traceback (most recent call last):
  File "./f00339_extract_features_biobert.py", line 38, in <module>
    test_extract_features_biobert()
  File "./f00339_extract_features_biobert.py", line 33, in test_extract_features_biobert
    assert features.shape[1] == len(text.split()), 'The number of tokens in the output should match the number of words in the input.'
AssertionError: The number of tokens in the output should match the number of words in the input.
