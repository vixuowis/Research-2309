2023-11-11 23:41:27.760100: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-11 23:41:27.821230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-11 23:41:28.797466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Downloading (…)lve/main/config.json:   0%|                                                                         | 0.00/1.39k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████| 1.39k/1.39k [00:00<00:00, 105kB/s]
Downloading pytorch_model.bin:   0%|                                                                               | 0.00/1.02G [00:00<?, ?B/s]Downloading pytorch_model.bin:   1%|▋                                                                     | 10.5M/1.02G [00:07<11:53, 1.42MB/s]Downloading pytorch_model.bin:   2%|█▍                                                                    | 21.0M/1.02G [00:19<16:25, 1.02MB/s]Downloading pytorch_model.bin:   2%|█▍                                                                    | 21.0M/1.02G [00:35<16:25, 1.02MB/s]Downloading pytorch_model.bin:   3%|██▏                                                                    | 31.5M/1.02G [00:38<22:06, 749kB/s]Downloading pytorch_model.bin:   3%|██▏                                                                    | 31.5M/1.02G [00:55<22:06, 749kB/s]Downloading pytorch_model.bin:   4%|██▉                                                                    | 41.9M/1.02G [01:03<28:54, 567kB/s]Downloading pytorch_model.bin:   4%|██▉                                                                    | 41.9M/1.02G [01:15<28:54, 567kB/s]Downloading pytorch_model.bin:   5%|███▋                                                                   | 52.4M/1.02G [01:32<34:22, 471kB/s]Downloading pytorch_model.bin:   5%|███▋                                                                   | 52.4M/1.02G [01:45<34:22, 471kB/s]Downloading pytorch_model.bin:   6%|████▎                                                                  | 62.9M/1.02G [02:01<37:36, 426kB/s]Downloading pytorch_model.bin:   6%|████▎                                                                  | 62.9M/1.02G [02:15<37:36, 426kB/s]Downloading pytorch_model.bin:   7%|█████                                                                  | 73.4M/1.02G [02:37<42:50, 370kB/s]Downloading pytorch_model.bin:   7%|█████                                                                  | 73.4M/1.02G [02:55<42:50, 370kB/s]Downloading pytorch_model.bin:   8%|█████▊                                                                 | 83.9M/1.02G [03:12<45:39, 343kB/s]Downloading pytorch_model.bin:   8%|█████▊                                                                 | 83.9M/1.02G [03:25<45:39, 343kB/s]Downloading pytorch_model.bin:   9%|██████▌                                                                | 94.4M/1.02G [03:56<51:15, 303kB/s]Downloading pytorch_model.bin:   9%|██████▌                                                                | 94.4M/1.02G [04:15<51:15, 303kB/s]Downloading pytorch_model.bin:  10%|███████▎                                                                | 105M/1.02G [04:39<54:11, 283kB/s]Downloading pytorch_model.bin:  10%|███████▎                                                                | 105M/1.02G [04:55<54:11, 283kB/s]Downloading pytorch_model.bin:  11%|████████                                                                | 115M/1.02G [05:13<52:33, 288kB/s]Downloading pytorch_model.bin:  11%|████████                                                                | 115M/1.02G [05:25<52:33, 288kB/s]Downloading pytorch_model.bin:  12%|████████▊                                                               | 126M/1.02G [05:46<50:19, 298kB/s]Downloading pytorch_model.bin:  12%|████████▊                                                               | 126M/1.02G [06:05<50:19, 298kB/s]Downloading pytorch_model.bin:  13%|█████████▌                                                              | 136M/1.02G [06:14<46:35, 318kB/s]Downloading pytorch_model.bin:  13%|█████████▌                                                              | 136M/1.02G [06:25<46:35, 318kB/s]Downloading pytorch_model.bin:  14%|██████████                                                            | 147M/1.02G [07:32<1:05:11, 224kB/s]Downloading pytorch_model.bin:  14%|██████████▏                                                           | 148M/1.02G [07:40<1:05:32, 223kB/s]Downloading pytorch_model.bin:  14%|██████████▍                                                             | 148M/1.02G [07:40<45:20, 322kB/s]
Downloading pytorch_model.bin:   0%|                                                                               | 0.00/1.02G [00:00<?, ?B/s]Downloading pytorch_model.bin:   1%|▋                                                                     | 10.5M/1.02G [00:07<12:51, 1.31MB/s]Downloading pytorch_model.bin:   2%|█▍                                                                    | 21.0M/1.02G [00:16<13:38, 1.23MB/s]Downloading pytorch_model.bin:   2%|█▍                                                                    | 21.0M/1.02G [00:32<13:38, 1.23MB/s]Downloading pytorch_model.bin:   3%|██▏                                                                    | 31.5M/1.02G [00:43<26:28, 625kB/s]Downloading pytorch_model.bin:   3%|██▏                                                                    | 31.5M/1.02G [01:02<26:28, 625kB/s]Downloading pytorch_model.bin:   3%|██▏                                                                    | 31.5M/1.02G [01:26<45:44, 362kB/s]
Traceback (most recent call last):
  File "./f00396_filter_inappropriate_messages.py", line 34, in <module>
    test_filter_inappropriate_messages()
  File "./f00396_filter_inappropriate_messages.py", line 27, in test_filter_inappropriate_messages
    assert filter_inappropriate_messages('Hello, how are you?') == 'Safe message.'
  File "./f00396_filter_inappropriate_messages.py", line 17, in filter_inappropriate_messages
    classifier = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 824, in pipeline
    framework, model = infer_framework_load_model(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 282, in infer_framework_load_model
    raise ValueError(
ValueError: Could not load model valhalla/distilbart-mnli-12-3 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bart.modeling_bart.BartForSequenceClassification'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForSequenceClassification'>). See the original errors:

while loading with AutoModelForSequenceClassification, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 557, in http_get
    raise EnvironmentError(
OSError: Consistency check failed: file should be of size 1024804435 but has size 148374766 (pytorch_model.bin).
We are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.
If the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.

while loading with TFAutoModelForSequenceClassification, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_tf_utils.py", line 2820, in from_pretrained
    raise EnvironmentError(
OSError: valhalla/distilbart-mnli-12-3 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.

while loading with BartForSequenceClassification, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 444, in _error_catcher
    yield
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 567, in read
    data = self._fp_read(amt) if not fp_closed else b""
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 533, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
  File "/root/miniconda3/envs/py38/lib/python3.8/http/client.py", line 459, in read
    n = self.readinto(b)
  File "/root/miniconda3/envs/py38/lib/python3.8/http/client.py", line 503, in readinto
    n = self.fp.readinto(b)
  File "/root/miniconda3/envs/py38/lib/python3.8/socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "/root/miniconda3/envs/py38/lib/python3.8/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/root/miniconda3/envs/py38/lib/python3.8/ssl.py", line 1132, in read
    return self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 816, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 628, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 593, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File "/root/miniconda3/envs/py38/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py", line 449, in _error_catcher
    raise ReadTimeoutError(self._pool, None, "Read timed out.")
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 551, in http_get
    for chunk in r.iter_content(chunk_size=10 * 1024 * 1024):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/models.py", line 822, in generate
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.

while loading with TFBartForSequenceClassification, an error is thrown:
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/root/miniconda3/envs/py38/lib/python3.8/ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "/root/miniconda3/envs/py38/lib/python3.8/ssl.py", line 1073, in _create
    self.do_handshake()
  File "/root/miniconda3/envs/py38/lib/python3.8/ssl.py", line 1342, in do_handshake
    self._sslobj.do_handshake()
socket.timeout: _ssl.c:1114: The handshake operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /valhalla/distilbart-mnli-12-3/resolve/main/pytorch_model.bin (Caused by ProxyError('Cannot connect to proxy.', timeout('_ssl.c:1114: The handshake operation timed out')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_tf_utils.py", line 2819, in from_pretrained
    if has_file(pretrained_model_name_or_path, WEIGHTS_NAME, **has_file_kwargs):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 647, in has_file
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=10)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/api.py", line 100, in head
    return request("head", url, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /valhalla/distilbart-mnli-12-3/resolve/main/pytorch_model.bin (Caused by ProxyError('Cannot connect to proxy.', timeout('_ssl.c:1114: The handshake operation timed out')))



