{'code': "###Instruction: Create a personalized online ad campaign for our new clothing line. The ad should generate a video showing a man wearing a stylish suit while walking in the city.\n###Output: <<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video-synthesis', model='modelscope-damo-text-to-video-synthesis')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. We import the necessary libraries and functions, including the Hugging Face pipeline and snapshot_download functions.\n2. We use the snapshot_download function to download the pretrained model 'modelscope-damo-text-to-video-synthesis'.\n3. We create a pipeline using the 'text-to-video-synthesis' functionality and the downloaded model.\n4. As input text, we provide a description for the desired ad video - in this case, 'A man wearing a stylish suit while walking in the city.'\n5. After processing the input text, the pipeline generates the output video based on the provided text description, which can be used in the ad campaign.\n<<<code>>>: from huggingface_hub import snapshot_download\nfrom modelscope.pipelines import pipeline\nfrom modelscope.outputs import OutputKeys\nimport pathlib\n\nmodel_dir = pathlib.Path('weights')\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis', repo_type='model', local_dir=model_dir)\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\n\ninput_text = {'text': 'A man wearing a stylish suit while walking in the city.'}\noutput_video_path = pipe(input_text,)[OutputKeys.OUTPUT_VIDEO]\nprint('output_video_path:', output_video_path)", 'api_call': "pipeline('text-to-video-synthesis', model_dir.as_posix())", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'modelscope-damo-text-to-video-synthesis', 'api_call': "pipeline('text-to-video-synthesis', model_dir.as_posix())", 'api_arguments': {'text': 'A short text description in English'}, 'python_environment_requirements': ['modelscope==1.4.2', 'open_clip_torch', 'pytorch-lightning'], 'example_code': "from huggingface_hub import snapshot_download\nfrom modelscope.pipelines import pipeline\nfrom modelscope.outputs import OutputKeys\nimport pathlib\n\nmodel_dir = pathlib.Path('weights')\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis',\n repo_type='model', local_dir=model_dir)\n\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\n\ntest_text = {\n 'text': 'A panda eating bamboo on a rock.',\n}\n\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\nprint('output_video_path:', output_video_path)", 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00830_generate_ad_video.py", line 4, in <module>
    from modelscope.pipelines import pipeline
ModuleNotFoundError: No module named 'modelscope'
