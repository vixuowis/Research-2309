{'code': '###Instruction: I\'m talking to a new person online. Can this API help keep me safe by filtering out any inappropriate messages they send me?\n###Output: <<<domain>>>: Natural Language Processing Zero-Shot Classification\n<<<api_call>>>: pipeline(\'zero-shot-classification\', model=\'valhalla/distilbart-mnli-12-3\')\n<<<api_provider>>>: Transformers\n<<<explanation>>>: 1. The pipeline function from the transformers library is imported. This is a function provided by Hugging Face that allows us to load pre-trained models for various tasks.\n2. We use the \'pipeline\' function to create a zero-shot classification model with the model \'valhalla/distilbart-mnli-12-3\'. This model is capable of classifying text into a number of categories based on semantic similarity.\n3. The created classifier can be used to identify inappropriate messages from the conversation.\n4. You can set up categories like \'safe\' and \'inappropriate\' and then use the classifier to categorize messages based on their content.\n<<<code>>>: from transformers import pipeline\nclassifier = pipeline(\'zero-shot-classification\', model=\'valhalla/distilbart-mnli-12-3\')\nmessage_classification = classifier(message_text, candidate_labels=[\'safe\', \'inappropriate\'])\nif message_classification[\'labels\'][0] == \'inappropriate\':\n    print("Warning: Inappropriate message detected.")\nelse:\n    print("Safe message.")\n', 'api_call': "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'valhalla/distilbart-mnli-12-3', 'api_call': "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': [{'name': 'matched acc', 'accuracy': 88.1}, {'name': 'mismatched acc', 'accuracy': 88.19}]}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop.'}}

