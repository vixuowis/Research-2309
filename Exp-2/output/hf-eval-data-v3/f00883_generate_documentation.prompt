{'code': "###Instruction: We are building a platform for developers and want to provide automatic code documentation generation for Python functions.\n###Output: <<<domain>>>: Natural Language Processing Summarization\n<<<api_call>>>: AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\n<<<api_provider>>>: Transformers\n<<<explanation>>>:1. Import the necessary libraries from the transformers package, including AutoTokenizer and AutoModelWithLMHead.\n2. Create a SummarizationPipeline object by specifying the pretrained model 'SEBIS/code_trans_t5_base_code_documentation_generation_python' and its tokenizer. This model has been trained on Python code and can generate summaries for a given piece of code.\n3. Pass the code snippet (Python function) as input to the pipeline, which will then generate a summary or documentation based on the code.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\npipeline = SummarizationPipeline(\n    model=AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python'),\n    tokenizer=AutoTokenizer.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python', skip_special_tokens=True),\n    device=0\n)\ntokenized_code = 'def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)'\ngenerated_documentation = pipeline([tokenized_code])\n", 'api_call': "AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'Code Documentation Generation', 'api_name': 'code_trans_t5_base_code_documentation_generation_python', 'api_call': "AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')", 'api_arguments': ['tokenized_code'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\npipeline = SummarizationPipeline(\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\n device=0\n)\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\npipeline([tokenized_code])', 'performance': {'dataset': 'CodeSearchNet Corpus python dataset', 'accuracy': '20.26 BLEU score'}, 'description': 'This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.'}}

/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1479: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  warnings.warn(
Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00883_generate_documentation.py", line 36, in <module>
    test_generate_documentation()
  File "output/hf-eval-data-v2/f00883_generate_documentation.py", line 32, in test_generate_documentation
    assert generate_documentation(tokenized_code) == expected_documentation
  File "output/hf-eval-data-v2/f00883_generate_documentation.py", line 18, in generate_documentation
    model=AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python'),
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py", line 1485, in from_pretrained
    return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 494, in from_pretrained
    resolved_config_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1193, in hf_hub_download
    os.makedirs(storage_folder, exist_ok=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/os.py", line 223, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: '/root/autodl-tmp/.cache/huggingface/hub/models--SEBIS--code_trans_t5_base_code_documentation_generation_python'
