{'code': "###Instruction: I am making a keyword search engine that ranks text passages based on their importance regarding a given keyword.\n###Output: <<<domain>>>: Natural Language Processing Text Classification\n<<<api_call>>>: AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the required libraries AutoTokenizer and AutoModelForSequenceClassification from the transformers library provided by Hugging Face.\n2. Load the pretrained model 'cross-encoder/ms-marco-MiniLM-L-12-v2' using the AutoModelForSequenceClassification.from_pretrained() function. This model has been specifically trained for Information Retrieval tasks like ranking text passages based on their importance.\n3. Load the associated tokenizer using the AutoTokenizer.from_pretrained() function.\n4. Tokenize the query and the set of possible passages using this tokenizer, making sure to include padding, truncation, and returning tensors in the PyTorch format.\n5. Use the pretrained model to score each passage in the context of the given query. The model outputs logits that can be used to rank the passages.\n6. Sort the passages in decreasing order of scores.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\nfeatures = tokenizer(query, passages, padding=True, truncation=True, return_tensors='pt')\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n   \n# Sort passages based on scores\nsorted_passages = [passage for _, passage in sorted(zip(scores, passages), reverse=True)]\n", 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Information Retrieval', 'api_name': 'cross-encoder/ms-marco-MiniLM-L-12-v2', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')", 'api_arguments': {'padding': 'True', 'truncation': 'True', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification', 'torch': 'import torch'}, 'example_code': "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)", 'performance': {'dataset': {'TREC Deep Learning 2019': {'NDCG@10': 74.31}, 'MS Marco Passage Reranking': {'MRR@10': 39.02, 'accuracy': '960 Docs / Sec'}}}, 'description': 'This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00674_rank_passages.py", line 47, in <module>
    test_rank_passages()
  File "output/hf-eval-data-v2/f00674_rank_passages.py", line 41, in test_rank_passages
    result = rank_passages(query, passages)
  File "output/hf-eval-data-v2/f00674_rank_passages.py", line 20, in rank_passages
    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 494, in from_pretrained
    resolved_config_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1193, in hf_hub_download
    os.makedirs(storage_folder, exist_ok=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/os.py", line 223, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: '/root/autodl-tmp/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L-12-v2'
