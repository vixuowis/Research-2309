# function_import --------------------

from transformers import AutoModelForCausalLM, AutoTokenizer

# function_code --------------------

def respond_to_message(input_message):
    """
    This function takes an input message as a string, tokenizes it, passes it through a pre-trained conversational AI model,
    and returns a response generated by the model.

    Args:
        input_message (str): The input message that needs to be processed by the model.

    Returns:
        str: The response generated by the model.

    Raises:
        IndexError: An error occurs if the tokenized input is out of range for the model.
    """
    tokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')
    model = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')
    tokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')
    output = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)
    return response

# test_function_code --------------------

def test_respond_to_message():
    """
    This function tests the 'respond_to_message' function with different test cases.
    """
    test_case_1 = 'Turn on the air conditioner.'
    test_case_2 = 'What is the weather like today?'
    test_case_3 = 'Tell me a joke.'
    assert isinstance(respond_to_message(test_case_1), str)
    assert isinstance(respond_to_message(test_case_2), str)
    assert isinstance(respond_to_message(test_case_3), str)
    return 'All Tests Passed'

# call_test_function_code --------------------

test_respond_to_message()