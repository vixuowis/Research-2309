{'code': "###Instruction: We want to build a product to classify images of pets into different categories.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary libraries, which include 'Image' from the PIL package and 'CLIPProcessor' and 'CLIPModel' from the transformers package.\n2. Initialize the pre-trained CLIP model by calling the 'CLIPModel.from_pretrained' method with the specified model name 'openai/clip-vit-large-patch14'.\n3. Preprocess the input data by using the 'CLIPProcessor.from_pretrained' method to tokenize the image and text input.\n4. In this case, the text input can be a list of categories for the pet images (e.g., 'a photo of a cat' or 'a photo of a dog').\n5. Finally, pass the preprocessed inputs to the model and obtain the classification probabilities, which indicate the likelihood of the image belonging to each specified category.\n<<<code>>>: from PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\nprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\nimage = Image.open('pet_image_path.jpg')\n# replace 'pet_image_path.jpg' with the path to your image\ninputs = processor(text=['a photo of a cat', 'a photo of a dog'], images=image, return_tensors='pt', padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)", 'api_call': "CLIPModel.from_pretrained('openai/clip-vit-large-patch14')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'openai/clip-vit-large-patch14', 'api_call': "CLIPModel.from_pretrained('openai/clip-vit-large-patch14')", 'api_arguments': {'text': ['a photo of a cat', 'a photo of a dog'], 'images': 'image', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': {'packages': ['PIL', 'requests', 'transformers']}, 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': ['Food101', 'CIFAR10', 'CIFAR100', 'Birdsnap', 'SUN397', 'Stanford Cars', 'FGVC Aircraft', 'VOC2007', 'DTD', 'Oxford-IIIT Pet dataset', 'Caltech101', 'Flowers102', 'MNIST', 'SVHN', 'IIIT5K', 'Hateful Memes', 'SST-2', 'UCF101', 'Kinetics700', 'Country211', 'CLEVR Counting', 'KITTI Distance', 'STL-10', 'RareAct', 'Flickr30', 'MSCOCO', 'ImageNet', 'ImageNet-A', 'ImageNet-R', 'ImageNet Sketch', 'ObjectNet (ImageNet Overlap)', 'Youtube-BB', 'ImageNet-Vid'], 'accuracy': 'varies depending on the dataset'}, 'description': 'The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.'}}


`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.

