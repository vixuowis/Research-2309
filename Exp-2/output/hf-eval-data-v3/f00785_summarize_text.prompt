{'code': '###Instruction: In this fast world, a user spends very little time on reading news articles and requires summary of the news articles.\n###Output: <<<domain>>>: Natural Language Processing Summarization\n<<<api_call>>>: pipeline(\'summarization\', model=\'facebook/bart-large-cnn\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Firstly, import the \'pipeline\' function from the transformers library provided by Hugging Face.\n2. Use the \'pipeline\' function to create a text summarization model.\n3. Specify the model \'facebook/bart-large-cnn\' to be loaded. This is a pretrained model fine-tuned on the CNN Daily Mail dataset for text summarization.\n4. Use the created summarizer to generate a summary of news articles with variable max_length, min_length, and do_sample options to configure the summary output.\n <<<code>>>: from transformers import pipeline\nsummarizer = pipeline(\'summarization\', model=\'facebook/bart-large-cnn\')\narticle = "News article text goes here..."\nsummary = summarizer(article, max_length=130, min_length=30, do_sample=False)[0][\'summary_text\']\n', 'api_call': "pipeline('summarization', model='facebook/bart-large-cnn')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Summarization', 'api_name': 'facebook/bart-large-cnn', 'api_call': "pipeline('summarization', model='facebook/bart-large-cnn')", 'api_arguments': ['ARTICLE', 'max_length', 'min_length', 'do_sample'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\nARTICLE = ...\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))', 'performance': {'dataset': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 42.949, 'ROUGE-2': 20.815, 'ROUGE-L': 30.619, 'ROUGE-LSUM': 40.038}}, 'description': 'BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.'}}

