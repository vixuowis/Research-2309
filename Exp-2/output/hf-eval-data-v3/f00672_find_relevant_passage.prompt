{'code': '###Instruction: I need a function to find the most relevent passage given a question and several candidate passages.\n###Output: <<<domain>>>: Natural Language Processing Text Classification\n<<<api_call>>>: AutoModelForSequenceClassification.from_pretrained(\'cross-encoder/ms-marco-MiniLM-L-6-v2\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries: AutoTokenizer, and AutoModelForSequenceClassification from transformers and torch.\n2. Load the pre-trained model cross-encoder/ms-marco-MiniLM-L-6-v2 using the AutoModelForSequenceClassification.from_pretrained function.\n3. Load the pre-trained tokenizer using the AutoTokenizer.from_pretrained function.\n4. Tokenize the given question and candidate passages with the tokenizer. The tokenizer will return input features that are compatible with the model.\n5. Evaluate the model with the tokenized features. The model outputs a logits score for each passage, which represents the relevance of each passage to the given question.\n6. Sort the passages in descending order based on the logits score.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained(\'cross-encoder/ms-marco-MiniLM-L-6-v2\')\ntokenizer = AutoTokenizer.from_pretrained(\'cross-encoder/ms-marco-MiniLM-L-6-v2\')\nquestion = "How many people live in Berlin?"\ncandidate_passages = ["Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.", "New York City is famous for the Metropolitan Museum of Art."]\nfeatures = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors=\'pt\')\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\nprint(sorted_passages[0])', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Information Retrieval', 'api_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')", 'api_arguments': {'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)", 'performance': {'dataset': 'MS Marco Passage Reranking', 'accuracy': 'MRR@10: 39.01%'}, 'description': 'This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order.'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00672_find_relevant_passage.py", line 40, in <module>
    test_find_relevant_passage()
  File "output/hf-eval-data-v2/f00672_find_relevant_passage.py", line 36, in test_find_relevant_passage
    assert isinstance(find_relevant_passage(question, candidate_passages), str)
  File "output/hf-eval-data-v2/f00672_find_relevant_passage.py", line 19, in find_relevant_passage
    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 494, in from_pretrained
    resolved_config_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1193, in hf_hub_download
    os.makedirs(storage_folder, exist_ok=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/os.py", line 223, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: '/root/autodl-tmp/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L-6-v2'
