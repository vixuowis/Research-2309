{'code': "###Instruction: Your client is a podcast aggregator website looking to transcribe episodes and display transcriptions on the site. Help them transcribe an audio file containing spoken English words.\n###Output: <<<domain>>>: Audio Automatic Speech Recognition\n<<<api_call>>>: WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import the necessary classes from the transformers package. This includes WhisperProcessor and WhisperForConditionalGeneration for the speech-to-text model.\n2. We then use the from_pretrained method to load the pre-trained model 'openai/whisper-medium'. This model has been trained for automatic speech recognition tasks, which is useful for transcribing spoken audio from podcasts.\n3. We can then process the audio file and apply the model to generate a transcription of the content.\n4. This transcription can then be used to display an easily readable text version of the podcast episode on the aggregator website.\n<<<code>>>: from transformers import WhisperProcessor, WhisperForConditionalGeneration\n\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-medium')\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\n\naudio_file_path = 'path/to/your/audio/file.wav'\naudio_data, sampling_rate = librosa.load(audio_file_path, sr=None)\ninput_features = processor(audio_data, sampling_rate=sampling_rate, return_tensors='pt').input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)", 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-medium', 'api_call': "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')", 'api_arguments': ['sample', 'sampling_rate', 'language', 'task', 'skip_special_tokens'], 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 2.9}, {'name': 'LibriSpeech (other)', 'accuracy': 5.9}, {'name': 'Common Voice 11.0', 'accuracy': 53.87}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00529_transcribe_audio.py", line 4, in <module>
    import librosa
ModuleNotFoundError: No module named 'librosa'
