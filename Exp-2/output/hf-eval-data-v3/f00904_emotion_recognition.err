2023-11-12 20:31:17.461103: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-12 20:31:17.528593: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-12 20:31:18.524392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Downloading (…)lve/main/config.json:   0%|                                                                                     | 0.00/2.15k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████████████████| 2.15k/2.15k [00:00<00:00, 201kB/s]
/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:381: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Downloading pytorch_model.bin:   0%|                                                                                            | 0.00/378M [00:00<?, ?B/s]Downloading pytorch_model.bin:   6%|████▋                                                                               | 21.0M/378M [00:00<00:01, 187MB/s]Downloading pytorch_model.bin:  11%|█████████▎                                                                          | 41.9M/378M [00:00<00:01, 181MB/s]Downloading pytorch_model.bin:  19%|████████████████▎                                                                   | 73.4M/378M [00:00<00:01, 206MB/s]Downloading pytorch_model.bin:  28%|███████████████████████▌                                                             | 105M/378M [00:00<00:01, 221MB/s]Downloading pytorch_model.bin:  36%|██████████████████████████████▌                                                      | 136M/378M [00:00<00:01, 222MB/s]Downloading pytorch_model.bin:  44%|█████████████████████████████████████▋                                               | 168M/378M [00:00<00:00, 231MB/s]Downloading pytorch_model.bin:  53%|████████████████████████████████████████████▊                                        | 199M/378M [00:00<00:00, 238MB/s]Downloading pytorch_model.bin:  61%|███████████████████████████████████████████████████▊                                 | 231M/378M [00:01<00:00, 236MB/s]Downloading pytorch_model.bin:  69%|██████████████████████████████████████████████████████████▉                          | 262M/378M [00:01<00:00, 233MB/s]Downloading pytorch_model.bin:  78%|█████████████████████████████████████████████████████████████████▉                   | 294M/378M [00:01<00:00, 240MB/s]Downloading pytorch_model.bin:  86%|█████████████████████████████████████████████████████████████████████████            | 325M/378M [00:01<00:00, 243MB/s]Downloading pytorch_model.bin:  94%|████████████████████████████████████████████████████████████████████████████████     | 357M/378M [00:01<00:00, 241MB/s]Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████| 378M/378M [00:01<00:00, 229MB/s]
Some weights of the model checkpoint at superb/wav2vec2-base-superb-er were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)rocessor_config.json:   0%|                                                                                       | 0.00/215 [00:00<?, ?B/s]Downloading (…)rocessor_config.json: 100%|████████████████████████████████████████████████████████████████████████████████| 215/215 [00:00<00:00, 84.5kB/s]
Traceback (most recent call last):
  File "./f00904_emotion_recognition.py", line 42, in <module>
    test_emotion_recognition()
  File "./f00904_emotion_recognition.py", line 29, in test_emotion_recognition
    emotion_label = emotion_recognition('valid_audio_file.wav')
  File "./f00904_emotion_recognition.py", line 19, in emotion_recognition
    emotion_label = emotion_classifier(audio_file_path, top_k=top_k)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/audio_classification.py", line 136, in __call__
    return super().__call__(inputs, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1140, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1146, in run_single
    model_inputs = self.preprocess(inputs, **preprocess_params)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/audio_classification.py", line 154, in preprocess
    with open(inputs, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'valid_audio_file.wav'
