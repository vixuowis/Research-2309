{'code': "###Instruction: We're developing a chatbot that can quickly identify and describe images for our Chinese-speaking users.\n###Output: <<<domain>>>: Natural Language Processing Zero-Shot Image Classification\n<<<api_call>>>: ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import the necessary classes from the transformers, PIL and requests packages. This includes ChineseCLIPModel for the image classification model and Image for processing image data.\n2. We then use the from_pretrained method of the ChineseCLIPModel class to load the pre-trained model 'OFA-Sys/chinese-clip-vit-large-patch14-336px'. This model is designed for zero-shot image classification tasks and is trained specifically for Chinese image-text pairs.\n3. We process an image and text using the ChineseCLIPProcessor from the transformers package. The image is opened with the Image class and can be acquired from a URL or a file path.\n4. The model then classifies the image based on semantic similarity to the text, providing the result to the chatbot's user.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\nimage = Image.open(requests.get(image_url, stream=True).raw)\ntexts = ['文本描述1', '文本描述2', '文本描述3']\ninputs = processor(text=texts, images=image, return_tensors='pt', padding=True)\noutputs = model(**inputs)\nprobs = outputs.logits_per_image.softmax(dim=1)\n", 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'OFA-Sys/chinese-clip-vit-large-patch14-336px', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')", 'api_arguments': {'images': 'image', 'text': 'texts', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = []\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'CIFAR10': 96.0, 'CIFAR100': 79.75, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}, 'accuracy': 'various'}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.'}}


`vision_config_dict` is provided which will be used to initialize `ChineseCLIPVisionConfig`. The value `vision_config["model_type"]` will be overriden.

