{'code': '###Instruction: A Chinese social media company has approached us for help in content moderation. We recorded an API to classify images which don\'t belong to suitable content.\n###Output: <<<domain>>>: Natural Language Processing Zero-Shot Classification\n<<<api_call>>>: ChineseCLIPModel.from_pretrained(\'OFA-Sys/chinese-clip-vit-large-patch14-336px\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the required classes from the transformers and PIL packages. This includes ChineseCLIPModel for the object detection model and Image for processing image data.\n2. We then use the from_pretrained method of the ChineseCLIPModel class to load the pre-trained model \'OFA-Sys/chinese-clip-vit-large-patch14-336px\'. This model has been trained for zero-shot image classification tasks, which is exactly what we need for content moderation.\n3. We load the image data from a file or acquire it in real-time from the platform\'s content.\n4. This model can then be used to analyze an image and classify it based on the given labels.\n<<<code>>>: from transformers import ChineseCLIPProcessor, ChineseCLIPModel\nfrom PIL import Image\nmodel = ChineseCLIPModel.from_pretrained(\'OFA-Sys/chinese-clip-vit-large-patch14-336px\')\nprocessor = ChineseCLIPProcessor.from_pretrained(\'OFA-Sys/chinese-clip-vit-large-patch14-336px\')\nimage = Image.open(\'image_path.jpg\')\n# replace \'image_path.jpg\' with path to your image\ntexts = ["label1", "label2", "label3"]\ninputs = processor(images=image, text=texts, return_tensors="pt", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\n', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'OFA-Sys/chinese-clip-vit-large-patch14-336px', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')", 'api_arguments': {'images': 'image', 'text': 'texts', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = []\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'CIFAR10': 96.0, 'CIFAR100': 79.75, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}, 'accuracy': 'various'}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.'}}

`vision_config_dict` is provided which will be used to initialize `ChineseCLIPVisionConfig`. The value `vision_config["model_type"]` will be overriden.
