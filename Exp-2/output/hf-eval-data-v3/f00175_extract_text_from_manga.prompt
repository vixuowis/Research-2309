{'code': "###Instruction: A new manga has been released and we would like to provide a manga reader app with translations. Get the text from the manga image.\n###Output: <<<domain>>>: Multimodal Image-to-Text\n<<<api_call>>>: pipeline('ocr', model='kha-white/manga-ocr-base')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. We can utilize the Manga OCR API, which is designed for recognizing Japanese text in manga images.\n2. First, import the pipeline function from the transformers library provided by Hugging Face.\n3. Create an OCR pipeline using the 'kha-white/manga-ocr-base' model.\n4. Pass the manga image as input to the pipeline, which will return the extracted text from the manga image.\n5. This text can then be used to provide translations for the manga reader app.\n<<<code>>>: from transformers import pipeline\nocr_pipeline = pipeline('ocr', model='kha-white/manga-ocr-base')\nmanga_text = ocr_pipeline(manga_image)\n", 'api_call': "pipeline('ocr', model='kha-white/manga-ocr-base')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': "pipeline('ocr', model='kha-white/manga-ocr-base')", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.'}}


Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00175_extract_text_from_manga.py", line 40, in <module>
    test_extract_text_from_manga()
  File "output/hf-eval-data-v2/f00175_extract_text_from_manga.py", line 35, in test_extract_text_from_manga
    result = extract_text_from_manga(test_image)
  File "output/hf-eval-data-v2/f00175_extract_text_from_manga.py", line 21, in extract_text_from_manga
    ocr_pipeline = pipeline('ocr', model='kha-white/manga-ocr-base')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 780, in pipeline
    normalized_task, targeted_task, task_options = check_task(task)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 499, in check_task
    return PIPELINE_REGISTRY.check_task(task)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1215, in check_task
    raise KeyError(
KeyError: "Unknown task ocr, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
