{'code': '###Instruction: Use this API to get a suggestion on how to respond to a customer\'s complaint about the late delivery of their package.\n###Input: {"instruction": "How can I respond to a customer complaint about late delivery?", "knowledge": "The courier had external delays due to bad winter weather.", "dialog": ["Customer: My package is late. What\'s going on?", "Support: I apologize for the inconvenience. I\'ll check what\'s happening with the package and get back to you."]}\n\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: AutoModelForSeq2SeqLM.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import `AutoTokenizer` and `AutoModelForSeq2SeqLM` from the transformers library.\n2. Load the tokenizer and the model using the `\'microsoft/GODEL-v1_1-base-seq2seq\'` checkpoint.\n3. Use the `generate` function with `instruction`, `knowledge`, and `dialog` as arguments.\n4. Format your input dialog as a string containing the conversation followed by the knowledge in the specified structure stated in the example_code.\n5. Use `model.generate` to get a generated output based on your inputs.\n6. Decode the generated output to get the suggested response.\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\'microsoft/GODEL-v1_1-base-seq2seq\')\n\ndef generate(instruction, knowledge, dialog):\n    knowledge = \'[KNOWLEDGE] \' + knowledge\n    dialog = \' EOS \'.join(dialog)\n    query = f"{instruction} [CONTEXT] {dialog} {knowledge}"\n    input_ids = tokenizer(query, return_tensors=\'pt\').input_ids\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return output\n\nresponse = generate("How can I respond to a customer complaint about late delivery?",\n                    "The courier had external delays due to bad winter weather.",\n                    ["Customer: My package is late. What\'s going on?",\n                     "Support: I apologize for the inconvenience. I\'ll check what\'s happening with the package and get back to you."])\n\nresponse', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/GODEL-v1_1-base-seq2seq', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'api_arguments': ['instruction', 'knowledge', 'dialog'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'N/A'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.'}}

