{'code': '###Instruction: You are building a virtual global tour guide that can identify languages from the audio of people speaking. Use a model to identify which language is being spoken.\n###Output: <<<domain>>>: Audio Classification\n<<<api_call>>>: AutoModelForSpeechClassification.from_pretrained(\'sanchit-gandhi/whisper-medium-fleurs-lang-id\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the necessary libraries: AutoModelForSpeechClassification and Wav2Vec2Processor from the transformers library.\n2. Load the pretrained language identification model \'sanchit-gandhi/whisper-medium-fleurs-lang-id\'.\n3. Load the corresponding Wav2Vec2Processor for preprocessing the audio data.\n4. Preprocess the input audio file using the processor.\n5. Pass the preprocessed audio data to the language identification model to predict the spoken language.\n6. The output will be the identified language.\n<<<code>>>: from transformers import AutoModelForSpeechClassification, Wav2Vec2Processor\nimport soundfile as sf\nmodel = AutoModelForSpeechClassification.from_pretrained(\'sanchit-gandhi/whisper-medium-fleurs-lang-id\')\nprocessor = Wav2Vec2Processor.from_pretrained(\'sanchit-gandhi/whisper-medium-fleurs-lang-id\')\naudio, sample_rate = sf.read(audio_file_path)\ninputs = processor(audio, sampling_rate=sample_rate, return_tensors="pt")\nlogits = model(**inputs).logits\npredicted_language_id = logits.argmax(-1).item()\n', 'api_call': "AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Language Identification', 'api_name': 'sanchit-gandhi/whisper-medium-fleurs-lang-id', 'api_call': "AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')", 'api_arguments': ["model = AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')", "processor = Wav2Vec2Processor.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')"], 'python_environment_requirements': ['transformers==4.27.0.dev0', 'pytorch==1.13.1', 'datasets==2.9.0', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'google/xtreme_s', 'accuracy': 0.8805}, 'description': 'This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset. It is used for language identification in audio classification tasks.'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00718_identify_spoken_language.py", line 3, in <module>
    from transformers import AutoModelForSpeechClassification, Wav2Vec2Processor
ImportError: cannot import name 'AutoModelForSpeechClassification' from 'transformers' (/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/__init__.py)
