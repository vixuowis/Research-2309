{'code': '###Instruction: We are building a social media site which creates automatic captions for users when they post a picture\n###Output: <<<domain>>>: Multimodal Image-to-Text\n<<<api_call>>>: VisionEncoderDecoderModel.from_pretrained(\'nlpconnect/vit-gpt2-image-captioning\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We import the necessary packages from the transformers library, such as VisionEncoderDecoderModel, ViTImageProcessor, and AutoTokenizer.\n2. Using the from_pretrained method, we load the pre-trained image captioning model \'nlpconnect/vit-gpt2-image-captioning\'.\n3. We create an instance of the ViTImageProcessor and AutoTokenizer classes to help with processing images and token handling.\n4. We use the model to generate captions for the input images, which could be the images that users post on the social media site.\n<<<code>>>: from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\'nlpconnect/vit-gpt2-image-captioning\')\nfeature_extractor = ViTImageProcessor.from_pretrained(\'nlpconnect/vit-gpt2-image-captioning\')\ntokenizer = AutoTokenizer.from_pretrained(\'nlpconnect/vit-gpt2-image-captioning\')\n\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\nmodel.to(device)\n\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {"max_length": max_length, "num_beams": num_beams}\n\ndef predict_caption(image_path):\n    input_image = Image.open(image_path)\n    if input_image.mode != "RGB":\n        input_image = input_image.convert(mode="RGB")\n\n    pixel_values = feature_extractor(images=[input_image], return_tensors="pt").pixel_values\n    pixel_values = pixel_values.to(device)\n    output_ids = model.generate(pixel_values, **gen_kwargs)\n    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    return caption.strip()\n\ncaption = predict_caption("sample_image.jpg")\n', 'api_call': "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.'}}

