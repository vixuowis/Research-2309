{'code': '###Instruction: We are developing an app that can help mediate virtual therapy sessions. We need a feature that can detect emotions in the voice of the user.\n###Output: <<<domain>>>: Audio Classification\n<<<api_call>>>: HubertForSpeechClassification.from_pretrained(\'Rajaram1996/Hubert_emotion\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary libraries, including HubertForSpeechClassification, Wav2Vec2FeatureExtractor, AudioSegment, and others required for audio processing and model inference.\n2. Load the pre-trained model, named \'Rajaram1996/Hubert_emotion\', using HubertForSpeechClassification\'s from_pretrained method.\n3. Load the necessary feature extractor for this model using Wav2Vec2FeatureExtractor\'s from_pretrained method. In this case, we\'ll use \'facebook/hubert-base-ls960\'.\n4. Define a function to convert an audio file into an array of samples with a given sampling rate.\n5. Then, use the feature extractor to process the audio file and generate input tensors for the model.\n6. Feed the input tensors to the model and retrieve the emotion predictions. Sort the predictions by their score and return the top predictions.\n7. You can use this function to analyze the emotions present in a user\'s voice during the virtual therapy sessions.\n<<<code>>>: from audio_models import HubertForSpeechClassification\nfrom transformers import Wav2Vec2FeatureExtractor, AutoConfig\nimport torch, torch.nn.functional as F, numpy as np\nfrom pydub import AudioSegment\n\nmodel = HubertForSpeechClassification.from_pretrained(\'Rajaram1996/Hubert_emotion\')\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\'facebook/hubert-base-ls960\')\nsampling_rate = 16000\nconfig = AutoConfig.from_pretrained(\'Rajaram1996/Hubert_emotion\')\n\ndef speech_file_to_array(path, sampling_rate):\n    sound = AudioSegment.from_file(path)\n    sound = sound.set_frame_rate(sampling_rate)\n    sound_array = np.array(sound.get_array_of_samples())\n    return sound_array\n\ndef predict_emotion_hubert(audio_file):\n    sound_array = speech_file_to_array(audio_file, sampling_rate)\n    inputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=\'pt\', padding=True)\n    inputs = {key: inputs[key].to(\'cpu\').float() for key in inputs}\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n    outputs = [{"emo": config.id2label[i], "score": round(score * 100, 1)} for i, score in enumerate(scores)]\n    return [row for row in sorted(outputs, key=lambda x: x[\'score\'], reverse=True) if row[\'score\'] != \'0.0\'][:2]\n\nresult = predict_emotion_hubert(\'male-crying.mp3\')\nresult\n', 'api_call': "HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'Rajaram1996/Hubert_emotion', 'api_call': "HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')", 'api_arguments': {'audio_file': 'string'}, 'python_environment_requirements': ['audio_models', 'transformers', 'torch', 'numpy', 'pydub'], 'example_code': "def predict_emotion_hubert(audio_file):\n from audio_models import HubertForSpeechClassification\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\n import torch.nn.functional as F\n import torch\n import numpy as np\n from pydub import AudioSegment\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\nsampling_rate=16000\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\ndef speech_file_to_array(path, sampling_rate):\n sound = AudioSegment.from_file(path)\n sound = sound.set_frame_rate(sampling_rate)\n sound_array = np.array(sound.get_array_of_samples())\n return sound_array\nsound_array = speech_file_to_array(audio_file, sampling_rate)\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\nwith torch.no_grad():\n logits = model(**inputs).logits\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\noutputs = [{\n emo: config.id2label[i],\n score: round(score * 100, 1)}\n for i, score in enumerate(scores)\n]\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\nresult = predict_emotion_hubert(male-crying.mp3)\nresult", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'A pretrained model for predicting emotion in local audio files using Hubert.'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00246_predict_emotion_hubert.py", line 3, in <module>
    from transformers import HubertForSpeechClassification, Wav2Vec2FeatureExtractor, AutoConfig
ImportError: cannot import name 'HubertForSpeechClassification' from 'transformers' (/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/__init__.py)
