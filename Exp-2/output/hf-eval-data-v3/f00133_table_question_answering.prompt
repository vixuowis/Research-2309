{'code': '###Instruction: A company is running a survey and they want to know how many respondents have given a specific answer for each question of the survey.\n###Output: <<<domain>>>: Natural Language Processing Table Question Answering\n<<<api_call>>>: pipeline(\'table-question-answering\', model=\'navteca/tapas-large-finetuned-wtq\', tokenizer=\'navteca/tapas-large-finetuned-wtq\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the necessary libraries, which include \'AutoModelForTableQuestionAnswering\', \'AutoTokenizer\', and \'pipeline\', provided by the transformers package.\n2. Load the pretrained \'navteca/tapas-large-finetuned-wtq\' model using \'AutoModelForTableQuestionAnswering.from_pretrained\' function and its corresponding tokenizer using \'AutoTokenizer.from_pretrained\' function.\n3. Create a table-question-answering pipeline by passing the loaded model and tokenizer to the \'pipeline\' function.\n4. Use the created pipeline to analyze survey results and get the number of respondents who have given a specific answer for each question.\n5. The model can parse and process the given survey table and take in queries, like "How many respondents chose option A for question 1?" and return the results.\n<<<code>>>: from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained(\'navteca/tapas-large-finetuned-wtq\')\ntapas_tokenizer = AutoTokenizer.from_pretrained(\'navteca/tapas-large-finetuned-wtq\')\n\nnlp = pipeline(\'table-question-answering\', model=tapas_model, tokenizer=tapas_tokenizer)\nsurvey_table = # Provide the survey table in appropriate format\nquery = "How many respondents chose option A for question 1?"\nresult = nlp({\'table\': survey_table, \'query\': query})\n', 'api_call': "AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Question Answering', 'api_name': 'navteca/tapas-large-finetuned-wtq', 'api_call': "AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')", 'api_arguments': {'table': 'table_data', 'query': 'query'}, 'python_environment_requirements': {'transformers': 'AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline'}, 'example_code': "from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model & tokenizer\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\n\n# Get predictions\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\nprint(result)", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not provided'}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.'}}



