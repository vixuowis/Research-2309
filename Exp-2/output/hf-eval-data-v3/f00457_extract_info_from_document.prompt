{'code': '###Instruction: We received a scanned document with a lot of information. We need to go through it and find answers to specific questions. Create a program to extract information from the document.\n###Output: <<<domain>>>: Multimodal Document Question Answering\n<<<api_call>>>: from transformers import LayoutLMv3ForQuestionAnswering\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(\'hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering\')\n<<<api_provider>>>: Transformers\n<<<explanation>>>: 1. Import the LayoutLMv3ForQuestionAnswering model from the transformers library.\n2. Load the \'hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering\' model, which is specifically designed for document question answering tasks.\n3. Once the model is loaded, preprocess the scanned document by converting it into the appropriate format and set up the questions to be asked.\n4. Pass the preprocessed document and questions to the model, and the model will generate the most probable answers based on the information in the document.\n<<<code>>>: from transformers import LayoutLMv3ForQuestionAnswering, LayoutLMv3Tokenizer\ntokenizer = LayoutLMv3Tokenizer.from_pretrained(\'hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering\')\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(\'hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering\')\n\n# Preprocess the scanned document\ndocument = "path/to/image/file"\nquestions = ["What is the total amount?", "When is the due date?"]\n\n# Prepare inputs and pass them to the model\nfor question in questions:\n    input_data = tokenizer(question, document, return_tensors="pt")\n    output = model(**input_data)\n    answer = tokenizer.convert_ids_to_tokens(output.start_logits.argmax(), output.end_logits.argmax() + 1)\n    print(f"Question: {question}\\nAnswer: {\' \'.join(answer)}")', 'api_call': "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')", 'provider': 'Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.'}}







Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00457_extract_info_from_document.py", line 52, in <module>
    test_extract_info_from_document()
  File "output/hf-eval-data-v2/f00457_extract_info_from_document.py", line 40, in test_extract_info_from_document
    answers = extract_info_from_document(document_path, questions)
  File "output/hf-eval-data-v2/f00457_extract_info_from_document.py", line 25, in extract_info_from_document
    input_data = tokenizer(question, document_path, return_tensors="pt")
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3.py", line 615, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).
