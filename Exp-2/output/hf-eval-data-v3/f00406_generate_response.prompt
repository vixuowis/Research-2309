{'code': '###Instruction: Can you give me advice on how to choose the best video game for me?\n###Input: {"instruction": "what is the best way to choose a video game?", "knowledge": "Some factors to consider when choosing a video game are personal preferences, genre, graphics, gameplay, storyline, platform, and reviews from other players or gaming websites.", "dialog": ["What type of video games do you prefer playing?", "I enjoy action-adventure games and a decent storyline."]}\n###Output: A good way to find the best video game for you is to first evaluate your personal preferences and narrow down your options according to the genre, in your case, action-adventure games with a decent storyline. Next, consider the graphics and gameplay, as these also contribute to the overall gaming experience. Make sure to check if the game is available on your preferred platform. Lastly, read reviews from other players or gaming websites to get an idea of the general consensus about the game before making your final decision.', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/GODEL-v1_1-base-seq2seq', 'api_call': "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')", 'api_arguments': ['instruction', 'knowledge', 'dialog'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'N/A'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.'}}

