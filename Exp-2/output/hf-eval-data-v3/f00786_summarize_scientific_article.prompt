{'code': '###Instruction: We need to summarize a scientific article. The input must include all the important points discussed in the article, and the result should be a concise abstraction of the content.\n###Output: <<<domain>>>: Natural Language Processing Summarization\n<<<api_call>>>: pipeline(\'summarization\', model=\'google/pegasus-large\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the pipeline function from the transformers package.\n2. Use the pipeline function to load the \'google/pegasus-large\' model for summarization tasks.\n3. The model has been pre-trained and fine-tuned for summarization tasks and can generate abstractive summaries of input text.\n4. The created summarizer can be used to process the provided scientific article and generate a concise summary of the important points discussed.\n\n<<<code>>>: from transformers import pipeline\nsummarizer = pipeline(\'summarization\', model=\'google/pegasus-large\')\narticle = "Here is the scientific article text..."\nsummary = summarizer(article)\n', 'api_call': "pipeline('summarization', model='google/pegasus-large')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Summarization', 'api_name': 'google/pegasus-large', 'api_call': "pipeline('summarization', model='google/pegasus-large')", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': "from transformers import pipeline\nsummarizer = pipeline('summarization', model='google/pegasus-large')\nsummary = summarizer('your_text_here')", 'performance': {'dataset': [{'name': 'xsum', 'accuracy': '47.60/24.83/39.64'}, {'name': 'cnn_dailymail', 'accuracy': '44.16/21.56/41.30'}, {'name': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, {'name': 'multi_news', 'accuracy': '47.65/18.75/24.95'}, {'name': 'gigaword', 'accuracy': '39.65/20.47/36.76'}, {'name': 'wikihow', 'accuracy': '46.39/22.12/38.41'}, {'name': 'reddit_tifu', 'accuracy': '27.99/9.81/22.94'}, {'name': 'big_patent', 'accuracy': '52.29/33.08/41.66'}, {'name': 'arxiv', 'accuracy': '44.21/16.95/25.67'}, {'name': 'pubmed', 'accuracy': '45.97/20.15/28.25'}, {'name': 'aeslc', 'accuracy': '37.68/21.25/36.51'}, {'name': 'billsum', 'accuracy': '59.67/41.58/47.59'}]}, 'description': 'google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks.'}}

Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00786_summarize_scientific_article.py", line 34, in <module>
    test_summarize_scientific_article()
  File "output/hf-eval-data-v2/f00786_summarize_scientific_article.py", line 28, in test_summarize_scientific_article
    summary = summarize_scientific_article(article)
  File "output/hf-eval-data-v2/f00786_summarize_scientific_article.py", line 17, in summarize_scientific_article
    summarizer = pipeline('summarization', model='google/pegasus-large')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 729, in pipeline
    maybe_adapter_path = find_adapter_config_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/peft_utils.py", line 87, in find_adapter_config_file
    adapter_cached_filename = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1193, in hf_hub_download
    os.makedirs(storage_folder, exist_ok=True)
  File "/root/miniconda3/envs/py38/lib/python3.8/os.py", line 223, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: '/root/autodl-tmp/.cache/huggingface/hub/models--google--pegasus-large'
