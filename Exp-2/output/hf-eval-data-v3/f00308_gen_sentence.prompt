{'code': '###Instruction: I am an English teacher. What kind of creative writing exercise could I give to my students that they could write a story using the words "moon", "rabbit", "forest", and "magic"?\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: AutoModelWithLMHead.from_pretrained(\'mrm8488/t5-base-finetuned-common_gen\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import the necessary packages, which include \'AutoModelWithLMHead\' and \'AutoTokenizer\' from the transformers library.\n2. We then load the pre-trained model \'mrm8488/t5-base-finetuned-common_gen\' using the \'AutoModelWithLMHead.from_pretrained\' method.\n3. We provide the words "moon", "rabbit", "forest", "magic" as input to the model and specify the maximum length for the text generation.\n4. The model generates a creative sentence with these words, which can be used as a prompt for your students in their writing exercise.\n<<<code>>>: from transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\'mrm8488/t5-base-finetuned-common_gen\')\nmodel = AutoModelWithLMHead.from_pretrained(\'mrm8488/t5-base-finetuned-common_gen\')\n\ndef gen_sentence(words, max_length=32):\n    input_text = words\n    features = tokenizer([input_text], return_tensors=\'pt\')\n    output = model.generate(input_ids=features[\'input_ids\'], attention_mask=features[\'attention_mask\'], max_length=max_length)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nwords = "moon rabbit forest magic"\nwriting_prompt = gen_sentence(words)', 'api_call': "AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-common_gen')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Generative Commonsense Reasoning', 'api_name': 'mrm8488/t5-base-finetuned-common_gen', 'api_call': "AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-common_gen')", 'api_arguments': ['words', 'max_length'], 'python_environment_requirements': ['transformers'], 'example_code': "from transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\ndef gen_sentence(words, max_length=32):\n input_text = words\n features = tokenizer([input_text], return_tensors='pt')\noutput = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'], max_length=max_length)\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\nwords = tree plant ground hole dig\ngen_sentence(words)", 'performance': {'dataset': 'common_gen', 'accuracy': {'ROUGE-2': 17.1, 'ROUGE-L': 39.47}}, 'description': "Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts."}}





You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1479: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  warnings.warn(

