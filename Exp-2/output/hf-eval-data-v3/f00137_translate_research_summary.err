Downloading (…)ve/main/spiece.model:   0%|                                                                          | 0.00/792k [00:00<?, ?B/s]Downloading (…)ve/main/spiece.model: 100%|███████████████████████████████████████████████████████████████████| 792k/792k [00:02<00:00, 379kB/s]Downloading (…)ve/main/spiece.model: 100%|███████████████████████████████████████████████████████████████████| 792k/792k [00:02<00:00, 379kB/s]
Downloading (…)okenizer_config.json:   0%|                                                                         | 0.00/2.32k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████████████████| 2.32k/2.32k [00:00<00:00, 201kB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading (…)lve/main/config.json:   0%|                                                                         | 0.00/1.21k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 112kB/s]
Downloading model.safetensors:   0%|                                                                                | 0.00/242M [00:00<?, ?B/s]Downloading model.safetensors:   4%|███                                                                    | 10.5M/242M [00:01<00:42, 5.41MB/s]Downloading model.safetensors:   9%|██████▏                                                                | 21.0M/242M [00:02<00:26, 8.42MB/s]Downloading model.safetensors:  13%|█████████▏                                                             | 31.5M/242M [00:03<00:20, 10.3MB/s]Downloading model.safetensors:  17%|████████████▎                                                          | 41.9M/242M [00:04<00:16, 12.0MB/s]Downloading model.safetensors:  22%|███████████████▍                                                       | 52.4M/242M [00:04<00:14, 13.5MB/s]Downloading model.safetensors:  26%|██████████████████▍                                                    | 62.9M/242M [00:05<00:12, 14.0MB/s]Downloading model.safetensors:  30%|█████████████████████▌                                                 | 73.4M/242M [00:06<00:11, 14.4MB/s]Downloading model.safetensors:  35%|████████████████████████▌                                              | 83.9M/242M [00:06<00:10, 14.9MB/s]Downloading model.safetensors:  39%|███████████████████████████▋                                           | 94.4M/242M [00:07<00:09, 15.1MB/s]Downloading model.safetensors:  43%|███████████████████████████████▏                                        | 105M/242M [00:08<00:09, 15.2MB/s]Downloading model.safetensors:  48%|██████████████████████████████████▎                                     | 115M/242M [00:08<00:08, 14.8MB/s]Downloading model.safetensors:  52%|█████████████████████████████████████▍                                  | 126M/242M [00:09<00:08, 14.4MB/s]Downloading model.safetensors:  56%|████████████████████████████████████████▌                               | 136M/242M [00:10<00:07, 14.3MB/s]Downloading model.safetensors:  61%|███████████████████████████████████████████▋                            | 147M/242M [00:11<00:06, 13.9MB/s]Downloading model.safetensors:  65%|██████████████████████████████████████████████▊                         | 157M/242M [00:11<00:05, 14.3MB/s]Downloading model.safetensors:  69%|█████████████████████████████████████████████████▉                      | 168M/242M [00:12<00:05, 14.5MB/s]Downloading model.safetensors:  74%|█████████████████████████████████████████████████████                   | 178M/242M [00:13<00:04, 14.6MB/s]Downloading model.safetensors:  78%|████████████████████████████████████████████████████████▏               | 189M/242M [00:14<00:03, 14.3MB/s]Downloading model.safetensors:  82%|███████████████████████████████████████████████████████████▎            | 199M/242M [00:14<00:02, 14.3MB/s]Downloading model.safetensors:  87%|██████████████████████████████████████████████████████████████▍         | 210M/242M [00:15<00:02, 13.0MB/s]Downloading model.safetensors:  91%|█████████████████████████████████████████████████████████████████▌      | 220M/242M [00:16<00:01, 12.6MB/s]Downloading model.safetensors:  95%|████████████████████████████████████████████████████████████████████▌   | 231M/242M [00:17<00:01, 11.0MB/s]Downloading model.safetensors: 100%|███████████████████████████████████████████████████████████████████████▋| 241M/242M [00:18<00:00, 11.9MB/s]Downloading model.safetensors: 100%|████████████████████████████████████████████████████████████████████████| 242M/242M [00:18<00:00, 13.0MB/s]
Traceback (most recent call last):
  File "./f00137_translate_research_summary.py", line 44, in <module>
    test_translate_research_summary()
  File "./f00137_translate_research_summary.py", line 37, in test_translate_research_summary
    assert isinstance(translate_research_summary(research_summary1), str)
  File "./f00137_translate_research_summary.py", line 24, in translate_research_summary
    decoded_text = model.generate(input_ids)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 1408, in generate
    self._validate_model_class()
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 1203, in _validate_model_class
    raise TypeError(exception_message)
TypeError: The current model class (T5Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'T5ForConditionalGeneration'}
