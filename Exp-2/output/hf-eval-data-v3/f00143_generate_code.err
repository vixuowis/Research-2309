Downloading (…)okenizer_config.json:   0%|                                                                           | 0.00/240 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████████████| 240/240 [00:00<00:00, 45.7kB/s]
Downloading (…)olve/main/vocab.json:   0%|                                                                          | 0.00/798k [00:00<?, ?B/s]Downloading (…)olve/main/vocab.json: 100%|███████████████████████████████████████████████████████████████████| 798k/798k [00:00<00:00, 812kB/s]Downloading (…)olve/main/vocab.json: 100%|███████████████████████████████████████████████████████████████████| 798k/798k [00:00<00:00, 811kB/s]
Downloading (…)olve/main/merges.txt:   0%|                                                                          | 0.00/456k [00:00<?, ?B/s]Downloading (…)olve/main/merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:04<00:00, 100kB/s]Downloading (…)olve/main/merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:04<00:00, 100kB/s]
Downloading (…)/main/tokenizer.json:   0%|                                                                         | 0.00/2.11M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████████| 2.11M/2.11M [00:02<00:00, 1.03MB/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████████| 2.11M/2.11M [00:02<00:00, 1.03MB/s]
Downloading (…)in/added_tokens.json:   0%|                                                                         | 0.00/1.00k [00:00<?, ?B/s]Downloading (…)in/added_tokens.json: 100%|█████████████████████████████████████████████████████████████████| 1.00k/1.00k [00:00<00:00, 526kB/s]
Downloading (…)cial_tokens_map.json:   0%|                                                                          | 0.00/90.0 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 49.3kB/s]
Downloading (…)lve/main/config.json:   0%|                                                                           | 0.00/998 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████████| 998/998 [00:00<00:00, 539kB/s]
Downloading pytorch_model.bin:   0%|                                                                               | 0.00/5.69G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|▏                                                                    | 10.5M/5.69G [00:19<2:53:44, 545kB/s]Downloading pytorch_model.bin:   0%|▏                                                                    | 10.5M/5.69G [00:32<2:53:44, 545kB/s]Downloading pytorch_model.bin:   0%|▎                                                                    | 21.0M/5.69G [00:52<4:08:46, 380kB/s]Downloading pytorch_model.bin:   0%|▎                                                                    | 21.0M/5.69G [01:02<4:08:46, 380kB/s]Downloading pytorch_model.bin:   1%|▎                                                                    | 29.6M/5.69G [01:24<4:49:47, 326kB/s]Traceback (most recent call last):
  File "./f00143_generate_code.py", line 40, in <module>
    test_generate_code()
  File "./f00143_generate_code.py", line 33, in test_generate_code
    assert isinstance(generate_code(description1), str)
  File "./f00143_generate_code.py", line 18, in generate_code
    model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2793, in from_pretrained
    resolved_archive_file = cached_file(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1429, in hf_hub_download
    http_get(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 557, in http_get
    raise EnvironmentError(
OSError: Consistency check failed: file should be of size 5693021687 but has size 29571696 (pytorch_model.bin).
We are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.
If the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.
Downloading pytorch_model.bin:   1%|▎                                                                    | 29.6M/5.69G [01:24<4:30:10, 349kB/s]