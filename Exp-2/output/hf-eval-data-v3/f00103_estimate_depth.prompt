{'code': "###Instruction: An interior design firm builds a software to understand the depth of rooms captured in photographs for remodeling activities.\n###Output: <<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: pipeline('cv-depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import necessary libraries, such as 'pipeline' from transformers.\n2. Use the 'pipeline' function from Hugging Face Transformers to load the depth estimation model, which in this case is 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'.\n3. The loaded model will be used for estimating the depth of objects within an image, allowing the interior design firm to measure distances and create accurate 3D representations of rooms.\n4. Once the model is loaded, simply pass an image of the room to the model, and it will provide depth estimations as output.\n<<<code>>>: from transformers import pipeline\ndepth_estimator = pipeline('cv-depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\nroom_image_path = 'path/to/room/image.jpg'\ndepth_map = depth_estimator(room_image_path)\n", 'api_call': "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.'}}


Traceback (most recent call last):
  File "output/hf-eval-data-v2/f00103_estimate_depth.py", line 49, in <module>
    test_estimate_depth()
  File "output/hf-eval-data-v2/f00103_estimate_depth.py", line 41, in test_estimate_depth
    depth_map = estimate_depth(test_image_path)
  File "output/hf-eval-data-v2/f00103_estimate_depth.py", line 21, in estimate_depth
    depth_estimator = pipeline('cv-depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221215-093747')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 780, in pipeline
    normalized_task, targeted_task, task_options = check_task(task)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 499, in check_task
    return PIPELINE_REGISTRY.check_task(task)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1215, in check_task
    raise KeyError(
KeyError: "Unknown task cv-depth-estimation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
