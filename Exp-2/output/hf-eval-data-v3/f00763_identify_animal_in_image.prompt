{'code': "###Instruction: I would like to create an application that identifies animals in Chinese language image captions. Specifically, we want to know if a picture includes a cat or a dog.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the required libraries, ChineseCLIPModel and ChineseCLIPProcessor from the transformers package, and Image and requests for handling the image input.\n2. Load the pre-trained ChineseCLIPModel using the specified model name 'OFA-Sys/chinese-clip-vit-base-patch16'. This model is designed to classify images and associate Chinese text descriptions.\n3. Load the ChineseCLIPProcessor with the same model name to pre-process the input images and captions.\n4. Obtain the image using the provided URL and open it using the Image.open() method.\n5. Define the Chinese texts for categories of interest, in this case, cat and dog.\n6. Process the image and text inputs using the ChineseCLIPProcessor, and calculate image and text features using the ChineseCLIPModel.\n7. Normalize the features and compute the probabilities of the image belonging to each category, using the softmax function.\n8. Determine the category with the highest probability.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')\nurl = 'https://example.com/image_url.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = ['猫', '狗']\ninputs = processor(images=image, text=texts, return_tensors='pt', padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nhighest_prob_idx = probs.argmax(dim=1)\nanimal = texts[highest_prob_idx]\n", 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'OFA-Sys/chinese-clip-vit-base-patch16', 'api_call': "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')", 'api_arguments': {'pretrained_model_name_or_path': 'OFA-Sys/chinese-clip-vit-base-patch16'}, 'python_environment_requirements': {'transformers': 'ChineseCLIPProcessor, ChineseCLIPModel'}, 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [, , , ]\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'MUGE Text-to-Image Retrieval': {'accuracy': {'Zero-shot R@1': 63.0, 'Zero-shot R@5': 84.1, 'Zero-shot R@10': 89.2, 'Finetune R@1': 68.9, 'Finetune R@5': 88.7, 'Finetune R@10': 93.1}}, 'Flickr30K-CN Retrieval': {'accuracy': {'Zero-shot Text-to-Image R@1': 71.2, 'Zero-shot Text-to-Image R@5': 91.4, 'Zero-shot Text-to-Image R@10': 95.5, 'Finetune Text-to-Image R@1': 83.8, 'Finetune Text-to-Image R@5': 96.9, 'Finetune Text-to-Image R@10': 98.6, 'Zero-shot Image-to-Text R@1': 81.6, 'Zero-shot Image-to-Text R@5': 97.5, 'Zero-shot Image-to-Text R@10': 98.8, 'Finetune Image-to-Text R@1': 95.3, 'Finetune Image-to-Text R@5': 99.7, 'Finetune Image-to-Text R@10': 100.0}}, 'COCO-CN Retrieval': {'accuracy': {'Zero-shot Text-to-Image R@1': 69.2, 'Zero-shot Text-to-Image R@5': 89.9, 'Zero-shot Text-to-Image R@10': 96.1, 'Finetune Text-to-Image R@1': 81.5, 'Finetune Text-to-Image R@5': 96.9, 'Finetune Text-to-Image R@10': 99.1, 'Zero-shot Image-to-Text R@1': 63.0, 'Zero-shot Image-to-Text R@5': 86.6, 'Zero-shot Image-to-Text R@10': 92.9, 'Finetune Image-to-Text R@1': 83.5, 'Finetune Image-to-Text R@5': 97.3, 'Finetune Image-to-Text R@10': 99.2}}, 'Zero-shot Image Classification': {'accuracy': {'CIFAR10': 96.0, 'CIFAR100': 79.7, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}}}}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.'}}

