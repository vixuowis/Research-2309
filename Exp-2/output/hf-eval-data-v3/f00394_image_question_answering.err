2023-11-11 23:39:22.863916: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-11 23:39:22.916109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-11 23:39:23.585689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Downloading (…)lve/main/config.json:   0%|                                                                          | 0.00/153k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████████████| 153k/153k [00:01<00:00, 136kB/s]Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████████████| 153k/153k [00:01<00:00, 135kB/s]
Downloading pytorch_model.bin:   0%|                                                                                | 0.00/455M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|█▋                                                                     | 10.5M/455M [00:00<00:07, 59.6MB/s]Downloading pytorch_model.bin:   5%|███▎                                                                   | 21.0M/455M [00:00<00:06, 69.0MB/s]Downloading pytorch_model.bin:   9%|██████▌                                                                | 41.9M/455M [00:00<00:04, 99.8MB/s]Downloading pytorch_model.bin:  14%|█████████▉                                                              | 62.9M/455M [00:00<00:03, 114MB/s]Downloading pytorch_model.bin:  18%|█████████████▎                                                          | 83.9M/455M [00:00<00:03, 120MB/s]Downloading pytorch_model.bin:  23%|████████████████▊                                                        | 105M/455M [00:00<00:02, 123MB/s]Downloading pytorch_model.bin:  28%|████████████████████▏                                                    | 126M/455M [00:01<00:02, 128MB/s]Downloading pytorch_model.bin:  32%|███████████████████████▏                                                | 147M/455M [00:02<00:06, 47.9MB/s]Downloading pytorch_model.bin:  37%|██████████████████████████▌                                             | 168M/455M [00:02<00:04, 59.8MB/s]Downloading pytorch_model.bin:  41%|█████████████████████████████▊                                          | 189M/455M [00:02<00:03, 72.2MB/s]Downloading pytorch_model.bin:  46%|█████████████████████████████████▏                                      | 210M/455M [00:02<00:02, 85.1MB/s]Downloading pytorch_model.bin:  51%|████████████████████████████████████▍                                   | 231M/455M [00:02<00:02, 91.8MB/s]Downloading pytorch_model.bin:  55%|███████████████████████████████████████▊                                | 252M/455M [00:02<00:02, 97.7MB/s]Downloading pytorch_model.bin:  60%|███████████████████████████████████████████▋                             | 273M/455M [00:03<00:01, 103MB/s]Downloading pytorch_model.bin:  64%|███████████████████████████████████████████████                          | 294M/455M [00:03<00:01, 111MB/s]Downloading pytorch_model.bin:  69%|██████████████████████████████████████████████████▍                      | 315M/455M [00:03<00:01, 118MB/s]Downloading pytorch_model.bin:  74%|█████████████████████████████████████████████████████▊                   | 336M/455M [00:03<00:00, 121MB/s]Downloading pytorch_model.bin:  78%|█████████████████████████████████████████████████████████▏               | 357M/455M [00:03<00:00, 124MB/s]Downloading pytorch_model.bin:  83%|████████████████████████████████████████████████████████████▍            | 377M/455M [00:03<00:00, 128MB/s]Downloading pytorch_model.bin:  87%|███████████████████████████████████████████████████████████████▊         | 398M/455M [00:04<00:00, 127MB/s]Downloading pytorch_model.bin:  92%|███████████████████████████████████████████████████████████████████▏     | 419M/455M [00:04<00:00, 126MB/s]Downloading pytorch_model.bin:  97%|██████████████████████████████████████████████████████████████████████▌  | 440M/455M [00:04<00:00, 128MB/s]Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████| 455M/455M [00:04<00:00, 101MB/s]
Traceback (most recent call last):
  File "./f00394_image_question_answering.py", line 47, in <module>
    test_image_question_answering()
  File "./f00394_image_question_answering.py", line 34, in test_image_question_answering
    result = image_question_answering(image_path, question)
  File "./f00394_image_question_answering.py", line 21, in image_question_answering
    model = pipeline('question-answering', model='uclanlp/visualbert-vqa')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 921, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 754, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1838, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'uclanlp/visualbert-vqa'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'uclanlp/visualbert-vqa' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.
