Downloading (…)okenizer_config.json:   0%|                                                                           | 0.00/104 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████████████| 104/104 [00:00<00:00, 12.7kB/s]
Downloading (…)lve/main/config.json:   0%|                                                                           | 0.00/479 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████████████| 479/479 [00:00<00:00, 42.1kB/s]
Downloading (…)solve/main/vocab.txt:   0%|                                                                          | 0.00/258k [00:00<?, ?B/s]Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████████████████████████████| 258k/258k [00:00<00:00, 414kB/s]Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████████████████████████████| 258k/258k [00:00<00:00, 413kB/s]
Traceback (most recent call last):
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 458, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./f00068_fill_mask.py", line 39, in <module>
    test_fill_mask()
  File "./f00068_fill_mask.py", line 32, in test_fill_mask
    assert fill_mask('テキストに[MASK]語があります。') != 'テキストに[MASK]語があります。'
  File "./f00068_fill_mask.py", line 17, in fill_mask
    tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 736, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1854, in from_pretrained
    return cls._from_pretrained(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2017, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 211, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 460, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.
