{"path": "output/hf-eval-data-v3-valid/f00217_determine_logical_relationship.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# function_code --------------------\n\ndef determine_logical_relationship(text1: str, text2: str) -> dict:\n    \"\"\"\n    Determine the logical relationship between two given sentences.\n\n    Args:\n        text1 (str): The first sentence.\n        text2 (str): The second sentence.\n\n    Returns:\n        dict: A dictionary containing the probabilities of each logical relationship (entailment, contradiction, or neutral).\n    \"\"\"\n    model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    with torch.inference_mode():\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n\n    result = {v: proba[k] for k, v in model.config.id2label.items()}\n    return result\n\n# test_function_code --------------------\n\ndef test_determine_logical_relationship():\n    \"\"\"\n    Test the function determine_logical_relationship.\n    \"\"\"\n    text1 = 'The cat is on the mat.'\n    text2 = 'There is a cat on the mat.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'It is raining.'\n    text2 = 'The weather is sunny.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'He is a boy.'\n    text2 = 'She is a girl.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_determine_logical_relationship())", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n", "function_code": "# function_code --------------------\n\ndef determine_logical_relationship(text1: str, text2: str) -> dict:\n    \"\"\"\n    Determine the logical relationship between two given sentences.\n\n    Args:\n        text1 (str): The first sentence.\n        text2 (str): The second sentence.\n\n    Returns:\n        dict: A dictionary containing the probabilities of each logical relationship (entailment, contradiction, or neutral).\n    \"\"\"\n    model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    with torch.inference_mode():\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n\n    result = {v: proba[k] for k, v in model.config.id2label.items()}\n    return result\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_determine_logical_relationship():\n    \"\"\"\n    Test the function determine_logical_relationship.\n    \"\"\"\n    text1 = 'The cat is on the mat.'\n    text2 = 'There is a cat on the mat.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'It is raining.'\n    text2 = 'The weather is sunny.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    text1 = 'He is a boy.'\n    text2 = 'She is a girl.'\n    result = determine_logical_relationship(text1, text2)\n    assert isinstance(result, dict)\n    assert set(result.keys()) == {'entailment', 'contradiction', 'neutral'}\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_determine_logical_relationship())", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# function_code --------------------\n\ndef determine_logical_relationship(text1: str, text2: str) -> dict:\n    \"\"\"\n    Determine the logical relationship between two given sentences.\n\n    Args:\n        text1 (str): The first sentence.\n        text2 (str): The second sentence.\n\n    Returns:\n        dict: A dictionary containing the probabilities of each logical relationship (entailment, contradiction, or neutral).\n    \"\"\"", "answer": "\n    model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    with torch.inference_mode():\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n\n    result = {v: proba[k] for k, v in model.config.id2label.items()}\n    return result\n\n", "prediction": "\n    \n    # Load model\n    tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-portuguese-cased-sentiment\")\n    model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-portuguese-cased-sentiment\")\n    \n    # Tokenize inputs\n    tokens1 = tokenizer(text1, return_tensors=\"pt\", truncation=True, padding='max_length') \n    tokens2 = tokenizer(text2, return_tensors=\"pt\", truncation=True, padding='max_length') \n    \n    # Perform inference\n    outputs = model(torch.cat((tokens1[\"input_ids\"], tokens2[\"input_ids\"]), dim=-1))[0]\n    predictions = torch.argmax(outputs, axis=1)\n    \n    result = {\n        'entailment': float(predictions[0]),\n        'neutral': float(predictions[1]),\n        'contradiction': float(predictions[2])\n    }\n    \n    return result"}