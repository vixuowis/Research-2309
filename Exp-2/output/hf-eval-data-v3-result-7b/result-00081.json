{"path": "output/hf-eval-data-v3-valid/f00349_generate_image_caption.py", "content": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n# test_function_code --------------------\n\ndef test_generate_image_caption():\n    \"\"\"\n    Test the function generate_image_caption.\n    \"\"\"\n    assert isinstance(generate_image_caption('test_image.jpg', 'product photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'landscape photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'portrait photography'), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\nprint(test_generate_image_caption())", "function_import": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n", "function_code": "# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_image_caption():\n    \"\"\"\n    Test the function generate_image_caption.\n    \"\"\"\n    assert isinstance(generate_image_caption('test_image.jpg', 'product photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'landscape photography'), str)\n    assert isinstance(generate_image_caption('test_image.jpg', 'portrait photography'), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\nprint(test_generate_image_caption())", "instruct": "# function_import --------------------\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# function_code --------------------\n\ndef generate_image_caption(image_path: str, text: str = 'product photography') -> str:\n    \"\"\"\n    Generate descriptive captions for photographs related to the products using Hugging Face Transformers.\n\n    Args:\n        image_path (str): The path to the image file.\n        text (str, optional): A short text that provides some context to the photograph. Defaults to 'product photography'.\n\n    Returns:\n        str: The generated caption for the input image.\n    \"\"\"", "answer": "\n    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n    image = Image.open(image_path)\n    inputs = processor(image, text, return_tensors='pt')\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n", "prediction": "\n    # Create processor and model\n    processor = BlipProcessor.from_pretrained('facebook/blip-large-coco-airbnb')\n    model = BlipForConditionalGeneration.from_pretrained('facebook/blip-large-coco-airbnb').to(\"cuda\") # Change to \"cpu\" if you do not have a GPU\n    \n    # Prepare image for processing\n    pil_img = Image.open(image_path).convert(\"RGB\").resize((384, 384), resample=0)\n    img = processor(images=pil_img, return_tensors=\"pt\")['pixel_values'].to(\"cuda\")\n    \n    # Prepare text for processing\n    input_ids = processor.tokenizer([text], truncation='longest_first', max_length=128, padding='max_length').to(\"cuda\")[\"input_ids\"]\n        \n    # Generate caption with model\n    output = model.generate(\n        input_ids, \n        attention_mask=(input_ids > 0),\n        max_length=512, \n        num_beams=4, \n        early_stopping=True)\n    \n    return processor.tokenizer.decode(output[0], skip_special_tokens=True)"}