{"path": "output/hf-eval-data-v3-valid/f00142_generate_conversation.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n# test_function_code --------------------\n\ndef test_generate_conversation():\n    \"\"\"\n    Test the generate_conversation function.\n    \"\"\"\n    situation = \"Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\"\n    instruction = \"You are Cosmo and you are talking to a friend.\"\n    conversation = [\"Hey, how was your trip to Abu Dhabi?\"]\n    response = generate_conversation(situation, instruction, conversation)\n    assert isinstance(response, str)\n    assert len(response) > 0\n    print('All Tests Passed')\n\n# call_test_function_code --------------------\n\ntest_generate_conversation()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n", "function_code": "# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_conversation():\n    \"\"\"\n    Test the generate_conversation function.\n    \"\"\"\n    situation = \"Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\"\n    instruction = \"You are Cosmo and you are talking to a friend.\"\n    conversation = [\"Hey, how was your trip to Abu Dhabi?\"]\n    response = generate_conversation(situation, instruction, conversation)\n    assert isinstance(response, str)\n    assert len(response) > 0\n    print('All Tests Passed')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_conversation()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# function_code --------------------\n\ndef generate_conversation(situation_narrative, role_instruction, conversation_history):\n    \"\"\"\n    Generate a conversation based on a situation narrative, role instruction, and conversation history.\n\n    Args:\n        situation_narrative (str): The situation narrative.\n        role_instruction (str): The role instruction.\n        conversation_history (list): The conversation history.\n\n    Returns:\n        str: The generated conversation.\n    \"\"\"", "answer": "\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = AutoTokenizer.from_pretrained('allenai/cosmo-xl')\n    model = AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl').to(device)\n\n    def set_input(situation_narrative, role_instruction, conversation_history):\n        input_text = \" <turn> \".join(conversation_history)\n        if role_instruction != \"\":\n            input_text = \"{} <sep> {}\".format(role_instruction, input_text)\n        if situation_narrative != \"\":\n            input_text = \"{} <sep> {}\".format(situation_narrative, input_text)\n        return input_text\n\n    input_text = set_input(situation_narrative, role_instruction, conversation_history)\n    inputs = tokenizer([input_text], return_tensors='pt').to(device)\n    outputs = model.generate(inputs['input_ids'], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return response\n\n", "prediction": "\n    \n    # load the model and tokenizer\n    print('Loading the model...')\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/DialoGPT-large\").to(\"cuda\")\n    \n    # generate a conversation history with the situation narrative and role instruction\n    conversation_history += ['Human: ' + situation_narrative, \n                              'Agent: ' + role_instruction]\n\n    # tokenize and create the input sequence\n    conversation_history = [f'{i}: {conversation}' for i, conversation in enumerate(conversation_history)]\n    \n    inputs = tokenizer('\\n'.join(conversation_history + ['>>>']), \n                       return_tensors='pt').to(\"cuda\")\n    \n    # generate the output sequence\n    reply = \"\"\n    while len(reply.split()) <= 3:\n        outputs = model.generate(**inputs, max_length=30)\n        \n        # decode the output sequence and add to conversation history\n        reply = tokenizer.batch_decode(outputs)[0]\n        conversation_history += ['Human: ' + reply]\n        \n        # generate input sequence for next dialog\n        inputs = tokenizer('\\n'.join(conversation_history), \n                           return_tensors='pt').to(\"cuda\")\n    \n    return conversation_history, reply"}