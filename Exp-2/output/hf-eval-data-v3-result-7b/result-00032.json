{"path": "output/hf-eval-data-v3-valid/f00141_generate_story.py", "content": "# function_import --------------------\n\nfrom transformers import pipeline\n\n# function_code --------------------\n\ndef generate_story(prompt: str) -> str:\n    \"\"\"\n    Generate a short story based on a given prompt using the LLaMA-7B language model.\n\n    Args:\n        prompt (str): The initial prompt to base the story on.\n\n    Returns:\n        str: The generated story.\n\n    Raises:\n        OSError: If the specified model is not found.\n    \"\"\"\n    try:\n        story_generator = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n        story = story_generator(prompt)\n        return story[0]['generated_text']\n    except OSError:\n        raise OSError('Model not found. Please make sure the model name is correct.')\n\n# test_function_code --------------------\n\ndef test_generate_story():\n    \"\"\"\n    Test the generate_story function.\n    \"\"\"\n    try:\n        # Test with a simple prompt\n        prompt = 'Once upon a time in a small village...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with a different prompt\n        prompt = 'In a galaxy far, far away...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with an empty prompt\n        prompt = ''\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        print('All Tests Passed')\n    except OSError:\n        print('Model not found. Skipping tests.')\n\n# call_test_function_code --------------------\n\ntest_generate_story()", "function_import": "# function_import --------------------\n\nfrom transformers import pipeline\n\n", "function_code": "# function_code --------------------\n\ndef generate_story(prompt: str) -> str:\n    \"\"\"\n    Generate a short story based on a given prompt using the LLaMA-7B language model.\n\n    Args:\n        prompt (str): The initial prompt to base the story on.\n\n    Returns:\n        str: The generated story.\n\n    Raises:\n        OSError: If the specified model is not found.\n    \"\"\"\n    try:\n        story_generator = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n        story = story_generator(prompt)\n        return story[0]['generated_text']\n    except OSError:\n        raise OSError('Model not found. Please make sure the model name is correct.')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_generate_story():\n    \"\"\"\n    Test the generate_story function.\n    \"\"\"\n    try:\n        # Test with a simple prompt\n        prompt = 'Once upon a time in a small village...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with a different prompt\n        prompt = 'In a galaxy far, far away...'\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        # Test with an empty prompt\n        prompt = ''\n        story = generate_story(prompt)\n        assert isinstance(story, str)\n\n        print('All Tests Passed')\n    except OSError:\n        print('Model not found. Skipping tests.')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_generate_story()", "instruct": "# function_import --------------------\n\nfrom transformers import pipeline\n\n# function_code --------------------\n\ndef generate_story(prompt: str) -> str:\n    \"\"\"\n    Generate a short story based on a given prompt using the LLaMA-7B language model.\n\n    Args:\n        prompt (str): The initial prompt to base the story on.\n\n    Returns:\n        str: The generated story.\n\n    Raises:\n        OSError: If the specified model is not found.\n    \"\"\"", "answer": "\n    try:\n        story_generator = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n        story = story_generator(prompt)\n        return story[0]['generated_text']\n    except OSError:\n        raise OSError('Model not found. Please make sure the model name is correct.')\n\n", "prediction": "\n\n    # Initialize the pipeline and generate a story based on the given prompt.\n    with open(\"models/LLaMA-7B/metadata.json\", \"r\") as f: metadata = json.load(f)\n    if metadata[\"model_name\"] != \"llama-7b\": raise OSError(\"Model not found.\")\n    \n    summarization_pipeline = pipeline(\"summarization\", model=\"LaLM-7B\", tokenizer=\"LaLM-7B\")\n    story: str = summarization_pipeline(prompt, min_length=100)[0]['summary_text'][len(prompt)+1:]\n    \n    # Return the generated story.\n    return story"}