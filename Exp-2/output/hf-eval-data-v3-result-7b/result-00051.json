{"path": "output/hf-eval-data-v3-valid/f00205_image_geolocalization.py", "content": "# function_import --------------------\n\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\n# function_code --------------------\n\ndef image_geolocalization(url: str, choices: list):\n    \"\"\"\n    This function uses a pretrained CLIP model to identify the location of a given image.\n\n    Args:\n        url (str): The URL of the image to be geolocalized.\n        choices (list): A list of possible choices for the location of the image.\n\n    Returns:\n        dict: A dictionary with the location choices as keys and their corresponding probabilities as values.\n    \"\"\"\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    return {choice: prob for choice, prob in zip(choices, probs.tolist()[0])}\n\n# test_function_code --------------------\n\ndef test_image_geolocalization():\n    url = 'https://placekitten.com/200/300'\n    choices = ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco']\n    result = image_geolocalization(url, choices)\n    assert isinstance(result, dict)\n    assert len(result) == len(choices)\n    assert all(isinstance(choice, str) for choice in result.keys())\n    assert all(isinstance(prob, float) for prob in result.values())\n    assert abs(sum(result.values()) - 1) < 1e-6\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_image_geolocalization()", "function_import": "# function_import --------------------\n\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\n", "function_code": "# function_code --------------------\n\ndef image_geolocalization(url: str, choices: list):\n    \"\"\"\n    This function uses a pretrained CLIP model to identify the location of a given image.\n\n    Args:\n        url (str): The URL of the image to be geolocalized.\n        choices (list): A list of possible choices for the location of the image.\n\n    Returns:\n        dict: A dictionary with the location choices as keys and their corresponding probabilities as values.\n    \"\"\"\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    return {choice: prob for choice, prob in zip(choices, probs.tolist()[0])}\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_image_geolocalization():\n    url = 'https://placekitten.com/200/300'\n    choices = ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco']\n    result = image_geolocalization(url, choices)\n    assert isinstance(result, dict)\n    assert len(result) == len(choices)\n    assert all(isinstance(choice, str) for choice in result.keys())\n    assert all(isinstance(prob, float) for prob in result.values())\n    assert abs(sum(result.values()) - 1) < 1e-6\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_image_geolocalization()", "instruct": "# function_import --------------------\n\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\n\n# function_code --------------------\n\ndef image_geolocalization(url: str, choices: list):\n    \"\"\"\n    This function uses a pretrained CLIP model to identify the location of a given image.\n\n    Args:\n        url (str): The URL of the image to be geolocalized.\n        choices (list): A list of possible choices for the location of the image.\n\n    Returns:\n        dict: A dictionary with the location choices as keys and their corresponding probabilities as values.\n    \"\"\"", "answer": "\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    return {choice: prob for choice, prob in zip(choices, probs.tolist()[0])}\n\n", "prediction": "\n    \n    # Get the image\n    response = requests.get(url)\n    img = Image.open(response.raw).convert(\"RGB\")\n\n    # Initialize pretrained CLIP model\n    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", feature_extractor_type='image')\n    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n    # Process image\n    inputs = processor(text=['a photo of'], images=img, return_tensors=\"pt\", padding=True)\n    pixels = inputs[\"pixel_values\"].unsqueeze(0).to(\"cuda\")\n    \n    # Get location choices embeddings\n    with torch.no_grad():\n        location_embeddings = model.get_image_features(pixels, return_tensors=\"pt\").to(\"cuda\")[:, 0]\n        \n    # Compute distances between each image and each choice (using cosine similarity)\n    distances = torch.cosine_similarity(location_embeddings.unsqueeze(1), model.get_text_features(choices, return_tensors=\"pt\").to(\"cuda\"), dim=2).mean(dim=1)\n    \n    # Normalize distances and build dictionary of possible locations with their corresponding probabilities\n    probs = softmax(distances/distances.sum())\n    locs = {choices[i]: round(probs[i].item(), 4) for i in range(len(probs))}\n    \n    return locs\n"}