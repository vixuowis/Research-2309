{"path": "output/hf-eval-data-v3-valid/f00364_detect_kitchen_objects.py", "content": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n# test_function_code --------------------\n\ndef test_detect_kitchen_objects():\n    \"\"\"\n    Tests the detect_kitchen_objects function.\n    \"\"\"\n    try:\n        detect_kitchen_objects('test_image.jpg', 0.1)\n        print('Test passed')\n    except Exception as e:\n        print(f'Test failed: {str(e)}')\n\n# call_test_function_code --------------------\n\ntest_detect_kitchen_objects()", "function_import": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n", "function_code": "# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_detect_kitchen_objects():\n    \"\"\"\n    Tests the detect_kitchen_objects function.\n    \"\"\"\n    try:\n        detect_kitchen_objects('test_image.jpg', 0.1)\n        print('Test passed')\n    except Exception as e:\n        print(f'Test failed: {str(e)}')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_detect_kitchen_objects()", "instruct": "# function_import --------------------\n\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_kitchen_objects(image_path: str, score_threshold: float):\n    \"\"\"\n    Detects kitchen objects in an image using the OwlViT object detection model.\n\n    Args:\n        image_path (str): The path to the image file.\n        score_threshold (float): The confidence score threshold for object detection.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and their locations.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        RuntimeError: If there is a problem loading the model or processing the image.\n    \"\"\"", "answer": "\n    processor = OwlViTProcessor.from_pretrained('google/owlvit-large-patch14')\n    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\n    image = Image.open(image_path)\n    texts = [[\"a photo of a fruit\", \"a photo of a dish\"]]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    for i in range(len(texts)):\n        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[0][label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "prediction": "\n\n    # Load the OwlViT processor used for inference with pretrained weights\n    try:\n        processor = OwlViTProcessor.from_pretrained(\"deepmotion/owl-vit\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load the model due to {e}.\")\n    \n    # Load the OwlViT object detection model with pretrained weights\n    try:\n        model = OwlViTForObjectDetection.from_pretrained(\"deepmotion/owl-vit\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load the model due to {e}.\")\n        \n    # Load the image file for inference\n    try: \n        img = Image.open(image_path)\n    except FileNotFoundError as e:\n        raise FileNotFoundError(\"The specified image file does not exist.\")\n    \n    # Process the input image to tensor and apply normalization\n    try:\n        processed_img = processor(images=img, return_tensors=\"pt\") \n        input_tensor = processed_img.pixel_values.unsqueeze(0)\n        normalized_tensor = (input_tensor - torch.mean(input_tensor)) / torch.std(input_tensor).item()\n    except Exception as e:\n        raise RuntimeError(f\"Failed to process the image for inference due to {e}.\")\n        \n    # Predict using the OwlViT object detection model\n    try: \n        model.eval()\n        with torch.no_grad():\n            outputs = model(normalized_tensor)\n            \n            logits = outputs[0][0]\n            preds = processor.post_process_logits(logits, processed_img[\"pixel_mask\"].unsqueeze(0))\n        \n        # Filter predictions with a score higher than the confidence threshold\n        filtered_preds = [pred for pred in preds if pred['score'] >= score_threshold]\n    except Exception as e:\n        raise RuntimeError(f\"Failed to perform inference due to {e}.\")\n        "}