{"path": "output/hf-eval-data-v3-valid/f00546_extract_features.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# function_code --------------------\n\ndef extract_features(entity_names):\n    \"\"\"\n    Extract features from a set of entity names using the SapBERT model.\n\n    Args:\n        entity_names (str): A string of biomedical entity names.\n\n    Returns:\n        torch.Tensor: The [CLS] embedding from the model output, which represents the aggregated features for the input biomedical entity names.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n\n    inputs = tokenizer(entity_names, return_tensors='pt')\n    outputs = model(**inputs)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n\n    return cls_embedding\n\n# test_function_code --------------------\n\ndef test_extract_features():\n    \"\"\"\n    Test the extract_features function.\n    \"\"\"\n    entity_names = 'covid infection'\n    cls_embedding = extract_features(entity_names)\n\n    assert cls_embedding.shape[0] == 1\n    assert cls_embedding.shape[1] == 768\n\n    entity_names = 'cancer cell'\n    cls_embedding = extract_features(entity_names)\n\n    assert cls_embedding.shape[0] == 1\n    assert cls_embedding.shape[1] == 768\n\n    entity_names = 'heart disease'\n    cls_embedding = extract_features(entity_names)\n\n    assert cls_embedding.shape[0] == 1\n    assert cls_embedding.shape[1] == 768\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_extract_features()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n", "function_code": "# function_code --------------------\n\ndef extract_features(entity_names):\n    \"\"\"\n    Extract features from a set of entity names using the SapBERT model.\n\n    Args:\n        entity_names (str): A string of biomedical entity names.\n\n    Returns:\n        torch.Tensor: The [CLS] embedding from the model output, which represents the aggregated features for the input biomedical entity names.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n\n    inputs = tokenizer(entity_names, return_tensors='pt')\n    outputs = model(**inputs)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n\n    return cls_embedding\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_features():\n    \"\"\"\n    Test the extract_features function.\n    \"\"\"\n    entity_names = 'covid infection'\n    cls_embedding = extract_features(entity_names)\n\n    assert cls_embedding.shape[0] == 1\n    assert cls_embedding.shape[1] == 768\n\n    entity_names = 'cancer cell'\n    cls_embedding = extract_features(entity_names)\n\n    assert cls_embedding.shape[0] == 1\n    assert cls_embedding.shape[1] == 768\n\n    entity_names = 'heart disease'\n    cls_embedding = extract_features(entity_names)\n\n    assert cls_embedding.shape[0] == 1\n    assert cls_embedding.shape[1] == 768\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_features()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# function_code --------------------\n\ndef extract_features(entity_names):\n    \"\"\"\n    Extract features from a set of entity names using the SapBERT model.\n\n    Args:\n        entity_names (str): A string of biomedical entity names.\n\n    Returns:\n        torch.Tensor: The [CLS] embedding from the model output, which represents the aggregated features for the input biomedical entity names.\n    \"\"\"", "answer": "\n    tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n\n    inputs = tokenizer(entity_names, return_tensors='pt')\n    outputs = model(**inputs)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n\n    return cls_embedding\n\n", "prediction": "\n    \n    # Tokenize the entity name list using SapBERT's tokenizer. This will also include a special \"<eos>\" token as the last element of each list.\n    tokens = [model_tokenizer(name, add_special_tokens=True, truncation='do_not_truncate', max_length=maxlen) for name in entity_names]\n    \n    # Pad all tokens to be the same length using SapBERT's tokenizer. Note that this will also add a special \"<pad>\" token as padding.\n    input_ids = model_tokenizer.pad(tokens, return_tensors='pt')['input_ids']\n    \n    # Use the pre-trained SapBERT to extract features from the entity names.\n    with torch.no_grad():\n        outputs = model(**input_ids)\n    \n    # Return the [CLS] feature vector (i.e., the last layer activation of the transformer's embedding layer).\n    return outputs[0][:, 0, :].clone().detach()\n\n# function_call --------------------\n\nmodel_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n\n# The maximum length of a biomedical entity name for SapBERT is 250 tokens as defined by the paper.\nmaxlen = 250\n"}