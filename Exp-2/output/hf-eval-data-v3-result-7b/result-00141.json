{"path": "output/hf-eval-data-v3-valid/f00578_retrieve_relevant_documents.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef retrieve_relevant_documents(query: str, documents: list) -> list:\n    \"\"\"\n    Retrieve relevant documents based on a user's query using Hugging Face Transformers.\n\n    Args:\n        query (str): The user's query.\n        documents (list): A list of documents to retrieve information from.\n\n    Returns:\n        list: A list of documents sorted based on their relevance to the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n\n    features = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors='pt')\n\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n    return sorted_docs\n\n# test_function_code --------------------\n\ndef test_retrieve_relevant_documents():\n    query = 'How many people live in Berlin?'\n    documents = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    expected_output = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the capital of Germany?'\n    documents = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    expected_output = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the population of New York City?'\n    documents = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    expected_output = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_retrieve_relevant_documents()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef retrieve_relevant_documents(query: str, documents: list) -> list:\n    \"\"\"\n    Retrieve relevant documents based on a user's query using Hugging Face Transformers.\n\n    Args:\n        query (str): The user's query.\n        documents (list): A list of documents to retrieve information from.\n\n    Returns:\n        list: A list of documents sorted based on their relevance to the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n\n    features = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors='pt')\n\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n    return sorted_docs\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_retrieve_relevant_documents():\n    query = 'How many people live in Berlin?'\n    documents = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    expected_output = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the capital of Germany?'\n    documents = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    expected_output = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    query = 'What is the population of New York City?'\n    documents = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    expected_output = ['New York City has a population of 8,398,748 people.', 'Los Angeles has a population of 3,979,576 people.']\n    assert retrieve_relevant_documents(query, documents) == expected_output\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_retrieve_relevant_documents()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef retrieve_relevant_documents(query: str, documents: list) -> list:\n    \"\"\"\n    Retrieve relevant documents based on a user's query using Hugging Face Transformers.\n\n    Args:\n        query (str): The user's query.\n        documents (list): A list of documents to retrieve information from.\n\n    Returns:\n        list: A list of documents sorted based on their relevance to the query.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n\n    features = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors='pt')\n\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n    return sorted_docs\n\n", "prediction": "    \n    # Load model and tokenizer.\n    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)\n    \n    # Prepare data for inference.\n    documents = [document[\"content\"] for document in documents]\n    batch_inputs = tokenizer(\n        documents, \n        padding=\"max_length\", \n        truncation=True, \n        max_length=512, \n        return_tensors=\"pt\"\n    )\n    \n    # Make prediction.\n    with torch.no_grad():\n        outputs = model(**batch_inputs)\n        predictions = outputs.logits.argmax(dim=-1).tolist()\n        \n    # Get indices of highest scores in descending order to get the relevant documents based on the query.\n    sorted_predictions = list(sorted(enumerate(predictions), key=lambda x:x[1], reverse=True))\n    relevant_documents = [documents[i] for i, _ in sorted_predictions[:len(documents)]]\n    \n    # Return the relevant documents.\n    return relevant_documents"}