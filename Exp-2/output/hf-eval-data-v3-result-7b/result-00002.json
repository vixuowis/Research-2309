{"path": "output/hf-eval-data-v3-valid/f00004_extract_sentence_embeddings.py", "content": "# function_import --------------------\n\nfrom transformers import AutoModel, AutoTokenizer\n\n# function_code --------------------\n\ndef extract_sentence_embeddings(input_text: str):\n    '''\n    This function takes a sentence as input and returns its embedding using the LaBSE model.\n    \n    Args:\n    input_text (str): The sentence to be encoded.\n    \n    Returns:\n    Tensor: The sentence embedding.\n    '''\n    model = AutoModel.from_pretrained('rasa/LaBSE')\n    tokenizer = AutoTokenizer.from_pretrained('rasa/LaBSE')\n    encoded_input = tokenizer(input_text, return_tensors='pt')\n    embeddings = model(**encoded_input)\n    sentence_embedding = embeddings.pooler_output\n    return sentence_embedding\n\n# test_function_code --------------------\n\ndef test_extract_sentence_embeddings():\n    '''\n    This function tests the extract_sentence_embeddings function.\n    '''\n    sentence1 = 'Here is a sentence in English.'\n    sentence2 = 'Voici une phrase en fran\u00e7ais.'\n    sentence3 = 'Aqu\u00ed hay una frase en espa\u00f1ol.'\n    \n    embedding1 = extract_sentence_embeddings(sentence1)\n    embedding2 = extract_sentence_embeddings(sentence2)\n    embedding3 = extract_sentence_embeddings(sentence3)\n    \n    assert embedding1.shape == (1, 768)\n    assert embedding2.shape == (1, 768)\n    assert embedding3.shape == (1, 768)\n    \n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_extract_sentence_embeddings()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoModel, AutoTokenizer\n\n", "function_code": "# function_code --------------------\n\ndef extract_sentence_embeddings(input_text: str):\n    '''\n    This function takes a sentence as input and returns its embedding using the LaBSE model.\n    \n    Args:\n    input_text (str): The sentence to be encoded.\n    \n    Returns:\n    Tensor: The sentence embedding.\n    '''\n    model = AutoModel.from_pretrained('rasa/LaBSE')\n    tokenizer = AutoTokenizer.from_pretrained('rasa/LaBSE')\n    encoded_input = tokenizer(input_text, return_tensors='pt')\n    embeddings = model(**encoded_input)\n    sentence_embedding = embeddings.pooler_output\n    return sentence_embedding\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_sentence_embeddings():\n    '''\n    This function tests the extract_sentence_embeddings function.\n    '''\n    sentence1 = 'Here is a sentence in English.'\n    sentence2 = 'Voici une phrase en fran\u00e7ais.'\n    sentence3 = 'Aqu\u00ed hay una frase en espa\u00f1ol.'\n    \n    embedding1 = extract_sentence_embeddings(sentence1)\n    embedding2 = extract_sentence_embeddings(sentence2)\n    embedding3 = extract_sentence_embeddings(sentence3)\n    \n    assert embedding1.shape == (1, 768)\n    assert embedding2.shape == (1, 768)\n    assert embedding3.shape == (1, 768)\n    \n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_sentence_embeddings()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoModel, AutoTokenizer\n\n# function_code --------------------\n\ndef extract_sentence_embeddings(input_text: str):\n    '''\n    This function takes a sentence as input and returns its embedding using the LaBSE model.\n    \n    Args:\n    input_text (str): The sentence to be encoded.\n    \n    Returns:\n    Tensor: The sentence embedding.\n    '''", "answer": "\n    model = AutoModel.from_pretrained('rasa/LaBSE')\n    tokenizer = AutoTokenizer.from_pretrained('rasa/LaBSE')\n    encoded_input = tokenizer(input_text, return_tensors='pt')\n    embeddings = model(**encoded_input)\n    sentence_embedding = embeddings.pooler_output\n    return sentence_embedding\n\n", "prediction": "\n    \n    # Load model & tokenizer\n    tokenizer = AutoTokenizer.from_pretrained('KoichiYasuoka/bert-base-japanese-LaBSE')\n    model = AutoModel.from_pretrained('KoichiYasuoka/bert-base-japanese-LaBSE')\n    \n    input_ids = tokenizer(input_text, return_tensors='pt').input_ids  # Batch size 1\n    outputs = model(input_ids).last_hidden_state  # [batch_size, sequence_length, hidden_size]\n    embeddings = outputs.mean(axis=1)  # average pooling\n    \n    return embeddings[0].detach().numpy()"}