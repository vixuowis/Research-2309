{"path": "output/hf-eval-data-v3-valid/f00365_segment_clothes.py", "content": "# function_import --------------------\n\nfrom transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\n# function_code --------------------\n\ndef segment_clothes(image_path):\n    \"\"\"\n    This function segments clothes in an image using a pre-trained SegFormer model.\n\n    Args:\n        image_path (str): The path to the image file or a URL.\n\n    Returns:\n        A matplotlib figure showing the segmented image.\n    \"\"\"\n    extractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\n    image = Image.open(requests.get(image_path, stream=True).raw)\n    model = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\n    inputs = extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n\n    logits = outputs.logits.cpu()\n    upsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\n    pred_seg = upsampled_logits.argmax(dim=1)[0]\n    plt.imshow(pred_seg)\n    return plt\n\n# test_function_code --------------------\n\ndef test_segment_clothes():\n    \"\"\"\n    This function tests the segment_clothes function with a few test cases.\n    \"\"\"\n    # Test case 1: An image of a person wearing clothes\n    url1 = 'https://placekitten.com/200/300'\n    result1 = segment_clothes(url1)\n    assert isinstance(result1, type(plt)), 'Test Case 1 Failed'\n\n    # Test case 2: Another image of a person wearing clothes\n    url2 = 'https://placekitten.com/400/600'\n    result2 = segment_clothes(url2)\n    assert isinstance(result2, type(plt)), 'Test Case 2 Failed'\n\n    # Test case 3: Yet another image of a person wearing clothes\n    url3 = 'https://placekitten.com/800/1200'\n    result3 = segment_clothes(url3)\n    assert isinstance(result3, type(plt)), 'Test Case 3 Failed'\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_segment_clothes()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\n", "function_code": "# function_code --------------------\n\ndef segment_clothes(image_path):\n    \"\"\"\n    This function segments clothes in an image using a pre-trained SegFormer model.\n\n    Args:\n        image_path (str): The path to the image file or a URL.\n\n    Returns:\n        A matplotlib figure showing the segmented image.\n    \"\"\"\n    extractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\n    image = Image.open(requests.get(image_path, stream=True).raw)\n    model = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\n    inputs = extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n\n    logits = outputs.logits.cpu()\n    upsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\n    pred_seg = upsampled_logits.argmax(dim=1)[0]\n    plt.imshow(pred_seg)\n    return plt\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_segment_clothes():\n    \"\"\"\n    This function tests the segment_clothes function with a few test cases.\n    \"\"\"\n    # Test case 1: An image of a person wearing clothes\n    url1 = 'https://placekitten.com/200/300'\n    result1 = segment_clothes(url1)\n    assert isinstance(result1, type(plt)), 'Test Case 1 Failed'\n\n    # Test case 2: Another image of a person wearing clothes\n    url2 = 'https://placekitten.com/400/600'\n    result2 = segment_clothes(url2)\n    assert isinstance(result2, type(plt)), 'Test Case 2 Failed'\n\n    # Test case 3: Yet another image of a person wearing clothes\n    url3 = 'https://placekitten.com/800/1200'\n    result3 = segment_clothes(url3)\n    assert isinstance(result3, type(plt)), 'Test Case 3 Failed'\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_segment_clothes()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\n# function_code --------------------\n\ndef segment_clothes(image_path):\n    \"\"\"\n    This function segments clothes in an image using a pre-trained SegFormer model.\n\n    Args:\n        image_path (str): The path to the image file or a URL.\n\n    Returns:\n        A matplotlib figure showing the segmented image.\n    \"\"\"", "answer": "\n    extractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\n    image = Image.open(requests.get(image_path, stream=True).raw)\n    model = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\n    inputs = extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n\n    logits = outputs.logits.cpu()\n    upsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\n    pred_seg = upsampled_logits.argmax(dim=1)[0]\n    plt.imshow(pred_seg)\n    return plt\n\n", "prediction": "\n    \n    # Load our pre-trained feature extractor and semantic segmentation model\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n    model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to('cuda')\n    \n    # Load the image from the given path\n    img = Image.open(requests.get(image_path, stream=True).raw if str(image_path).startswith('http') else image_path)\n\n    # Apply our feature extractor to get the features of the input image\n    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n    \n    # Feed those features into the model and obtain the logits (i.e., the predictions before softmax)\n    outputs = model(**inputs)['logits']\n    \n    # Get the highest scoring segmentation from the model\n    predicted_segmentation = nn.functional.softmax(outputs, dim=1).argmax(dim=1)\n\n    # Create a PIL image object from the segmentation and plot it using matplotlib\n    fig, ax = plt.subplots()\n    seg = Image.open(requests.get('https://github.com/huggingface/transformers/raw/master/tests/dummy-data/samples/image-segmentation/ade20k_val_00000003.jpg', stream=True).raw if str(image_path).startswith('http') else 'ade20k_val_00000003.jpg')\n    ax.imshow((predicted_segmentation == 16).numpy().astype(float))\n    \n    return fig"}