{"path": "output/hf-eval-data-v3-valid/f00580_extract_entities.py", "content": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n# test_function_code --------------------\n\ndef test_extract_entities():\n    \"\"\"\n    Test the extract_entities function.\n    \"\"\"\n    sentence1 = \"Apple's CEO is Tim Cook and Microsoft's CEO is Satya Nadella\"\n    sentence2 = \"Google's CEO is Sundar Pichai\"\n    sentence3 = \"Amazon's CEO is Andy Jassy\"\n    assert isinstance(extract_entities(sentence1), dict)\n    assert isinstance(extract_entities(sentence2), dict)\n    assert isinstance(extract_entities(sentence3), dict)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_extract_entities()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_entities():\n    \"\"\"\n    Test the extract_entities function.\n    \"\"\"\n    sentence1 = \"Apple's CEO is Tim Cook and Microsoft's CEO is Satya Nadella\"\n    sentence2 = \"Google's CEO is Sundar Pichai\"\n    sentence3 = \"Amazon's CEO is Andy Jassy\"\n    assert isinstance(extract_entities(sentence1), dict)\n    assert isinstance(extract_entities(sentence2), dict)\n    assert isinstance(extract_entities(sentence3), dict)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_entities()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# function_code --------------------\n\ndef extract_entities(sentence: str) -> dict:\n    \"\"\"\n    Extract entities from a provided sentence mentioning various companies and their CEOs.\n\n    Args:\n        sentence (str): The sentence from which to extract entities.\n\n    Returns:\n        dict: A dictionary with the entities and their types.\n    \"\"\"", "answer": "\n    model = AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    tokenizer = AutoTokenizer.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\n    inputs = tokenizer(sentence, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs\n\n", "prediction": "\n    \n    # Load the pretrained model and tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    model = AutoModelForTokenClassification.from_pretrained(\"./model/\")\n    \n    # Encode the sentence using BERT tokenizer\n    encoding = tokenizer(sentence, return_offsets_mapping=True)\n    input_ids = torch.tensor([encoding[\"input_ids\"]])\n    attention_mask = torch.tensor([encoding[\"attention_mask\"]])\n    \n    # Get the predicted tags using our model\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        \n    labels = np.argmax(outputs[0].numpy(), axis=2)[0]\n    \n    # Create a mapping between the tagged tokens and the original sentence\n    tokens = tokenizer.convert_ids_to_tokens(input_ids.numpy()[0])\n    offsets = encoding[\"offset_mapping\"][0]\n    tags = [(token, label) for token, label in zip(tokens, labels) if not token.startswith(\"##\")]\n    \n    # Collect all entities and their types\n    entities = {}\n    current_entity = \"\"\n    entity_type = \"O\"\n    for item in tags:\n        if item[1] != 0:\n            if item[1] == 1 or item[1] ==2: #B-CEO/B-CMPY\n                if current_entity != \"\":\n                    entities[current_entity] = entity_type\n                entity_type = \"B\" + item[0].split(\"-\")[-1]\n                current_entity = \"\"\n            else: #I-CEO/I-CMPY\n                if not current_entity or entity_type.endswith(item[0].split(\"-\")[-1]):\n                    current_entity += item[0] \n    \n    return entities"}