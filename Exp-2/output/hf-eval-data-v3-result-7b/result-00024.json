{"path": "output/hf-eval-data-v3-valid/f00126_get_best_answer.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef get_best_answer(query: str, passages: list) -> str:\n    \"\"\"\n    Given a query and a list of passages, this function returns the passage that best answers the query.\n    \n    Args:\n        query (str): The question to be answered.\n        passages (list): A list of possible answer passages.\n    \n    Returns:\n        str: The passage that best answers the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    features = tokenizer([query]*len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_passages = [passages[idx] for idx in scores.argsort(descending=True)]\n    best_passage = sorted_passages[0]\n    return best_passage\n\n# test_function_code --------------------\n\ndef test_get_best_answer():\n    \"\"\"\n    Test the function get_best_answer.\n    \"\"\"\n    query = 'What is the capital of France?'\n    passages = ['Paris is the capital of France.', 'London is the capital of England.', 'Berlin is the capital of Germany.']\n    assert get_best_answer(query, passages) == 'Paris is the capital of France.'\n    \n    query = 'Who won the world cup in 2018?'\n    passages = ['France won the world cup in 2018.', 'Germany won the world cup in 2014.', 'Brazil won the world cup in 2002.']\n    assert get_best_answer(query, passages) == 'France won the world cup in 2018.'\n    \n    query = 'Who is the CEO of Tesla?'\n    passages = ['Elon Musk is the CEO of Tesla.', 'Bill Gates is the CEO of Microsoft.', 'Jeff Bezos is the CEO of Amazon.']\n    assert get_best_answer(query, passages) == 'Elon Musk is the CEO of Tesla.'\n    \n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_get_best_answer()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef get_best_answer(query: str, passages: list) -> str:\n    \"\"\"\n    Given a query and a list of passages, this function returns the passage that best answers the query.\n    \n    Args:\n        query (str): The question to be answered.\n        passages (list): A list of possible answer passages.\n    \n    Returns:\n        str: The passage that best answers the query.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    features = tokenizer([query]*len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_passages = [passages[idx] for idx in scores.argsort(descending=True)]\n    best_passage = sorted_passages[0]\n    return best_passage\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_get_best_answer():\n    \"\"\"\n    Test the function get_best_answer.\n    \"\"\"\n    query = 'What is the capital of France?'\n    passages = ['Paris is the capital of France.', 'London is the capital of England.', 'Berlin is the capital of Germany.']\n    assert get_best_answer(query, passages) == 'Paris is the capital of France.'\n    \n    query = 'Who won the world cup in 2018?'\n    passages = ['France won the world cup in 2018.', 'Germany won the world cup in 2014.', 'Brazil won the world cup in 2002.']\n    assert get_best_answer(query, passages) == 'France won the world cup in 2018.'\n    \n    query = 'Who is the CEO of Tesla?'\n    passages = ['Elon Musk is the CEO of Tesla.', 'Bill Gates is the CEO of Microsoft.', 'Jeff Bezos is the CEO of Amazon.']\n    assert get_best_answer(query, passages) == 'Elon Musk is the CEO of Tesla.'\n    \n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_get_best_answer()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef get_best_answer(query: str, passages: list) -> str:\n    \"\"\"\n    Given a query and a list of passages, this function returns the passage that best answers the query.\n    \n    Args:\n        query (str): The question to be answered.\n        passages (list): A list of possible answer passages.\n    \n    Returns:\n        str: The passage that best answers the query.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n    features = tokenizer([query]*len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        scores = model(**features).logits\n    sorted_passages = [passages[idx] for idx in scores.argsort(descending=True)]\n    best_passage = sorted_passages[0]\n    return best_passage\n\n", "prediction": "\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n    model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n    \n    # Tokenize query and passages, then add them to the tokenizer\n    inputs = tokenizer([\"[CLS] \" + query + \" [SEP]\"], \n                       text_pair=passages, \n                       padding=\"max_length\", \n                       max_length=512, \n                       truncation=True)\n    \n    # Load the model into CUDA if available, otherwise load to CPU.\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n    \n    # Convert the input ids to a PyTorch tensor and add it as an argument to the forward method.\n    inputs = {k:torch.tensor([v], device=device) for (k, v) in inputs.items()} \n    \n    outputs = model(**inputs)   # Perform inference on the input\n    logits = outputs[0]         # Grab the scores produced by the first head of our base model\n    pred_score, pred_index = torch.max(logits, dim=1)  # Get the highest prediction score and index\n    \n    return passages[int(pred_index)]\n\ndef get_best_answer_batches(queries: list, passages: list) -> list:\n    \"\"\"\n    Given a batch of queries and a list of passages, this function returns the passage that best answers each query.\n    \n    Args:\n        query (list): A list containing the questions to be answered.\n        passages (list): A list of possible answer passages.\n        \n    Returns:\n        str: The passage that best answers the question.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned"}