{"path": "output/hf-eval-data-v3-valid/f00845_detect_objects.py", "content": "# function_import --------------------\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_objects(url: str, texts: list, model_name: str = 'google/owlvit-large-patch14', score_threshold: float = 0.1):\n    \"\"\"\n    Detect objects in an image based on specific text phrases using the OwlViT model.\n\n    Args:\n        url (str): The URL of the image.\n        texts (list): A list of text descriptions.\n        model_name (str, optional): The name of the OwlViT model. Defaults to 'google/owlvit-large-patch14'.\n        score_threshold (float, optional): The score threshold for filtering detections. Defaults to 0.1.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and bounding box locations.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n    for i, result in enumerate(results):\n        boxes, scores, labels = result['boxes'], result['scores'], result['labels']\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[label]} with confidence {round(score.item(), 3)} at location {box}')\n\n# test_function_code --------------------\n\ndef test_detect_objects():\n    \"\"\"\n    Test the detect_objects function.\n    \"\"\"\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    texts = ['a photo of a cat', 'a photo of a dog']\n    try:\n        detect_objects(url, texts)\n        print('Test passed.')\n    except Exception as e:\n        print('Test failed. Error: ', e)\n\n# call_test_function_code --------------------\n\ntest_detect_objects()", "function_import": "# function_import --------------------\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n", "function_code": "# function_code --------------------\n\ndef detect_objects(url: str, texts: list, model_name: str = 'google/owlvit-large-patch14', score_threshold: float = 0.1):\n    \"\"\"\n    Detect objects in an image based on specific text phrases using the OwlViT model.\n\n    Args:\n        url (str): The URL of the image.\n        texts (list): A list of text descriptions.\n        model_name (str, optional): The name of the OwlViT model. Defaults to 'google/owlvit-large-patch14'.\n        score_threshold (float, optional): The score threshold for filtering detections. Defaults to 0.1.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and bounding box locations.\n    \"\"\"\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n    for i, result in enumerate(results):\n        boxes, scores, labels = result['boxes'], result['scores'], result['labels']\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_detect_objects():\n    \"\"\"\n    Test the detect_objects function.\n    \"\"\"\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    texts = ['a photo of a cat', 'a photo of a dog']\n    try:\n        detect_objects(url, texts)\n        print('Test passed.')\n    except Exception as e:\n        print('Test failed. Error: ', e)\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_detect_objects()", "instruct": "# function_import --------------------\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n# function_code --------------------\n\ndef detect_objects(url: str, texts: list, model_name: str = 'google/owlvit-large-patch14', score_threshold: float = 0.1):\n    \"\"\"\n    Detect objects in an image based on specific text phrases using the OwlViT model.\n\n    Args:\n        url (str): The URL of the image.\n        texts (list): A list of text descriptions.\n        model_name (str, optional): The name of the OwlViT model. Defaults to 'google/owlvit-large-patch14'.\n        score_threshold (float, optional): The score threshold for filtering detections. Defaults to 0.1.\n\n    Returns:\n        None. Prints the detected objects, their confidence scores, and bounding box locations.\n    \"\"\"", "answer": "\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name)\n    image = Image.open(requests.get(url, stream=True).raw)\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n    for i, result in enumerate(results):\n        boxes, scores, labels = result['boxes'], result['scores'], result['labels']\n        for box, score, label in zip(boxes, scores, labels):\n            box = [round(i, 2) for i in box.tolist()]\n            if score >= score_threshold:\n                print(f'Detected {texts[label]} with confidence {round(score.item(), 3)} at location {box}')\n\n", "prediction": "\n    \n    # Setup the processor\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    processor = OwlViTProcessor.from_pretrained(model_name)\n    model = OwlViTForObjectDetection.from_pretrained(model_name).to(device)\n    \n    # Get the image and preprocess it using PIL\n    response = requests.get(url, stream=True).raw\n    im = Image.open(response)\n    img_pil = Image.new(\"RGB\", (2048, 1728), color=\"black\") # This is to handle images that do not have a multiple of 64 width and height\n    img_pil.paste(im)\n    \n    inputs = processor(images=img_pil, return_tensors='pt')\n    pixel_values = inputs['pixel_values'].to(device)\n    \n    # Perform object detection\n    outputs = model(pixel_values)\n\n    # Process the detections\n    probs = outputs.logits_per_image[0][0].softmax(dim=-1).cpu().detach().numpy()\n    bboxes = outputs.bboxes_per_image[0].cpu().detach().numpy()\n    \n    # Print the detections with their confidence scores and bounding box locations\n    for text, prob in zip(texts, probs):\n        filtered_probs = [prob[i] for i, label in enumerate(processor.get_labels()) if '{}'.format(text) == label]\n        if len(filtered_probs) > 0:\n            index = list(filtered_probs).index(max(list(filtered_probs)))\n            score = round((max(filtered_probs)), 3)\n            print('{}: {}'.format(text, score))\n            \n            if score >= score_threshold:\n                xmin = int(round(bboxes[index][0]))\n                ymin = int(round(bboxes[index][1]))\n               "}