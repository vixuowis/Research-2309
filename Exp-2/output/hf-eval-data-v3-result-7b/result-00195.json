{"path": "output/hf-eval-data-v3-valid/f00762_video_action_recognition.py", "content": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n# test_function_code --------------------\n\ndef test_video_action_recognition():\n    '''\n    Function to test the video_action_recognition function.\n    '''\n    file_path = hf_hub_download('archery.mp4')\n    assert isinstance(video_action_recognition(file_path), str), 'The function should return a string.'\n    assert video_action_recognition(file_path) != '', 'The function should not return an empty string.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_video_action_recognition()", "function_import": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n", "function_code": "# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_video_action_recognition():\n    '''\n    Function to test the video_action_recognition function.\n    '''\n    file_path = hf_hub_download('archery.mp4')\n    assert isinstance(video_action_recognition(file_path), str), 'The function should return a string.'\n    assert video_action_recognition(file_path) != '', 'The function should not return an empty string.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_video_action_recognition()", "instruct": "# function_import --------------------\n\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\n\n# function_code --------------------\n\ndef video_action_recognition(file_path: str, clip_len: int = 16, frame_sample_rate: int = 4):\n    '''\n    Function to recognize the main action in a video clip using VideoMAE model.\n    \n    Args:\n        file_path (str): Path to the video file.\n        clip_len (int, optional): Length of the clip for which the action is to be recognized. Default is 16.\n        frame_sample_rate (int, optional): Frame sample rate. Default is 4.\n    \n    Returns:\n        str: The recognized main action in the video clip.\n    '''", "answer": "\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n    model = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\n\n    inputs = feature_extractor(list(video), return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n\n", "prediction": "\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_id = 'microsoft/videmae-large' # pretrained model on Clevrer dataset\n    \n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained(model_id)\n    feature_extractor = feature_extractor.to(device)\n\n    max_frames = 288\n    frame_rate = 30\n\n    video_reader = VideoReader(file_path, ctx=cpu(0)) # create a decord video reader object\n    \n    clip = []\n    for frame in range(int(min((video_reader.total_frames // frame_sample_rate), max_frames) * frame_sample_rate)):\n        clip.append(video_reader[frame::frame_sample_rate].asnumpy()) # extract video frames and store them as numpy array\n    \n    if clip_len < len(clip): \n        start = np.random.randint(0, len(clip) - clip_len)\n        end = start + clip_len\n        \n    else:\n        start = 0\n        end = clip_len - (len(clip) % clip_len)\n    \n    video_array = np.stack(clip[start:end]) # convert the numpy array frames to a tensor\n    video_tensor = torch.from_numpy(video_array).to(device) / 255 # normalize the values of pixels between [0,1]\n    \n    inputs = feature_extractor(video_tensor, sampling_strategy='random', frames_per_second=frame_rate, max_frames=max_frames)['input_values'][None] # extract features from video clip\n    \n    model = VideoMAEForVideoClassification.from_pretrained(model_id).to(device)\n    \n    with torch.no_grad():\n        outputs = model(inputs)[0].cpu().numpy()\n        \n    action_dict = {'action': 'None', 'probability': 1}\n    for i in range(len(outputs)):"}