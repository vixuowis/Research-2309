{"path": "output/hf-eval-data-v3-valid/f00862_rank_search_results.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef rank_search_results(query: str, passages: list) -> list:\n    \"\"\"\n    Ranks the given passages based on their relevance to the given query using a pretrained model.\n\n    Args:\n        query (str): The search query.\n        passages (list): The list of passages to be ranked.\n\n    Returns:\n        list: The list of passages ranked in descending order of relevance.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    features = tokenizer([query] * len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n\n    sorted_passages = sorted(zip(passages, scores.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n    return sorted_passages\n\n# test_function_code --------------------\n\ndef test_rank_search_results():\n    \"\"\"\n    Tests the rank_search_results function with some test cases.\n    \"\"\"\n    query = 'How many people live in Berlin?'\n    passages = [\n        'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.',\n        'New York City is famous for the Metropolitan Museum of Art.',\n        'Berlin is the capital of Germany and one of the 16 states of Germany.',\n        'Berlin is known for its festivals, diverse architecture, nightlife, contemporary arts, and a high quality of living.'\n    ]\n    result = rank_search_results(query, passages)\n    assert isinstance(result, list), 'The result should be a list.'\n    assert len(result) == len(passages), 'The result should have the same length as the input passages.'\n    assert all(isinstance(item, tuple) and len(item) == 2 for item in result), 'Each item in the result should be a tuple with two elements.'\n    assert all(isinstance(item[0], str) and isinstance(item[1], float) for item in result), 'Each item in the result should be a tuple with a string and a float.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_rank_search_results()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef rank_search_results(query: str, passages: list) -> list:\n    \"\"\"\n    Ranks the given passages based on their relevance to the given query using a pretrained model.\n\n    Args:\n        query (str): The search query.\n        passages (list): The list of passages to be ranked.\n\n    Returns:\n        list: The list of passages ranked in descending order of relevance.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    features = tokenizer([query] * len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n\n    sorted_passages = sorted(zip(passages, scores.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n    return sorted_passages\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_rank_search_results():\n    \"\"\"\n    Tests the rank_search_results function with some test cases.\n    \"\"\"\n    query = 'How many people live in Berlin?'\n    passages = [\n        'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.',\n        'New York City is famous for the Metropolitan Museum of Art.',\n        'Berlin is the capital of Germany and one of the 16 states of Germany.',\n        'Berlin is known for its festivals, diverse architecture, nightlife, contemporary arts, and a high quality of living.'\n    ]\n    result = rank_search_results(query, passages)\n    assert isinstance(result, list), 'The result should be a list.'\n    assert len(result) == len(passages), 'The result should have the same length as the input passages.'\n    assert all(isinstance(item, tuple) and len(item) == 2 for item in result), 'Each item in the result should be a tuple with two elements.'\n    assert all(isinstance(item[0], str) and isinstance(item[1], float) for item in result), 'Each item in the result should be a tuple with a string and a float.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_rank_search_results()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef rank_search_results(query: str, passages: list) -> list:\n    \"\"\"\n    Ranks the given passages based on their relevance to the given query using a pretrained model.\n\n    Args:\n        query (str): The search query.\n        passages (list): The list of passages to be ranked.\n\n    Returns:\n        list: The list of passages ranked in descending order of relevance.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    features = tokenizer([query] * len(passages), passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n\n    sorted_passages = sorted(zip(passages, scores.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n    return sorted_passages\n\n", "prediction": "\n    \n    if not passages: # empty list check\n        return []\n        \n    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n    model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", \n                                                               return_dict=True)\n    \n    # Preprocess the list of passages to a single string: passage_str\n    passage_str = \"\"\n    for passage in passages:\n        passage_str += \" \".join(passage) + \" \"\n        \n    # Tokenize and encode the query and passage.\n    inputs = tokenizer(\"[CLS] \" + query + \" [SEP]\"  + passage_str + \"[SEP]\", return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        # Get the logits for each example.\n        outputs = model(**inputs)\n        \n    # get the scores for each example\n    logits = outputs.logits[:, 0]\n    \n    # Rank in descending order.\n    _, indices = torch.sort(-1 * logits)\n    \n    return [passages[i] for i in indices]"}