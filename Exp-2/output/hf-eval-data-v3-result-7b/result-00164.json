{"path": "output/hf-eval-data-v3-valid/f00672_find_relevant_passage.py", "content": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n# test_function_code --------------------\n\ndef test_find_relevant_passage():\n    \"\"\"\n    Test the function find_relevant_passage.\n    \"\"\"\n    question = 'How many people live in Berlin?'\n    candidate_passages = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert isinstance(find_relevant_passage(question, candidate_passages), str)\n    question = 'What is the capital of Germany?'\n    candidate_passages = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert find_relevant_passage(question, candidate_passages) == 'Berlin is the capital of Germany.'\n    question = 'Who won the world cup in 2014?'\n    candidate_passages = ['Germany won the world cup in 2014.', 'Brazil hosted the world cup in 2014.']\n    assert find_relevant_passage(question, candidate_passages) == 'Germany won the world cup in 2014.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_find_relevant_passage()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n", "function_code": "# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_find_relevant_passage():\n    \"\"\"\n    Test the function find_relevant_passage.\n    \"\"\"\n    question = 'How many people live in Berlin?'\n    candidate_passages = ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.']\n    assert isinstance(find_relevant_passage(question, candidate_passages), str)\n    question = 'What is the capital of Germany?'\n    candidate_passages = ['Berlin is the capital of Germany.', 'Paris is the capital of France.']\n    assert find_relevant_passage(question, candidate_passages) == 'Berlin is the capital of Germany.'\n    question = 'Who won the world cup in 2014?'\n    candidate_passages = ['Germany won the world cup in 2014.', 'Brazil hosted the world cup in 2014.']\n    assert find_relevant_passage(question, candidate_passages) == 'Germany won the world cup in 2014.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_find_relevant_passage()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# function_code --------------------\n\ndef find_relevant_passage(question: str, candidate_passages: list) -> str:\n    \"\"\"\n    Find the most relevant passage given a question and several candidate passages.\n\n    Args:\n        question (str): The question to be answered.\n        candidate_passages (list): A list of candidate passages.\n\n    Returns:\n        str: The most relevant passage.\n\n    Raises:\n        OSError: If there is a problem with loading the pre-trained model or tokenizer.\n    \"\"\"", "answer": "\n    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    features = tokenizer([question] * len(candidate_passages), candidate_passages, padding=True, truncation=True, return_tensors='pt')\n    model.eval()\n    with torch.no_grad():\n        scores = model(**features).logits\n        sorted_passages = [x for _, x in sorted(zip(scores.detach().numpy(), candidate_passages), reverse=True)]\n    return sorted_passages[0]\n\n", "prediction": "\n    # load pretrained model and tokenizer for BERT-base\n    try:\n        model_name = \"bert-large-uncased\"\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)\n    except OSError as e: print(e)\n\n    # prepare the input data for BERT model (question + passages)\n    question_inputs = tokenizer(f\"question: {question} \", padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n    passage_inputs = [tokenizer(\" \".join([x[\"title\"], x[\"text\"]]), padding=\"max_length\", truncation=True, max_length=512) for x in candidate_passages]\n\n    # prepare the labels for BERT model (0: not relevant, 1: relevant)\n    labels = torch.tensor(range(len(candidate_passages)))\n\n    # calculate the prediction probabilities for each passage based on the question using BERT-base\n    logits = []\n    with torch.no_grad():\n        model.eval()\n        for i in range(0, len(candidate_passages), 8):\n            batch = [question_inputs] + [x[\"input_ids\"] for x in passage_inputs[i:i+4]]\n            input_dict = {k: torch.stack([v[j] for v in batch], dim=0) for j, k in enumerate(batch[0].keys())}\n            logits += model(**input_dict).logits.tolist()\n    probabilities = [torch.sigmoid(torch.tensor([x])).item() for x in logits]\n\n    # choose the most relevant passage based on its prediction probability by the BERT-base model (relevant > not relevant)\n    return candidate_passages[probabilities.index(max(probabilities))][\"title\"]"}