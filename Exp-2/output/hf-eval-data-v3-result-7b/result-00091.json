{"path": "output/hf-eval-data-v3-valid/f00395_answer_question.py", "content": "# function_import --------------------\n\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\n\n# function_code --------------------\n\ndef answer_question(question: str, context: str) -> str:\n    \"\"\"\n    This function uses a pre-trained model from Hugging Face Transformers to answer a question based on a given context.\n\n    Args:\n        question (str): The question to be answered.\n        context (str): The context in which the question is asked.\n\n    Returns:\n        str: The answer to the question.\n    \"\"\"\n    model = AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')\n    tokenizer = AutoTokenizer.from_pretrained('deepset/deberta-v3-large-squad2')\n\n    inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)\n    output = model(**inputs)\n    answer_start = output.start_logits.argmax().item()\n    answer_end = output.end_logits.argmax().item()\n\n    ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end+1]))\n    return ans\n\n# test_function_code --------------------\n\ndef test_answer_question():\n    \"\"\"\n    This function tests the answer_question function with some test cases.\n    \"\"\"\n    question1 = 'What is the capital of France?'\n    context1 = 'Paris is the capital of France.'\n    assert answer_question(question1, context1) == 'Paris'\n\n    question2 = 'Who won the world cup in 2018?'\n    context2 = 'The 2018 FIFA World Cup was won by France.'\n    assert answer_question(question2, context2) == 'France'\n\n    question3 = 'Who is the CEO of Tesla?'\n    context3 = 'Elon Musk is the CEO of Tesla.'\n    assert answer_question(question3, context3) == 'Elon Musk'\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_answer_question()", "function_import": "# function_import --------------------\n\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\n\n", "function_code": "# function_code --------------------\n\ndef answer_question(question: str, context: str) -> str:\n    \"\"\"\n    This function uses a pre-trained model from Hugging Face Transformers to answer a question based on a given context.\n\n    Args:\n        question (str): The question to be answered.\n        context (str): The context in which the question is asked.\n\n    Returns:\n        str: The answer to the question.\n    \"\"\"\n    model = AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')\n    tokenizer = AutoTokenizer.from_pretrained('deepset/deberta-v3-large-squad2')\n\n    inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)\n    output = model(**inputs)\n    answer_start = output.start_logits.argmax().item()\n    answer_end = output.end_logits.argmax().item()\n\n    ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end+1]))\n    return ans\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_answer_question():\n    \"\"\"\n    This function tests the answer_question function with some test cases.\n    \"\"\"\n    question1 = 'What is the capital of France?'\n    context1 = 'Paris is the capital of France.'\n    assert answer_question(question1, context1) == 'Paris'\n\n    question2 = 'Who won the world cup in 2018?'\n    context2 = 'The 2018 FIFA World Cup was won by France.'\n    assert answer_question(question2, context2) == 'France'\n\n    question3 = 'Who is the CEO of Tesla?'\n    context3 = 'Elon Musk is the CEO of Tesla.'\n    assert answer_question(question3, context3) == 'Elon Musk'\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_answer_question()", "instruct": "# function_import --------------------\n\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\n\n# function_code --------------------\n\ndef answer_question(question: str, context: str) -> str:\n    \"\"\"\n    This function uses a pre-trained model from Hugging Face Transformers to answer a question based on a given context.\n\n    Args:\n        question (str): The question to be answered.\n        context (str): The context in which the question is asked.\n\n    Returns:\n        str: The answer to the question.\n    \"\"\"", "answer": "\n    model = AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')\n    tokenizer = AutoTokenizer.from_pretrained('deepset/deberta-v3-large-squad2')\n\n    inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)\n    output = model(**inputs)\n    answer_start = output.start_logits.argmax().item()\n    answer_end = output.end_logits.argmax().item()\n\n    ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end+1]))\n    return ans\n\n", "prediction": "\n    # Set up model and tokenizer ----------------\n    \n    model_name = \"mrm8488/spanbert-finetuned-squadv2\"\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Tokenize ----------------------------\n    \n    inputs = tokenizer(question, context, add_special_tokens=True, truncation=\"only_second\", return_tensors=\"pt\")\n    \n    input_ids = inputs[\"input_ids\"].tolist()[0]\n    attention_mask = inputs[\"attention_mask\"].tolist()[0]\n    \n    # Get answer -------------------------------\n    \n    outputs = model(**inputs)\n    answer_start_scores=outputs.start_logits\n    answer_end_scores=outputs.end_logits\n    start_index=answer_start_scores.argmax() \n    end_index=answer_end_scores.argmax()\n    \n    token_ids=[tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n    answer=\"\"\n    if len(token_ids[0])>start_index:\n        answer+=\" \".join(token_ids[0][start_index:end_index + 1])\n    \n    # Return ------------------------------------\n    \n    return answer.strip()\n"}