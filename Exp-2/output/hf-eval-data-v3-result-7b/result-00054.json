{"path": "output/hf-eval-data-v3-valid/f00213_get_answer_from_book.py", "content": "# function_import --------------------\n\nfrom transformers import pipeline\n\n# function_code --------------------\n\ndef get_answer_from_book(book_text: str, user_question: str) -> str:\n    '''\n    This function uses a pre-trained model from Hugging Face Transformers to answer questions based on a given context.\n\n    Args:\n        book_text (str): The context in which to find the answer.\n        user_question (str): The question to answer.\n\n    Returns:\n        str: The answer to the question.\n    '''\n    qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\n    result = qa_pipeline({'context': book_text, 'question': user_question})\n    return result['answer']\n\n# test_function_code --------------------\n\ndef test_get_answer_from_book():\n    '''\n    This function tests the get_answer_from_book function.\n    '''\n    book_text = 'The sky is blue.'\n    user_question = 'What color is the sky?'\n    assert get_answer_from_book(book_text, user_question) == 'blue'\n    \n    book_text = 'Dogs are mammals.'\n    user_question = 'What are dogs?'\n    assert get_answer_from_book(book_text, user_question) == 'mammals'\n    \n    book_text = 'The earth revolves around the sun.'\n    user_question = 'What does the earth revolve around?'\n    assert get_answer_from_book(book_text, user_question) == 'the sun'\n    \n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_get_answer_from_book()", "function_import": "# function_import --------------------\n\nfrom transformers import pipeline\n\n", "function_code": "# function_code --------------------\n\ndef get_answer_from_book(book_text: str, user_question: str) -> str:\n    '''\n    This function uses a pre-trained model from Hugging Face Transformers to answer questions based on a given context.\n\n    Args:\n        book_text (str): The context in which to find the answer.\n        user_question (str): The question to answer.\n\n    Returns:\n        str: The answer to the question.\n    '''\n    qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\n    result = qa_pipeline({'context': book_text, 'question': user_question})\n    return result['answer']\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_get_answer_from_book():\n    '''\n    This function tests the get_answer_from_book function.\n    '''\n    book_text = 'The sky is blue.'\n    user_question = 'What color is the sky?'\n    assert get_answer_from_book(book_text, user_question) == 'blue'\n    \n    book_text = 'Dogs are mammals.'\n    user_question = 'What are dogs?'\n    assert get_answer_from_book(book_text, user_question) == 'mammals'\n    \n    book_text = 'The earth revolves around the sun.'\n    user_question = 'What does the earth revolve around?'\n    assert get_answer_from_book(book_text, user_question) == 'the sun'\n    \n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_get_answer_from_book()", "instruct": "# function_import --------------------\n\nfrom transformers import pipeline\n\n# function_code --------------------\n\ndef get_answer_from_book(book_text: str, user_question: str) -> str:\n    '''\n    This function uses a pre-trained model from Hugging Face Transformers to answer questions based on a given context.\n\n    Args:\n        book_text (str): The context in which to find the answer.\n        user_question (str): The question to answer.\n\n    Returns:\n        str: The answer to the question.\n    '''", "answer": "\n    qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\n    result = qa_pipeline({'context': book_text, 'question': user_question})\n    return result['answer']\n\n", "prediction": "\n    # Initialize model.\n    nlp = pipeline('question-answering')\n\n    # Define input for model.\n    inputs = {0: book_text, 1: user_question}\n\n    # Call model on given context and question.\n    answer = nlp(inputs)['answer']\n    return answer"}