{"path": "output/hf-eval-data-v3-valid/f00138_summarize_text.py", "content": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"Test the function summarize_text.\"\"\"\n    input_text1 = 'This is a long article about the history of the world. It covers many different topics and periods.'\n    input_text2 = 'This is another long article, this time about the future of technology. It discusses many potential advancements and challenges.'\n    assert isinstance(summarize_text(input_text1), str)\n    assert isinstance(summarize_text(input_text2), str)\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_summarize_text()", "function_import": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n", "function_code": "# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_summarize_text():\n    \"\"\"Test the function summarize_text.\"\"\"\n    input_text1 = 'This is a long article about the history of the world. It covers many different topics and periods.'\n    input_text2 = 'This is another long article, this time about the future of technology. It discusses many potential advancements and challenges.'\n    assert isinstance(summarize_text(input_text1), str)\n    assert isinstance(summarize_text(input_text2), str)\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_summarize_text()", "instruct": "# function_import --------------------\n\nimport requests\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# function_code --------------------\n\ndef summarize_text(input_text: str) -> str:\n    \"\"\"\n    Summarize a given text using the pre-trained model 'sshleifer/distilbart-cnn-12-6'.\n\n    Args:\n        input_text (str): The text to be summarized.\n\n    Returns:\n        str: The summarized text.\n\n    Raises:\n        requests.exceptions.ChunkedEncodingError: If there is a connection error while downloading the model.\n    \"\"\"", "answer": "\n    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n    inputs = tokenizer(input_text, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary_text\n\n", "prediction": "\n    \n    model_name = 'sshleifer/distilbart-cnn-12-6'\n    \n    model_dir = '/opt/ml/input/artifacts/model/' if run_locally else sagemaker_session.download_model(job_name, f'{model_name}')\n    tokenizer = BartTokenizer.from_pretrained(f'{model_dir}/tokenizer') \n    model = BartForConditionalGeneration.from_pretrained(f'{model_dir}/model/')\n    \n    input_ids = tokenizer([input_text], return_tensors=\"pt\", padding=True).input_ids # add batch dimension (see https://huggingface.co/transformers/main_classes/tokenizer.html)\n    summary_ids = model.generate(input_ids)[0]\n    \n    return tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0]\n\n# ----------------------------------\n\n# main_code ------------------------\n\nif __name__ == '__main__':\n    \n    run_locally = False # set this flag to true if running code locally and not in an AWS SageMaker environment\n    \n    if run_locally: \n        \n        text = \"Summarize this article, please!\"\n                     \n        model_name = 'sshleifer/distilbart-cnn-12-6'\n        \n        tokenizer = BartTokenizer.from_pretrained(model_name) # add additional arguments as required by the model\n        model = BartForConditionalGeneration.from_pretrained(model_name, force_download=True) \n    \n        input_ids = tokenizer([text], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=1024).input_ids # add batch dimension (see https://huggingface.co/transformers/main_classes/tokenizer.html)\n        summary_ids = model.generate(input_ids)[0]\n        \n        print('The summarized text is:')\n        print("}