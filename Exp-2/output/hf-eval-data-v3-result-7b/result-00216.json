{"path": "output/hf-eval-data-v3-valid/f00826_encode_sentences.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import BertModel, BertTokenizerFast\n\n# function_code --------------------\n\ndef encode_sentences(sentences):\n    \"\"\"\n    Encode sentences using the pre-trained LaBSE model.\n\n    Args:\n        sentences (list): A list of sentences to be encoded.\n\n    Returns:\n        torch.Tensor: The encoded sentences.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\n    model = BertModel.from_pretrained('setu4993/LaBSE')\n    model = model.eval()\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.pooler_output\n    return embeddings\n\n# test_function_code --------------------\n\ndef test_encode_sentences():\n    \"\"\"\n    Test the encode_sentences function.\n    \"\"\"\n    sentences = [\n        'dog',\n        'Cuccioli sono carini.',\n        '\u72ac\u3068\u4e00\u7dd2\u306b\u30d3\u30fc\u30c1\u3092\u6563\u6b69\u3059\u308b\u306e\u304c\u597d\u304d',\n    ]\n    embeddings = encode_sentences(sentences)\n    assert embeddings.shape[0] == len(sentences), 'The number of embeddings should be equal to the number of sentences.'\n    assert embeddings.shape[1] == 768, 'The dimension of each embedding should be 768.'\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_encode_sentences()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import BertModel, BertTokenizerFast\n\n", "function_code": "# function_code --------------------\n\ndef encode_sentences(sentences):\n    \"\"\"\n    Encode sentences using the pre-trained LaBSE model.\n\n    Args:\n        sentences (list): A list of sentences to be encoded.\n\n    Returns:\n        torch.Tensor: The encoded sentences.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\n    model = BertModel.from_pretrained('setu4993/LaBSE')\n    model = model.eval()\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.pooler_output\n    return embeddings\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_encode_sentences():\n    \"\"\"\n    Test the encode_sentences function.\n    \"\"\"\n    sentences = [\n        'dog',\n        'Cuccioli sono carini.',\n        '\u72ac\u3068\u4e00\u7dd2\u306b\u30d3\u30fc\u30c1\u3092\u6563\u6b69\u3059\u308b\u306e\u304c\u597d\u304d',\n    ]\n    embeddings = encode_sentences(sentences)\n    assert embeddings.shape[0] == len(sentences), 'The number of embeddings should be equal to the number of sentences.'\n    assert embeddings.shape[1] == 768, 'The dimension of each embedding should be 768.'\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_encode_sentences()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import BertModel, BertTokenizerFast\n\n# function_code --------------------\n\ndef encode_sentences(sentences):\n    \"\"\"\n    Encode sentences using the pre-trained LaBSE model.\n\n    Args:\n        sentences (list): A list of sentences to be encoded.\n\n    Returns:\n        torch.Tensor: The encoded sentences.\n    \"\"\"", "answer": "\n    tokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\n    model = BertModel.from_pretrained('setu4993/LaBSE')\n    model = model.eval()\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.pooler_output\n    return embeddings\n\n", "prediction": "\n    # Tokenize with the LaBSE tokenizer.\n    tokenized = [tokenizer(sentence)[\"input_ids\"] for sentence in sentences]\n    \n    # Pad all the sequences to the same length.\n    lengths = [len(sequence) for sequence in tokenized]\n    padded   = torch.zeros((len(tokenized), max(lengths)), dtype=torch.long)\n\n    # Replace with the actual values.\n    for index, (seq, length) in enumerate(zip(tokenized, lengths)):\n        padded[index, :length] = torch.tensor(seq[:length])\n    \n    # Encode all of these sentences together using the LaBSE model.\n    with torch.no_grad():\n        encoded = model(padded)['sentence_embedding']\n\n    return encoded\n\n# main --------------------\n\nif __name__ == \"__main__\":\n    # Initialize the tokenizer and pre-trained LaBSE model.\n    model      = BertModel.from_pretrained(\"monologg/LaBSE\")\n    tokenizer  = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n    # Encode a list of sentences.\n    sentences = [\n        \"I love the smell of burnt rubber in the morning.\",\n        \"You're so ugly I can't see anything else\",\n        \"I am not afraid of any monster under my bed.\"\n    ]\n    embedded  = encode_sentences(sentences)\n    print(embedded.shape)"}