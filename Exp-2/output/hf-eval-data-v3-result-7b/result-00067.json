{"path": "output/hf-eval-data-v3-valid/f00255_extract_positional_relations.py", "content": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# function_code --------------------\n\ndef extract_positional_relations(text):\n    \"\"\"\n    Extracts the positional relations between various keywords of a given medical text using the SapBERT model.\n\n    Args:\n        text (str): The medical text from which to extract positional relations.\n\n    Returns:\n        torch.Tensor: The [CLS] embedding of the last layer, indicating the position of the embedded biomedical entities.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n    return cls_embedding\n\n# test_function_code --------------------\n\ndef test_extract_positional_relations():\n    \"\"\"\n    Tests the extract_positional_relations function.\n    \"\"\"\n    # Test case: Normal case\n    output = extract_positional_relations('covid infection')\n    assert isinstance(output, torch.Tensor), 'Output should be a PyTorch Tensor.'\n\n    # Test case: Empty string\n    output = extract_positional_relations('')\n    assert isinstance(output, torch.Tensor), 'Output should be a PyTorch Tensor.'\n\n    # Test case: Long string\n    output = extract_positional_relations('covid infection' * 100)\n    assert isinstance(output, torch.Tensor), 'Output should be a PyTorch Tensor.'\n\n    print('All Tests Passed')\n\n# call_test_function_code --------------------\n\ntest_extract_positional_relations()", "function_import": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n", "function_code": "# function_code --------------------\n\ndef extract_positional_relations(text):\n    \"\"\"\n    Extracts the positional relations between various keywords of a given medical text using the SapBERT model.\n\n    Args:\n        text (str): The medical text from which to extract positional relations.\n\n    Returns:\n        torch.Tensor: The [CLS] embedding of the last layer, indicating the position of the embedded biomedical entities.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n    return cls_embedding\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_extract_positional_relations():\n    \"\"\"\n    Tests the extract_positional_relations function.\n    \"\"\"\n    # Test case: Normal case\n    output = extract_positional_relations('covid infection')\n    assert isinstance(output, torch.Tensor), 'Output should be a PyTorch Tensor.'\n\n    # Test case: Empty string\n    output = extract_positional_relations('')\n    assert isinstance(output, torch.Tensor), 'Output should be a PyTorch Tensor.'\n\n    # Test case: Long string\n    output = extract_positional_relations('covid infection' * 100)\n    assert isinstance(output, torch.Tensor), 'Output should be a PyTorch Tensor.'\n\n    print('All Tests Passed')\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_extract_positional_relations()", "instruct": "# function_import --------------------\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# function_code --------------------\n\ndef extract_positional_relations(text):\n    \"\"\"\n    Extracts the positional relations between various keywords of a given medical text using the SapBERT model.\n\n    Args:\n        text (str): The medical text from which to extract positional relations.\n\n    Returns:\n        torch.Tensor: The [CLS] embedding of the last layer, indicating the position of the embedded biomedical entities.\n    \"\"\"", "answer": "\n    tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n    return cls_embedding\n\n", "prediction": "\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/sapbert\")\n    model = AutoModel.from_pretrained(\"dmis-lab/sapbert\").to('cuda')\n    \n    tokens = tokenizer.encode(text, return_tensors='pt').to('cuda')\n    outputs = model(tokens).pooler_output\n    \n    return outputs[0]"}