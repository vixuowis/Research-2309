{"path": "output/hf-eval-data-v3-valid/f00290_identify_street_location.py", "content": "# function_import --------------------\n\nfrom transformers import CLIPModel, CLIPProcessor\nfrom PIL import Image\nimport requests\n\n# function_code --------------------\n\ndef identify_street_location(image_url: str, choices: list):\n    \"\"\"\n    Identify the location of a street-level image using the Hugging Face Transformers' CLIPModel.\n\n    Args:\n        image_url (str): The URL of the street-level image.\n        choices (list): A list of possible locations.\n\n    Returns:\n        str: The location with the highest probability.\n    \"\"\"\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n\n    image = Image.open(requests.get(image_url, stream=True).raw)\n\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    max_prob_index = probs.argmax().item()\n    return choices[max_prob_index]\n\n# test_function_code --------------------\n\ndef test_identify_street_location():\n    \"\"\"\n    Test the identify_street_location function.\n    \"\"\"\n    image_url = 'https://placekitten.com/200/300'\n    choices = ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco']\n\n    location = identify_street_location(image_url, choices)\n    assert location in choices\n\n    image_url = 'https://placekitten.com/200/300'\n    choices = ['New York', 'Chicago', 'Boston', 'Seattle', 'Austin']\n\n    location = identify_street_location(image_url, choices)\n    assert location in choices\n\n    image_url = 'https://placekitten.com/200/300'\n    choices = ['London', 'Paris', 'Berlin', 'Rome', 'Madrid']\n\n    location = identify_street_location(image_url, choices)\n    assert location in choices\n\n    return 'All Tests Passed'\n\n# call_test_function_code --------------------\n\ntest_identify_street_location()", "function_import": "# function_import --------------------\n\nfrom transformers import CLIPModel, CLIPProcessor\nfrom PIL import Image\nimport requests\n\n", "function_code": "# function_code --------------------\n\ndef identify_street_location(image_url: str, choices: list):\n    \"\"\"\n    Identify the location of a street-level image using the Hugging Face Transformers' CLIPModel.\n\n    Args:\n        image_url (str): The URL of the street-level image.\n        choices (list): A list of possible locations.\n\n    Returns:\n        str: The location with the highest probability.\n    \"\"\"\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n\n    image = Image.open(requests.get(image_url, stream=True).raw)\n\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    max_prob_index = probs.argmax().item()\n    return choices[max_prob_index]\n\n", "test_function_code": "# test_function_code --------------------\n\ndef test_identify_street_location():\n    \"\"\"\n    Test the identify_street_location function.\n    \"\"\"\n    image_url = 'https://placekitten.com/200/300'\n    choices = ['San Jose', 'San Diego', 'Los Angeles', 'Las Vegas', 'San Francisco']\n\n    location = identify_street_location(image_url, choices)\n    assert location in choices\n\n    image_url = 'https://placekitten.com/200/300'\n    choices = ['New York', 'Chicago', 'Boston', 'Seattle', 'Austin']\n\n    location = identify_street_location(image_url, choices)\n    assert location in choices\n\n    image_url = 'https://placekitten.com/200/300'\n    choices = ['London', 'Paris', 'Berlin', 'Rome', 'Madrid']\n\n    location = identify_street_location(image_url, choices)\n    assert location in choices\n\n    return 'All Tests Passed'\n\n", "call_test_function_code": "# call_test_function_code --------------------\n\ntest_identify_street_location()", "instruct": "# function_import --------------------\n\nfrom transformers import CLIPModel, CLIPProcessor\nfrom PIL import Image\nimport requests\n\n# function_code --------------------\n\ndef identify_street_location(image_url: str, choices: list):\n    \"\"\"\n    Identify the location of a street-level image using the Hugging Face Transformers' CLIPModel.\n\n    Args:\n        image_url (str): The URL of the street-level image.\n        choices (list): A list of possible locations.\n\n    Returns:\n        str: The location with the highest probability.\n    \"\"\"", "answer": "\n    model = CLIPModel.from_pretrained('geolocal/StreetCLIP')\n    processor = CLIPProcessor.from_pretrained('geolocal/StreetCLIP')\n\n    image = Image.open(requests.get(image_url, stream=True).raw)\n\n    inputs = processor(text=choices, images=image, return_tensors='pt', padding=True)\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    max_prob_index = probs.argmax().item()\n    return choices[max_prob_index]\n\n", "prediction": "\n    \n    model_name = \"openai/clip-vit-base-patch32\" # CLIPModel from Hugging Face Transformers.\n    processor_name = \"openai/clip-vit-base-patch32\" # CLIPProcessor from Hugging Face Transformers.\n    \n    model = CLIPModel.from_pretrained(model_name) # Load the model.\n    processor = CLIPProcessor.from_pretrained(processor_name) # Load the processor.\n    \n    image_url = str(image_url) # Convert the image URL to a string.\n        \n    response = requests.get(image_url, stream=True).raw\n    input_image = Image.open(response)\n    preprocessed_img = processor(images=input_image, return_tensors=\"pt\").pixel_values # Preprocess the image for CLIP. \n    \n    image_embedding = model.get_image_features(preprocessed_img).unsqueeze(0) # Create an embedding of the image.\n    text_embeddings = processor.encode(\"street\", return_tensors='pt').unsqueeze(0) # Create an embedding of \"street\".\n    \n    logits_per_image, logits_per_text = model(pixel_values=preprocessed_img, text=text_embeddings) # Obtain the probabilities for each choice. \n\n    probs_list = [] # Create an empty list to store the probabilities of each choice.\n\n    for i in range(len(choices)):\n        logits_per_choice = model(pixel_values=image_embedding, text=text_embeddings[:, [i]])[0]\n        probs_list.append(logits_per_choice.softmax(dim=-1)[-1].detach().numpy())\n    \n    probabilities = list(zip(choices, probs_list)) # Zip the choices with their respective probabilities. \n    highest_probability = max(probs_list) # Get the choice with the highest probability from the list of probabilities. \n"}