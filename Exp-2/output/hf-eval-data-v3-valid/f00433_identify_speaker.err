2023-11-12 00:32:32.848411: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-12 00:32:32.903768: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-12 00:32:33.771244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Downloading (…)lve/main/config.json:   0%|                                                                         | 0.00/54.9k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████| 54.9k/54.9k [00:00<00:00, 289kB/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████| 54.9k/54.9k [00:00<00:00, 288kB/s]
/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Downloading pytorch_model.bin:   0%|                                                                                | 0.00/380M [00:00<?, ?B/s]Downloading pytorch_model.bin:   6%|███▉                                                                    | 21.0M/380M [00:00<00:02, 160MB/s]Downloading pytorch_model.bin:  14%|█████████▉                                                              | 52.4M/380M [00:00<00:01, 198MB/s]Downloading pytorch_model.bin:  19%|█████████████▉                                                          | 73.4M/380M [00:00<00:01, 197MB/s]Downloading pytorch_model.bin:  28%|████████████████████▏                                                    | 105M/380M [00:00<00:01, 222MB/s]Downloading pytorch_model.bin:  36%|██████████████████████████▏                                              | 136M/380M [00:00<00:01, 232MB/s]Downloading pytorch_model.bin:  44%|████████████████████████████████▎                                        | 168M/380M [00:00<00:00, 230MB/s]Downloading pytorch_model.bin:  52%|██████████████████████████████████████▎                                  | 199M/380M [00:00<00:00, 230MB/s]Downloading pytorch_model.bin:  61%|████████████████████████████████████████████▎                            | 231M/380M [00:01<00:00, 229MB/s]Downloading pytorch_model.bin:  69%|██████████████████████████████████████████████████▍                      | 262M/380M [00:01<00:00, 238MB/s]Downloading pytorch_model.bin:  77%|████████████████████████████████████████████████████████▍                | 294M/380M [00:01<00:00, 235MB/s]Downloading pytorch_model.bin:  86%|██████████████████████████████████████████████████████████████▌          | 325M/380M [00:01<00:00, 230MB/s]Downloading pytorch_model.bin:  94%|████████████████████████████████████████████████████████████████████▌    | 357M/380M [00:01<00:00, 231MB/s]Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████| 380M/380M [00:01<00:00, 226MB/s]
Some weights of the model checkpoint at superb/wav2vec2-base-superb-sid were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-sid and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)rocessor_config.json:   0%|                                                                           | 0.00/215 [00:00<?, ?B/s]Downloading (…)rocessor_config.json: 100%|████████████████████████████████████████████████████████████████████| 215/215 [00:00<00:00, 69.8kB/s]
