{'code': '###Instruction: We want our System to generate possible user queries for a document provided as a text input.\n###Output: <<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: T5ForConditionalGeneration.from_pretrained(\'castorini/doc2query-t5-base-msmarco\')\n<<<api_provider>>>: Transformers\n<<<explanation>>>: 1. Import the required classes from the transformers library, such as T5Tokenizer and T5ForConditionalGeneration.\n2. Use the from_pretrained method of T5ForConditionalGeneration and T5Tokenizer classes to load the pre-trained model \'castorini/doc2query-t5-base-msmarco\' and its corresponding tokenizer.\n3. Tokenize the input document using the tokenizer\'s \'encode\' method.\n4. Perform text-to-text generation using the T5 language model by passing the tokenized input to the model\'s \'generate\' method.\n5. Decode the generated tokens back into text format using the tokenizer\'s \'decode\' method.\n6. The resulting text will consist of potential user queries based on the input document.\n<<<code>>>: from transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\'castorini/doc2query-t5-base-msmarco\')\nmodel = T5ForConditionalGeneration.from_pretrained(\'castorini/doc2query-t5-base-msmarco\')\ndocument = "Your document text goes here..."\ninput_ids = tokenizer.encode(document, return_tensors=\'pt\')\ngenerated_ids = model.generate(input_ids)\ngenerated_queries = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n', 'api_call': "T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'castorini/doc2query-t5-base-msmarco', 'api_call': "T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')", 'api_arguments': 'text, max_length', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'MS MARCO', 'accuracy': 'Not specified'}, 'description': 'A T5 model trained on the MS MARCO dataset for generating queries from documents.'}}

You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
