{'code': "###Instruction: We want to facilitate a conversation with our Russian customers. Build a model that can respond to customer inquiries in Russian.\n###Output: <<<domain>>>: Natural Language Processing Conversational\n<<<api_call>>>: AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We first import the necessary libraries, AutoTokenizer and AutoModelWithLMHead from the transformers package provided by Hugging Face.\n2. We use the from_pretrained method of AutoTokenizer to load the tokenizer 'tinkoff-ai/ruDialoGPT-medium'. This tokenizer is trained to tokenize conversational text in Russian.\n3. We similarly use the method AutoModelWithLMHead.from_pretrained to load the pre-trained model 'tinkoff-ai/ruDialoGPT-medium'.\n4. Now the loaded conversational model can be used to generate responses to customer inquiries in Russian.\n5. Following is an example of simple code that can be used to generate a response to a given input text.\n<<<code>>>: import torch\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\nmodel = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\ninputs = tokenizer('@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@', return_tensors='pt')\ngenerated_token_ids = model.generate(\n    **inputs,\n    top_k=10,\n    top_p=0.95,\n    num_beams=3,\n    num_return_sequences=3,\n    do_sample=True,\n    no_repeat_ngram_size=2,\n    temperature=1.2,\n    repetition_penalty=1.2,\n    length_penalty=1.0,\n    eos_token_id=50257,\n    max_new_tokens=40\n)\ncontext_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\ncontext_with_response\n", 'api_call': "AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tinkoff-ai/ruDialoGPT-medium', 'api_call': "AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')", 'api_arguments': {'pretrained_model_name_or_path': 'tinkoff-ai/ruDialoGPT-medium'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': "import torch\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\nmodel = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\ninputs = tokenizer('@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@', return_tensors='pt')\ngenerated_token_ids = model.generate(\n **inputs,\n top_k=10,\n top_p=0.95,\n num_beams=3,\n num_return_sequences=3,\n do_sample=True,\n no_repeat_ngram_size=2,\n temperature=1.2,\n repetition_penalty=1.2,\n length_penalty=1.0,\n eos_token_id=50257,\n max_new_tokens=40\n)\ncontext_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\ncontext_with_response", 'performance': {'dataset': 'Private Validation Set', 'sensibleness': 0.78, 'specificity': 0.69, 'SSA': 0.735}, 'description': "This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents. The model was trained with context size 3."}}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
