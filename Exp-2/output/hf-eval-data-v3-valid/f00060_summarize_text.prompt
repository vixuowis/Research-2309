{'code': '###Instruction: I developed a document generation app, I need to create a summary of a long article given as input to provide to my users before they read the full article.\n###Output: <<<domain>>>: Natural Language Processing Summarization\n<<<api_call>>>: BigBirdPegasusForConditionalGeneration.from_pretrained(\'google/bigbird-pegasus-large-bigpatent\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import BigBirdPegasusForConditionalGeneration and AutoTokenizer from the transformers library.\n2. Instantiate the tokenizer by using AutoTokenizer.from_pretrained() with the model name \'google/bigbird-pegasus-large-bigpatent\'.\n3. Load the pre-trained BigBird Pegasus model for text summarization using BigBirdPegasusForConditionalGeneration.from_pretrained() with the model name \'google/bigbird-pegasus-large-bigpatent\'.\n4. Provide the long article as input to the tokenizer, which returns a dictionary of input tensors.\n5. Use the model\'s generate() method to create a summary of the article from the input tensors.\n6. Decode the generated tokens back into a summary text by using the tokenizer\'s batch_decode() method.\n<<<code>>>: from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\'google/bigbird-pegasus-large-bigpatent\')\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\'google/bigbird-pegasus-large-bigpatent\')\ntext = "Your long article text here..."\ninputs = tokenizer(text, return_tensors=\'pt\')\nprediction = model.generate(**inputs)\nsummary = tokenizer.batch_decode(prediction)[0]', 'api_call': "BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'google/bigbird-pegasus-large-bigpatent', 'api_call': "BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')", 'api_arguments': {'attention_type': 'original_full', 'block_size': 16, 'num_random_blocks': 2}, 'python_environment_requirements': 'transformers', 'example_code': "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-bigpatent)\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-bigpatent)\ntext = Replace me by any text you'd like.\ninputs = tokenizer(text, return_tensors='pt')\nprediction = model.generate(**inputs)\nprediction = tokenizer.batch_decode(prediction)", 'performance': {'dataset': 'big_patent', 'accuracy': 'Not provided'}, 'description': 'BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts.'}}

