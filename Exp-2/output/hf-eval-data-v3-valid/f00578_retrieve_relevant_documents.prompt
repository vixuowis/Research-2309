{'code': '###Instruction: I am developing software that needs to retrieve relevant information from a collection of documents based on a user\'s query.\n###Output: <<<domain>>>: Natural Language Processing Text Classification\n<<<api_call>>>: AutoModelForSequenceClassification.from_pretrained(\'cross-encoder/ms-marco-TinyBERT-L-2-v2\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the required libraries from the \'transformers\' package: AutoTokenizer and AutoModelForSequenceClassification.\n2. Load the model \'cross-encoder/ms-marco-TinyBERT-L-2-v2\' using the AutoModelForSequenceClassification.from_pretrained() method.\n3. Load the tokenizer for the specified model using AutoTokenizer.from_pretrained() method.\n4. Tokenize the user\'s query and documents using the tokenizer\'s method, while also padding and truncating features as required, with return_tensors set to \'pt\' (PyTorch tensors).\n5. Pass the tokenized tensors to the model to calculate relevance scores for each document.\n6. Sort the documents based on the relevance scores in decreasing order and return the top-ranked documents as the most relevant results.\n\n<<<code>>>: from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\'cross-encoder/ms-marco-TinyBERT-L-2-v2\')\ntokenizer = AutoTokenizer.from_pretrained(\'cross-encoder/ms-marco-TinyBERT-L-2-v2\')\nquery = "How many people live in Berlin?"\ndocuments = ["Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.", "New York City is famous for the Metropolitan Museum of Art."]\n\nfeatures = tokenizer([query]*len(documents), documents, padding=True, truncation=True, return_tensors=\'pt\')\n\nwith torch.no_grad():\n    scores = model(**features).logits\nsorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Information Retrieval', 'api_name': 'cross-encoder/ms-marco-TinyBERT-L-2-v2', 'api_call': "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')", 'api_arguments': {'tokenizer': "tokenizer = AutoTokenizer.from_pretrained('model_name')", 'features': "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')"}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': {'import': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification', 'model': "model = AutoModelForSequenceClassification.from_pretrained('model_name')", 'tokenizer': "tokenizer = AutoTokenizer.from_pretrained('model_name')", 'features': "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')", 'scores': 'with torch.no_grad():\n    scores = model(**features).logits\n    print(scores)'}, 'performance': {'dataset': 'TREC Deep Learning 2019', 'accuracy': '69.84 (NDCG@10)'}, 'description': 'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.'}}

