{'code': '###Instruction: Design a model to classify the following image: a city park with a playground and a lake, surrounded by trees and skyscrapers.\n###Output: <<<domain>>>: Computer Vision Zero-Shot Image Classification\n<<<api_call>>>: CLIPModel.from_pretrained(\'flax-community/clip-rsicd-v2\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. Import the required libraries (PIL, requests, CLIPProcessor, and CLIPModel) and load the fine-tuned CLIP model called \'flax-community/clip-rsicd-v2\'.\n2. Download the image of the city park (or provide the image file path) and open it using the Image class from the PIL library.\n3. Define labels for classification: \'residential area\', \'playground\', \'stadium\', \'forest\', \'airport\'.\n4. Process the text and the image using the CLIPProcessor to create input tensors.\n5. Pass the input tensors to the model and obtain the logits_per_image output.\n6. Apply a softmax function to the logits_per_image output to determine probabilities for each label.\n7. Print the probabilities of each label as output for classification.\n<<<code>>>: from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\'flax-community/clip-rsicd-v2\')\nprocessor = CLIPProcessor.from_pretrained(\'flax-community/clip-rsicd-v2\')\nimg_url = \'https://example.com/city_park_image.jpg\'\nimage = Image.open(requests.get(img_url, stream=True).raw)\nlabels = [\'residential area\', \'playground\', \'stadium\', \'forest\', \'airport\']\ninputs = processor(text=[f\'a photo of a {l}\' for l in labels], images=image, return_tensors=\'pt\', padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nfor l, p in zip(labels, probs[0]):\n  print(f"{l:<16} {p:.4f}")', 'api_call': "CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'flax-community/clip-rsicd-v2', 'api_call': "CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')", 'api_arguments': {'text': ['a photo of a residential area', 'a photo of a playground', 'a photo of a stadium', 'a photo of a forest', 'a photo of an airport'], 'images': 'image', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nlabels = [residential area, playground, stadium, forest, airport]\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nfor l, p in zip(labels, probs[0]):\n print(f{l:&lt;16} {p:.4f})', 'performance': {'dataset': {'RSICD': {'original CLIP': {'k=1': 0.572, 'k=3': 0.745, 'k=5': 0.837, 'k=10': 0.939}, 'clip-rsicd-v2 (this model)': {'k=1': 0.883, 'k=3': 0.968, 'k=5': 0.982, 'k=10': 0.998}}}}, 'description': 'This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.'}}

