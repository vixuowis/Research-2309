{'code': '###Instruction: Someone asked the question "What is the capital of Sweden?" in the context "Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions."\n###Input: {"question": "What is the capital of Sweden?", "context": "Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions."}\n###Output: <<<domain>>>: Natural Language Processing Question Answering\n<<<api_call>>>: pipeline(\'question-answering\', model=\'bert-large-uncased-whole-word-masking-finetuned-squad\')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Import the pipeline function from the transformers library provided by Hugging Face.\n2. Use the pipeline function to create a question-answering model.\n3. Specify the model \'bert-large-uncased-whole-word-masking-finetuned-squad\' to be loaded. This model is trained to answer questions based on context.\n4. Pass a dictionary containing the question and context as input to the model. The model will find the most likely answer in the provided context.\n<<<code>>>: from transformers import pipeline\nqa_model = pipeline(\'question-answering\', model=\'bert-large-uncased-whole-word-masking-finetuned-squad\')\nresult = qa_model({\'question\': \'What is the capital of Sweden?\', \'context\': \'Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions.\'})\nanswer = result[\'answer\']', 'api_call': "AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'api_call': "AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')", 'api_arguments': {'model_name_or_path': 'bert-large-uncased-whole-word-masking', 'dataset_name': 'squad', 'do_train': True, 'do_eval': True, 'learning_rate': 3e-05, 'num_train_epochs': 2, 'max_seq_length': 384, 'doc_stride': 128, 'output_dir': './examples/models/wwm_uncased_finetuned_squad/', 'per_device_eval_batch_size': 3, 'per_device_train_batch_size': 3}, 'python_environment_requirements': {'torch': '1.9.0', 'transformers': '4.9.2'}, 'example_code': "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\nprint(result)", 'performance': {'dataset': 'SQuAD', 'accuracy': {'f1': 93.15, 'exact_match': 86.91}}, 'description': 'BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.'}}



