Downloading (…)lve/main/config.json:   0%|                                                                           | 0.00/729 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████████████| 729/729 [00:00<00:00, 64.4kB/s]
Downloading model.safetensors:   0%|                                                                                | 0.00/496M [00:00<?, ?B/s]Downloading model.safetensors:   4%|███                                                                     | 21.0M/496M [00:00<00:03, 134MB/s]Downloading model.safetensors:   8%|██████                                                                  | 41.9M/496M [00:00<00:03, 145MB/s]Downloading model.safetensors:  13%|█████████▏                                                              | 62.9M/496M [00:00<00:03, 144MB/s]Downloading model.safetensors:  17%|████████████▏                                                           | 83.9M/496M [00:00<00:02, 153MB/s]Downloading model.safetensors:  21%|███████████████▍                                                         | 105M/496M [00:00<00:02, 155MB/s]Downloading model.safetensors:  25%|██████████████████▌                                                      | 126M/496M [00:00<00:02, 157MB/s]Downloading model.safetensors:  30%|█████████████████████▌                                                   | 147M/496M [00:00<00:02, 151MB/s]Downloading model.safetensors:  34%|████████████████████████▋                                                | 168M/496M [00:01<00:02, 152MB/s]Downloading model.safetensors:  38%|███████████████████████████▊                                             | 189M/496M [00:01<00:02, 152MB/s]Downloading model.safetensors:  42%|██████████████████████████████▊                                          | 210M/496M [00:01<00:01, 157MB/s]Downloading model.safetensors:  46%|█████████████████████████████████▉                                       | 231M/496M [00:01<00:01, 160MB/s]Downloading model.safetensors:  51%|█████████████████████████████████████                                    | 252M/496M [00:01<00:01, 160MB/s]Downloading model.safetensors:  55%|████████████████████████████████████████                                 | 273M/496M [00:01<00:01, 161MB/s]Downloading model.safetensors:  59%|███████████████████████████████████████████▏                             | 294M/496M [00:01<00:01, 161MB/s]Downloading model.safetensors:  63%|██████████████████████████████████████████████▎                          | 315M/496M [00:02<00:01, 161MB/s]Downloading model.safetensors:  68%|█████████████████████████████████████████████████▎                       | 336M/496M [00:02<00:00, 163MB/s]Downloading model.safetensors:  72%|████████████████████████████████████████████████████▍                    | 357M/496M [00:02<00:00, 165MB/s]Downloading model.safetensors:  76%|███████████████████████████████████████████████████████▌                 | 377M/496M [00:02<00:00, 162MB/s]Downloading model.safetensors:  80%|██████████████████████████████████████████████████████████▌              | 398M/496M [00:02<00:00, 159MB/s]Downloading model.safetensors:  85%|█████████████████████████████████████████████████████████████▋           | 419M/496M [00:02<00:00, 151MB/s]Downloading model.safetensors:  89%|████████████████████████████████████████████████████████████████▊        | 440M/496M [00:02<00:00, 144MB/s]Downloading model.safetensors:  93%|███████████████████████████████████████████████████████████████████▊     | 461M/496M [00:02<00:00, 150MB/s]Downloading model.safetensors:  97%|██████████████████████████████████████████████████████████████████████▉  | 482M/496M [00:03<00:00, 156MB/s]Downloading model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████| 496M/496M [00:03<00:00, 155MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2-distilled and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "./f00051_get_answer_from_document.py", line 48, in <module>
    test_get_answer_from_document()
  File "./f00051_get_answer_from_document.py", line 34, in test_get_answer_from_document
    assert get_answer_from_document(context, question) is not None
  File "./f00051_get_answer_from_document.py", line 22, in get_answer_from_document
    qa_pipeline = pipeline('question-answering', model=qa_model)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 904, in pipeline
    raise Exception(
Exception: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.
