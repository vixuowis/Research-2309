2023-11-12 20:13:15.984168: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-12 20:13:16.038861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-12 20:13:16.930513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Downloading model.safetensors:   0%|                                                                                            | 0.00/412M [00:00<?, ?B/s]Downloading model.safetensors:   5%|████▎                                                                               | 21.0M/412M [00:00<00:02, 155MB/s]Downloading model.safetensors:  10%|████████▌                                                                           | 41.9M/412M [00:00<00:02, 166MB/s]Downloading model.safetensors:  18%|██████████████▉                                                                     | 73.4M/412M [00:00<00:02, 133MB/s]Downloading model.safetensors:  23%|███████████████████▎                                                                | 94.4M/412M [00:00<00:02, 151MB/s]Downloading model.safetensors:  28%|███████████████████████▊                                                             | 115M/412M [00:00<00:01, 165MB/s]Downloading model.safetensors:  33%|████████████████████████████▏                                                        | 136M/412M [00:00<00:01, 165MB/s]Downloading model.safetensors:  41%|██████████████████████████████████▋                                                  | 168M/412M [00:01<00:01, 180MB/s]Downloading model.safetensors:  48%|█████████████████████████████████████████▏                                           | 199M/412M [00:01<00:01, 193MB/s]Downloading model.safetensors:  54%|█████████████████████████████████████████████▍                                       | 220M/412M [00:01<00:01, 191MB/s]Downloading model.safetensors:  59%|█████████████████████████████████████████████████▊                                   | 241M/412M [00:01<00:00, 194MB/s]Downloading model.safetensors:  66%|████████████████████████████████████████████████████████▎                            | 273M/412M [00:01<00:00, 198MB/s]Downloading model.safetensors:  71%|████████████████████████████████████████████████████████████▋                        | 294M/412M [00:01<00:00, 199MB/s]Downloading model.safetensors:  76%|████████████████████████████████████████████████████████████████▉                    | 315M/412M [00:01<00:00, 201MB/s]Downloading model.safetensors:  82%|█████████████████████████████████████████████████████████████████████▎               | 336M/412M [00:01<00:00, 203MB/s]Downloading model.safetensors:  87%|█████████████████████████████████████████████████████████████████████████▋           | 357M/412M [00:01<00:00, 204MB/s]Downloading model.safetensors:  92%|█████████████████████████████████████████████████████████████████████████████▉       | 377M/412M [00:02<00:00, 200MB/s]Downloading model.safetensors:  97%|██████████████████████████████████████████████████████████████████████████████████▎  | 398M/412M [00:02<00:00, 197MB/s]Downloading model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████| 412M/412M [00:02<00:00, 185MB/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "./f00892_fill_mask_chinese.py", line 53, in <module>
    test_fill_mask_chinese()
  File "./f00892_fill_mask_chinese.py", line 32, in test_fill_mask_chinese
    result1 = fill_mask_chinese(text1)
  File "./f00892_fill_mask_chinese.py", line 21, in fill_mask_chinese
    result = fill_mask(text)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 270, in __call__
    outputs = super().__call__(inputs, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1140, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1146, in run_single
    model_inputs = self.preprocess(inputs, **preprocess_params)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 123, in preprocess
    self.ensure_exactly_one_mask_token(model_inputs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 112, in ensure_exactly_one_mask_token
    self._ensure_exactly_one_mask_token(input_ids)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 100, in _ensure_exactly_one_mask_token
    raise PipelineException(
transformers.pipelines.base.PipelineException: No mask_token ([MASK]) found on the input
