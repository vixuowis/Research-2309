{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa9815c-5c45-44d6-9ca5-056e1d582994",
   "metadata": {},
   "source": [
    "- 准备数据集，处理 gorilla 的 instruction + code example\n",
    "    - Instruction 任务说明\n",
    "    - Function，接受端到端任务\n",
    "    - Test function\n",
    "    - Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e91a969-02a6-4f28-bb73-2615c4e52670",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936,\n",
       " {'domain': 'Natural Language Processing Feature Extraction',\n",
       "  'framework': 'Hugging Face Transformers',\n",
       "  'functionality': 'Feature Extraction',\n",
       "  'api_name': 'YituTech/conv-bert-base',\n",
       "  'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\",\n",
       "  'api_arguments': 'N/A',\n",
       "  'python_environment_requirements': 'transformers',\n",
       "  'example_code': 'N/A',\n",
       "  'performance': {'dataset': 'N/A', 'accuracy': 'N/A'},\n",
       "  'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "def load_jsonl_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            d = json.loads(l)\n",
    "            data.append(d)\n",
    "            \n",
    "    return data\n",
    "\n",
    "hf_api_data = load_jsonl_data(\"gorilla/data/api/huggingface_api.jsonl\")\n",
    "len(hf_api_data), hf_api_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "224550eb-1e53-4654-993a-3ca0d0a8a847",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural Language Processing Text2Text Generation', 41),\n",
      " ('Natural Language Processing Text Generation', 39),\n",
      " ('Natural Language Processing Sentence Similarity', 33),\n",
      " ('Computer Vision Image Classification', 33),\n",
      " ('Natural Language Processing Token Classification', 33),\n",
      " ('Natural Language Processing Zero-Shot Classification', 33),\n",
      " ('Natural Language Processing Text Classification', 32),\n",
      " ('Audio Automatic Speech Recognition', 31),\n",
      " ('Natural Language Processing Table Question Answering', 31),\n",
      " ('Computer Vision Video Classification', 30),\n",
      " ('Multimodal Text-to-Image', 30),\n",
      " ('Multimodal Image-to-Text', 30),\n",
      " ('Computer Vision Object Detection', 30),\n",
      " ('Computer Vision Image Segmentation', 30),\n",
      " ('Natural Language Processing Fill-Mask', 30),\n",
      " ('Natural Language Processing Question Answering', 29),\n",
      " ('Multimodal Document Question Answer', 29),\n",
      " ('Computer Vision Depth Estimation', 29),\n",
      " ('Computer Vision Unconditional Image Generation', 29),\n",
      " ('Audio Text-to-Speech', 29),\n",
      " ('Audio Audio-to-Audio', 27),\n",
      " ('Computer Vision Image-to-Image', 26),\n",
      " ('Natural Language Processing Translation', 26),\n",
      " ('Tabular Tabular Classification', 25),\n",
      " ('Natural Language Processing Summarization', 24),\n",
      " ('Audio Audio Classification', 24),\n",
      " ('Tabular Tabular Regression', 23),\n",
      " ('Computer Vision Zero-Shot Image Classification', 22),\n",
      " ('Reinforcement Learning', 19),\n",
      " ('Multimodal Visual Question Answering', 18),\n",
      " ('Natural Language Processing Conversational', 18),\n",
      " ('Audio Voice Activity Detection', 12),\n",
      " ('Multimodal Text-to-Video', 10),\n",
      " ('Multimodal Feature Extraction', 9),\n",
      " ('Natural Language Processing Feature Extraction', 6),\n",
      " ('Audio Classification', 6),\n",
      " ('Reinforcement Learning Robotics', 4),\n",
      " ('Multimodal Graph Machine Learning', 3),\n",
      " ('Multimodal Zero-Shot Image Classification', 2),\n",
      " ('Multimodal Document Question Answering', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "domain_counter_dict = Counter()\n",
    "\n",
    "for d in hf_api_data:\n",
    "    domain_counter_dict[d['domain']] += 1\n",
    "        \n",
    "pprint.pp(domain_counter_dict.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf0b66eb-2242-4584-b04c-61b71cb14c87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8191,\n",
       " {'code': \"###Instruction: Write an API implementation that takes customer reviews as input and extracts features to analyze customer sentiment.\\n###Output: <<<domain>>>: Natural Language Processing Feature Extraction\\n<<<api_call>>>: AutoModel.from_pretrained('YituTech/conv-bert-base')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. We import the necessary classes from the transformers package. This includes AutoTokenizer and AutoModel for tokenizing and processing customer review text.\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'YituTech/conv-bert-base'. This model is based on ConvBERT and is suitable for feature extraction in text data.\\n3. We load the customer review text, tokenize it, and use the model to extract features from the review. These features can then be used to analyze customer sentiment.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('YituTech/conv-bert-base')\\nmodel = AutoModel.from_pretrained('YituTech/conv-bert-base')\\ninputs = tokenizer(customer_review, return_tensors='pt')\\nfeatures = model(**inputs)\\n\",\n",
       "  'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\",\n",
       "  'provider': 'Hugging Face Transformers',\n",
       "  'api_data': {'domain': 'Natural Language Processing Feature Extraction',\n",
       "   'framework': 'Hugging Face Transformers',\n",
       "   'functionality': 'Feature Extraction',\n",
       "   'api_name': 'YituTech/conv-bert-base',\n",
       "   'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\",\n",
       "   'api_arguments': 'N/A',\n",
       "   'python_environment_requirements': 'transformers',\n",
       "   'example_code': 'N/A',\n",
       "   'performance': {'dataset': 'N/A', 'accuracy': 'N/A'},\n",
       "   'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.'}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_data = load_jsonl_data(\"gorilla/data/apibench/huggingface_train.json\")\n",
    "len(hf_train_data), hf_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e9b2af0-7faf-46c3-b3a9-91b60ee3ea52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911,\n",
       " {'code': \"###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\\n###Output: <<<domain>>>: Natural Language Processing Sentence Similarity\\n<<<api_call>>>: AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>:1. We first import the necessary classes and modules from the transformers package. This includes AutoTokenizer and AutoModel for loading the pre-trained models from Hugging Face.\\n2. We use the AutoModel.from_pretrained() method to load the 'princeton-nlp/unsup-simcse-roberta-base' model, which is specially designed for calculating sentence similarity.\\n3. To build the recommendation feature, we process the text of previously liked articles and compute sentence embeddings. For each new article, we compute its sentence embedding and compare it to the embeddings of previously liked articles.\\n4. If the similarity between the new article's embedding and any previous liked articles' embeddings is above a certain threshold, the new article is recommended to the user.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\",\n",
       "  'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "  'provider': 'Hugging Face Transformers',\n",
       "  'api_data': {'domain': 'Natural Language Processing Sentence Similarity',\n",
       "   'framework': 'Hugging Face Transformers',\n",
       "   'functionality': 'Feature Extraction',\n",
       "   'api_name': 'princeton-nlp/unsup-simcse-roberta-base',\n",
       "   'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "   'api_arguments': None,\n",
       "   'python_environment_requirements': ['transformers'],\n",
       "   'example_code': None,\n",
       "   'performance': {'dataset': None, 'accuracy': None},\n",
       "   'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.'}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_eval_data = load_jsonl_data(\"gorilla/data/apibench/huggingface_eval.json\")\n",
    "len(hf_eval_data), hf_eval_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cec1028a-e021-4ce3-83b8-fcab7b154fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': \"###Instruction: As a journalist, I am curious about speech sentiment analysis in a group of people in a crowd. I want to extract features from the audio to run sentiment analysis.\\n###Output: <<<domain>>>: Multimodal Feature Extraction\\n<<<api_call>>>: HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. Import the necessary libraries, which include the 'HubertModel' from transformers.\\n2. Load the pretrained model 'facebook/hubert-large-ll60k', which is a self-supervised speech representation learning model, capable of dealing with unique problems in speech representation learning and extracting useful features from audio data.\\n3. Process the crowd audio data and convert it into an acceptable input format for the Hubert model.\\n4. Pass the preprocessed audio data through the Hubert model to extract features that can be used for further sentiment analysis.\\n<<<code>>>: from transformers import HubertModel\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\n# Preprocess the crowd audio data (as input_data) to a suitable input format\\ninput_data = preprocess_audio(crowd_audio)\\n# Extract features using the Hubert model\\nfeatures = hubert(input_data)\\n\",\n",
       " 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\",\n",
       " 'provider': 'Hugging Face Transformers',\n",
       " 'api_data': {'domain': 'Multimodal Feature Extraction',\n",
       "  'framework': 'Hugging Face Transformers',\n",
       "  'functionality': 'Feature Extraction',\n",
       "  'api_name': 'hubert-large-ll60k',\n",
       "  'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\",\n",
       "  'api_arguments': 'pretrained model name',\n",
       "  'python_environment_requirements': 'transformers',\n",
       "  'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\",\n",
       "  'performance': {'dataset': 'Libri-Light',\n",
       "   'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'},\n",
       "  'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.'}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_eval_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c96dd-70f5-4037-9318-b41d0cbb27d4",
   "metadata": {},
   "source": [
    "# 1. Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22da9c82-b4fe-43aa-8116-996d2dbfcbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'instruction': '###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.',\n",
       "  'domain': 'Natural Language Processing Sentence Similarity',\n",
       "  'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "  'api_provider': 'Hugging Face Transformers',\n",
       "  'code': \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\"},\n",
       " '###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instruction: apibench - {lib}_train.json - code - instruction\n",
    "\n",
    "import re\n",
    "\n",
    "def get_code_parts_from_apibench_data(data):\n",
    "    text = data['code']\n",
    "    instruction, _ = text.split(\"\\n###Output\")\n",
    "    \n",
    "    # Extracting domain, api_call, api_provider, and code using regular expressions\n",
    "    domain_pattern = r'<<<domain>>>: (.+?)\\n'\n",
    "    api_call_pattern = r'<<<api_call>>>: (.+?)\\n'\n",
    "    api_provider_pattern = r'<<<api_provider>>>: (.+?)\\n'\n",
    "    code_pattern = r'<<<code>>>: (.+)'\n",
    "\n",
    "    domain = re.search(domain_pattern, text).group(1)\n",
    "    api_call = re.search(api_call_pattern, text).group(1)\n",
    "    api_provider = re.search(api_provider_pattern, text).group(1)\n",
    "    code = re.search(code_pattern, text, re.DOTALL).group(1).strip()\n",
    "\n",
    "    return {\n",
    "        'instruction': instruction, \n",
    "        'domain': domain, \n",
    "        'api_call': api_call, \n",
    "        'api_provider': api_provider, \n",
    "        'code': code\n",
    "    }\n",
    "\n",
    "d = hf_eval_data[0]\n",
    "code_parts = get_code_parts_from_apibench_data(d)\n",
    "code_parts, code_parts['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eafb7907-b9ab-458b-ba10-5ffb814077d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\n",
      "{'code': '###Instruction: Design a feature for a social media website to '\n",
      "         'recommend articles to users based on how similar the articles are to '\n",
      "         'their previously liked articles.\\n'\n",
      "         '###Output: <<<domain>>>: Natural Language Processing Sentence '\n",
      "         'Similarity\\n'\n",
      "         '<<<api_call>>>: '\n",
      "         \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\"\n",
      "         '<<<api_provider>>>: Hugging Face Transformers\\n'\n",
      "         '<<<explanation>>>:1. We first import the necessary classes and '\n",
      "         'modules from the transformers package. This includes AutoTokenizer '\n",
      "         'and AutoModel for loading the pre-trained models from Hugging Face.\\n'\n",
      "         '2. We use the AutoModel.from_pretrained() method to load the '\n",
      "         \"'princeton-nlp/unsup-simcse-roberta-base' model, which is specially \"\n",
      "         'designed for calculating sentence similarity.\\n'\n",
      "         '3. To build the recommendation feature, we process the text of '\n",
      "         'previously liked articles and compute sentence embeddings. For each '\n",
      "         'new article, we compute its sentence embedding and compare it to the '\n",
      "         'embeddings of previously liked articles.\\n'\n",
      "         \"4. If the similarity between the new article's embedding and any \"\n",
      "         \"previous liked articles' embeddings is above a certain threshold, \"\n",
      "         'the new article is recommended to the user.\\n'\n",
      "         '<<<code>>>: from transformers import AutoTokenizer, AutoModel\\n'\n",
      "         'tokenizer = '\n",
      "         \"AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\"\n",
      "         'model = '\n",
      "         \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\",\n",
      " 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
      " 'provider': 'Hugging Face Transformers',\n",
      " 'api_data': {'domain': 'Natural Language Processing Sentence Similarity',\n",
      "              'framework': 'Hugging Face Transformers',\n",
      "              'functionality': 'Feature Extraction',\n",
      "              'api_name': 'princeton-nlp/unsup-simcse-roberta-base',\n",
      "              'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
      "              'api_arguments': None,\n",
      "              'python_environment_requirements': ['transformers'],\n",
      "              'example_code': None,\n",
      "              'performance': {'dataset': None, 'accuracy': None},\n",
      "              'description': 'An unsupervised sentence embedding model trained '\n",
      "                             'using the SimCSE approach with a Roberta base '\n",
      "                             'architecture.'}}\n"
     ]
    }
   ],
   "source": [
    "for d in hf_eval_data:\n",
    "    code_parts = get_code_parts_from_apibench_data(d)\n",
    "    print(code_parts['instruction'])\n",
    "    pprint.pp(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba423e2f-c909-435f-9326-8ec77ed554c5",
   "metadata": {},
   "source": [
    "# 2. Function / Test Function\n",
    "- code part -> gpt -> function\n",
    "- dataset 问题，先通过 prompt 解决一部分，需要对应到 huggingface dataset 名称才能对应\n",
    "- prompt:\n",
    "    generate following code based on above infomation:\n",
    "    1. function with：\n",
    "    - detailed comments\n",
    "    - function description\n",
    "    2. test function with：\n",
    "    - test dataset\n",
    "    - using assert in test function\n",
    "    - do not compare number strictly\n",
    "    - if dataset is provided in performance - dataset, load the dataset, then select several sample from the dataset, otherwise, using online source, do not leave blank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc4efd-42dd-435c-9682-ea4c336aa189",
   "metadata": {},
   "source": [
    "# 3. Dataset\n",
    "- search from {lib}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c437a-a7ae-40df-9423-838da5b5016d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
