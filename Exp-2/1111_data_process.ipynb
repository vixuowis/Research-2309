{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa9815c-5c45-44d6-9ca5-056e1d582994",
   "metadata": {},
   "source": [
    "- 准备数据集，处理 gorilla 的 instruction + code example\n",
    "    - Instruction 任务说明\n",
    "    - Function，接受端到端任务\n",
    "    - Test function\n",
    "    - Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e91a969-02a6-4f28-bb73-2615c4e52670",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936,\n",
       " {'domain': 'Natural Language Processing Feature Extraction',\n",
       "  'framework': 'Hugging Face Transformers',\n",
       "  'functionality': 'Feature Extraction',\n",
       "  'api_name': 'YituTech/conv-bert-base',\n",
       "  'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\",\n",
       "  'api_arguments': 'N/A',\n",
       "  'python_environment_requirements': 'transformers',\n",
       "  'example_code': 'N/A',\n",
       "  'performance': {'dataset': 'N/A', 'accuracy': 'N/A'},\n",
       "  'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "def load_jsonl_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            d = json.loads(l)\n",
    "            data.append(d)\n",
    "            \n",
    "    return data\n",
    "\n",
    "hf_api_data = load_jsonl_data(\"gorilla/data/api/huggingface_api.jsonl\")\n",
    "len(hf_api_data), hf_api_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224550eb-1e53-4654-993a-3ca0d0a8a847",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural Language Processing Text2Text Generation', 41),\n",
      " ('Natural Language Processing Text Generation', 39),\n",
      " ('Natural Language Processing Sentence Similarity', 33),\n",
      " ('Computer Vision Image Classification', 33),\n",
      " ('Natural Language Processing Token Classification', 33),\n",
      " ('Natural Language Processing Zero-Shot Classification', 33),\n",
      " ('Natural Language Processing Text Classification', 32),\n",
      " ('Audio Automatic Speech Recognition', 31),\n",
      " ('Natural Language Processing Table Question Answering', 31),\n",
      " ('Computer Vision Video Classification', 30),\n",
      " ('Multimodal Text-to-Image', 30),\n",
      " ('Multimodal Image-to-Text', 30),\n",
      " ('Computer Vision Object Detection', 30),\n",
      " ('Computer Vision Image Segmentation', 30),\n",
      " ('Natural Language Processing Fill-Mask', 30),\n",
      " ('Natural Language Processing Question Answering', 29),\n",
      " ('Multimodal Document Question Answer', 29),\n",
      " ('Computer Vision Depth Estimation', 29),\n",
      " ('Computer Vision Unconditional Image Generation', 29),\n",
      " ('Audio Text-to-Speech', 29),\n",
      " ('Audio Audio-to-Audio', 27),\n",
      " ('Computer Vision Image-to-Image', 26),\n",
      " ('Natural Language Processing Translation', 26),\n",
      " ('Tabular Tabular Classification', 25),\n",
      " ('Natural Language Processing Summarization', 24),\n",
      " ('Audio Audio Classification', 24),\n",
      " ('Tabular Tabular Regression', 23),\n",
      " ('Computer Vision Zero-Shot Image Classification', 22),\n",
      " ('Reinforcement Learning', 19),\n",
      " ('Multimodal Visual Question Answering', 18),\n",
      " ('Natural Language Processing Conversational', 18),\n",
      " ('Audio Voice Activity Detection', 12),\n",
      " ('Multimodal Text-to-Video', 10),\n",
      " ('Multimodal Feature Extraction', 9),\n",
      " ('Natural Language Processing Feature Extraction', 6),\n",
      " ('Audio Classification', 6),\n",
      " ('Reinforcement Learning Robotics', 4),\n",
      " ('Multimodal Graph Machine Learning', 3),\n",
      " ('Multimodal Zero-Shot Image Classification', 2),\n",
      " ('Multimodal Document Question Answering', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "domain_counter_dict = Counter()\n",
    "\n",
    "for d in hf_api_data:\n",
    "    domain_counter_dict[d['domain']] += 1\n",
    "        \n",
    "pprint.pp(domain_counter_dict.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0b66eb-2242-4584-b04c-61b71cb14c87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8191,\n",
       " {'code': \"###Instruction: Write an API implementation that takes customer reviews as input and extracts features to analyze customer sentiment.\\n###Output: <<<domain>>>: Natural Language Processing Feature Extraction\\n<<<api_call>>>: AutoModel.from_pretrained('YituTech/conv-bert-base')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. We import the necessary classes from the transformers package. This includes AutoTokenizer and AutoModel for tokenizing and processing customer review text.\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'YituTech/conv-bert-base'. This model is based on ConvBERT and is suitable for feature extraction in text data.\\n3. We load the customer review text, tokenize it, and use the model to extract features from the review. These features can then be used to analyze customer sentiment.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('YituTech/conv-bert-base')\\nmodel = AutoModel.from_pretrained('YituTech/conv-bert-base')\\ninputs = tokenizer(customer_review, return_tensors='pt')\\nfeatures = model(**inputs)\\n\",\n",
       "  'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\",\n",
       "  'provider': 'Hugging Face Transformers',\n",
       "  'api_data': {'domain': 'Natural Language Processing Feature Extraction',\n",
       "   'framework': 'Hugging Face Transformers',\n",
       "   'functionality': 'Feature Extraction',\n",
       "   'api_name': 'YituTech/conv-bert-base',\n",
       "   'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\",\n",
       "   'api_arguments': 'N/A',\n",
       "   'python_environment_requirements': 'transformers',\n",
       "   'example_code': 'N/A',\n",
       "   'performance': {'dataset': 'N/A', 'accuracy': 'N/A'},\n",
       "   'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.'}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_data = load_jsonl_data(\"gorilla/data/apibench/huggingface_train.json\")\n",
    "len(hf_train_data), hf_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9b2af0-7faf-46c3-b3a9-91b60ee3ea52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911,\n",
       " {'code': '###Instruction: The user is interested in a tool to find relationships between medical terms.\\n###Output: <<<domain>>>: Multimodal Feature Extraction\\n<<<api_call>>>: AutoModel.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. We import the necessary classes from the transformers package provided by Hugging Face.\\n2. We then call the \"AutoModel.from_pretrained\" method with the argument \\'GanjinZero/UMLSBert_ENG\\' to load this pretrained model.\\n3. This model, which is particularly suitable for finding relationships between medical terms, can be used to convert medical terms into embeddings (dense vectors).\\n4. These embeddings can then be compared to find similarities and relationships between various medical terms.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\nmodel = AutoModel.from_pretrained(\\'GanjinZero/UMLSBert_ENG\\')\\n\\ninputs = tokenizer(medical_term, return_tensors=\"pt\")\\noutputs = model(**inputs)\\nembeddings = outputs.last_hidden_state\\n',\n",
       "  'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\",\n",
       "  'provider': 'Hugging Face Transformers',\n",
       "  'api_data': {'domain': 'Multimodal Feature Extraction',\n",
       "   'framework': 'Hugging Face Transformers',\n",
       "   'functionality': 'Feature Extraction',\n",
       "   'api_name': 'GanjinZero/UMLSBert_ENG',\n",
       "   'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\",\n",
       "   'api_arguments': [],\n",
       "   'python_environment_requirements': ['transformers'],\n",
       "   'example_code': '',\n",
       "   'performance': {'dataset': '', 'accuracy': ''},\n",
       "   'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_eval_data = load_jsonl_data(\"gorilla/data/apibench/huggingface_eval.json\")\n",
    "len(hf_eval_data), hf_eval_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c96dd-70f5-4037-9318-b41d0cbb27d4",
   "metadata": {},
   "source": [
    "# 1. Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22da9c82-b4fe-43aa-8116-996d2dbfcbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'instruction': '###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.',\n",
       "  'domain': 'Natural Language Processing Sentence Similarity',\n",
       "  'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
       "  'api_provider': 'Hugging Face Transformers',\n",
       "  'code': \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\"},\n",
       " '###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instruction: apibench - {lib}_train.json - code - instruction\n",
    "\n",
    "import re\n",
    "\n",
    "def get_code_parts_from_apibench_data(data):\n",
    "    text = data['code']\n",
    "    instruction, _ = text.split(\"\\n###Output\")\n",
    "    \n",
    "    # Extracting domain, api_call, api_provider, and code using regular expressions\n",
    "    domain_pattern = r'<<<domain>>>: (.+?)\\n'\n",
    "    api_call_pattern = r'<<<api_call>>>: (.+?)\\n'\n",
    "    api_provider_pattern = r'<<<api_provider>>>: (.+?)\\n'\n",
    "    code_pattern = r'<<<code>>>: (.+)'\n",
    "\n",
    "    domain = re.search(domain_pattern, text).group(1)\n",
    "    api_call = re.search(api_call_pattern, text).group(1)\n",
    "    api_provider = re.search(api_provider_pattern, text).group(1)\n",
    "    code = re.search(code_pattern, text, re.DOTALL).group(1).strip()\n",
    "\n",
    "    return {\n",
    "        'instruction': instruction, \n",
    "        'domain': domain, \n",
    "        'api_call': api_call, \n",
    "        'api_provider': api_provider, \n",
    "        'code': code\n",
    "    }\n",
    "\n",
    "d = hf_eval_data[0]\n",
    "code_parts = get_code_parts_from_apibench_data(d)\n",
    "code_parts, code_parts['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafb7907-b9ab-458b-ba10-5ffb814077d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Instruction: Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\n",
      "{'code': '###Instruction: Design a feature for a social media website to '\n",
      "         'recommend articles to users based on how similar the articles are to '\n",
      "         'their previously liked articles.\\n'\n",
      "         '###Output: <<<domain>>>: Natural Language Processing Sentence '\n",
      "         'Similarity\\n'\n",
      "         '<<<api_call>>>: '\n",
      "         \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\"\n",
      "         '<<<api_provider>>>: Hugging Face Transformers\\n'\n",
      "         '<<<explanation>>>:1. We first import the necessary classes and '\n",
      "         'modules from the transformers package. This includes AutoTokenizer '\n",
      "         'and AutoModel for loading the pre-trained models from Hugging Face.\\n'\n",
      "         '2. We use the AutoModel.from_pretrained() method to load the '\n",
      "         \"'princeton-nlp/unsup-simcse-roberta-base' model, which is specially \"\n",
      "         'designed for calculating sentence similarity.\\n'\n",
      "         '3. To build the recommendation feature, we process the text of '\n",
      "         'previously liked articles and compute sentence embeddings. For each '\n",
      "         'new article, we compute its sentence embedding and compare it to the '\n",
      "         'embeddings of previously liked articles.\\n'\n",
      "         \"4. If the similarity between the new article's embedding and any \"\n",
      "         \"previous liked articles' embeddings is above a certain threshold, \"\n",
      "         'the new article is recommended to the user.\\n'\n",
      "         '<<<code>>>: from transformers import AutoTokenizer, AutoModel\\n'\n",
      "         'tokenizer = '\n",
      "         \"AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\"\n",
      "         'model = '\n",
      "         \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\n\",\n",
      " 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
      " 'provider': 'Hugging Face Transformers',\n",
      " 'api_data': {'domain': 'Natural Language Processing Sentence Similarity',\n",
      "              'framework': 'Hugging Face Transformers',\n",
      "              'functionality': 'Feature Extraction',\n",
      "              'api_name': 'princeton-nlp/unsup-simcse-roberta-base',\n",
      "              'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\",\n",
      "              'api_arguments': None,\n",
      "              'python_environment_requirements': ['transformers'],\n",
      "              'example_code': None,\n",
      "              'performance': {'dataset': None, 'accuracy': None},\n",
      "              'description': 'An unsupervised sentence embedding model trained '\n",
      "                             'using the SimCSE approach with a Roberta base '\n",
      "                             'architecture.'}}\n"
     ]
    }
   ],
   "source": [
    "for d in hf_eval_data:\n",
    "    code_parts = get_code_parts_from_apibench_data(d)\n",
    "    print(code_parts['instruction'])\n",
    "    pprint.pp(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba423e2f-c909-435f-9326-8ec77ed554c5",
   "metadata": {},
   "source": [
    "# 2. Function / Test Function\n",
    "- code part -> gpt -> function\n",
    "- dataset 问题，先通过 prompt 解决一部分，需要对应到 huggingface dataset 名称才能对应\n",
    "- prompt:\n",
    "    generate following code based on above infomation:\n",
    "    1. function with：\n",
    "    - detailed comments\n",
    "    - function description\n",
    "    2. test function with：\n",
    "    - test dataset\n",
    "    - using assert in test function\n",
    "    - do not compare number strictly\n",
    "    - if dataset is provided in performance - dataset, load the dataset, then select several sample from the dataset, otherwise, using online source, do not leave blank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ef38a3-000e-405a-8494-54a090a39d33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: langchain in /root/miniconda3/lib/python3.8/site-packages (0.0.333)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.62 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (0.0.63)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: anyio<4.0 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (3.6.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: requests<3,>=2 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (2.25.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /root/miniconda3/lib/python3.8/site-packages (from langchain) (1.22.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.8/site-packages (from anyio<4.0->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.8/site-packages (from anyio<4.0->langchain) (2.10)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/miniconda3/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/miniconda3/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/miniconda3/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /root/miniconda3/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/miniconda3/lib/python3.8/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /root/miniconda3/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /root/miniconda3/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2->langchain) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2021.5.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/miniconda3/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/miniconda3/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d10756ea-36a6-40dc-a9cd-e15ab883976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    "    create_openai_fn_runnable,\n",
    "    create_structured_output_runnable,\n",
    ")\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def get_chat_model():\n",
    "    BASE_URL = \"https://autoagents-ca-east.openai.azure.com/\"\n",
    "    API_KEY = \"2864ce19a46540b2a0943df607ca6225\"\n",
    "    model = AzureChatOpenAI(\n",
    "        temperature=0.0,\n",
    "        openai_api_base=BASE_URL,\n",
    "        openai_api_version=\"2023-08-01-preview\",\n",
    "        deployment_name=\"gpt-4-32k\",\n",
    "        openai_api_key=API_KEY,\n",
    "        openai_api_type=\"azure\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# def get_chat_model():\n",
    "#     BASE_URL = \"https://autoagents-global.openai.azure.com\"\n",
    "#     API_KEY = \"6c1c61bd992146a1bbcde4a80fef51ba\"\n",
    "#     model = AzureChatOpenAI(\n",
    "#         temperature=0.0,\n",
    "#         openai_api_base=BASE_URL,\n",
    "#         openai_api_version=\"2023-08-01-preview\",\n",
    "#         deployment_name=\"gpt-35-turbo-16k\",\n",
    "#         openai_api_key=API_KEY,\n",
    "#         openai_api_type=\"azure\",\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "######################################################################\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class CodeResp(BaseModel):\n",
    "    \"\"\"\n",
    "    generate function_code and test_function_code based on input.\n",
    "    \n",
    "    function comments should follow Google Python Style Guide, includes args, returns, and raises\n",
    "    \"\"\"\n",
    "    function_name: str = Field(..., description=\"function name\")\n",
    "    function_import: str = Field(..., description=\"import nessary lib before function code\")\n",
    "    function_code: str = Field(..., description=\"standalone function with:\\n- detailed comments\\n- function description\")\n",
    "    test_function_code: str = Field(..., description=\"standalone test function\")\n",
    "    call_test_function_code: str = Field(..., description=\"test function call in the end of test function file\")\n",
    "    function_import_fixed: str = Field(..., description=\"import nessary lib before function code, fixed by checking the function, e.g. 'import torch' or 'import numpy as np' if needed\")\n",
    "    requirements_file: str =  Field(..., description=\"dependency packages needed to install, usage: pip install -r requirements.txt, do not specific the version\")\n",
    "    \n",
    "######################################################################\n",
    "from langchain.chains.openai_functions import (\n",
    "    convert_to_openai_function,\n",
    "    get_openai_output_parser,\n",
    ")\n",
    "    \n",
    "def get_function_from_data(data, err):\n",
    "    code_desc = \"\"\"\n",
    "    generate function_code and test_function_code based on input.\n",
    "    \n",
    "    function comment can be descripted as following\n",
    "    \n",
    "    Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function’s name and signature are informative enough that it can be aptly described using a one-line docstring.\n",
    "\n",
    "    Args:\n",
    "        List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar.\n",
    "    Returns: (or Yields: for generators)\n",
    "        Describe the semantics of the return value, including any type information that the type annotation does not provide. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \\\"\\\"\\\"Returns row from Bigtable as a tuple of strings.\\\"\\\"\\\") and the opening sentence is sufficient to describe the return value. Do not imitate older ‘NumPy style’ (example), which frequently documented a tuple return value as if it were multiple return values with individual names (never mentioning the tuple). Instead, describe such a return value as: “Returns: A tuple (mat_a, mat_b), where mat_a is …, and …”. The auxiliary names in the docstring need not necessarily correspond to any internal names used in the function body (as those are not part of the API).\n",
    "    Raises:\n",
    "        List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API).\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a world class algorithm for recording entities.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\"\"\n",
    "basic api info:\n",
    "{input}\n",
    "\n",
    "consider potential err:\n",
    "{err}\n",
    "\n",
    "function comments should follow:\n",
    "{code_desc}\n",
    "\n",
    "generate following code based on above information:\n",
    "1. function with:\n",
    "    - detailed comments\n",
    "    - function description\n",
    "    - not using \" in file\n",
    "2. test function with:\n",
    "    - 3-5 test cases\n",
    "        - for multi-media task: try to load official dataset, or using online resource url\n",
    "        - e.g. https://placekitten.com/200/300\n",
    "    - using assert in test function\n",
    "    - do not compare number strictly\n",
    "    - return 'All Tests Passed' in final if every assertion passed\n",
    "    - do not use fake file url in test case\n",
    "    - not using \" in file\n",
    "3. requirements_file with no specific package version\n",
    "\"\"\",\n",
    "            ),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    llm = get_chat_model()\n",
    "    runnable = create_openai_fn_runnable([CodeResp], llm, prompt)\n",
    "    \n",
    "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
    "    return resp\n",
    "\n",
    "import glob\n",
    "\n",
    "def get_v2_err(base_dir, idx):\n",
    "    formatted_number = str(idx).zfill(5)\n",
    "    matching_files = glob.glob(f\"{base_dir}/f{formatted_number}_*.err\")\n",
    "    for file_path in matching_files:\n",
    "        lines = []\n",
    "        with open(file_path) as f:\n",
    "            for idx, l in enumerate(f):\n",
    "                if 'Downloading' in l:\n",
    "                    continue\n",
    "                lines.append(l)\n",
    "                \n",
    "            return \"\".join(lines[-50:])\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "693f2e88-a1dc-408a-b0e2-dd21deb15988",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': \"###Instruction: Create a program to generate a description for an image provided as input.\\n###Output: <<<domain>>>: Multimodal Image-to-Text\\n<<<api_call>>>: pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>:1. Import the pipeline function from the transformers library provided by Hugging Face.\\n2. Use the pipeline function to create a text generation model.\\n3. Specify the model 'microsoft/git-large-r-textcaps' to be loaded. This model has been fine-tuned on the TextCaps dataset and is capable of generating image descriptions based on the content of the image.\\n4. The created model can be used to generate a description for a given input image by simply passing the image into the pipeline's generate method.\\n<<<code>>>: from transformers import pipeline\\ndescription_generator = pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\nimage_description = description_generator(image)\\n\", 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\n",
      "Traceback (most recent call last):\n",
      "  File \"output/hf-eval-data-v2/f00007_generate_image_description.py\", line 36, in <module>\n",
      "    test_generate_image_description()\n",
      "  File \"output/hf-eval-data-v2/f00007_generate_image_description.py\", line 28, in test_generate_image_description\n",
      "    test_image = Image.open('test_image.jpg')\n",
      "NameError: name 'Image' is not defined\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CodeResp(function_name='generate_image_description', function_import='from transformers import pipeline\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO', function_code='def generate_image_description(image_url: str) -> str:\\n    \"\"\"\\n    Generate a description for an image provided as input.\\n\\n    Args:\\n        image_url (str): The URL of the image to be described.\\n\\n    Returns:\\n        str: The generated description of the image.\\n\\n    Raises:\\n        Exception: If the image cannot be loaded from the provided URL.\\n    \"\"\"\\n    try:\\n        response = requests.get(image_url)\\n        image = Image.open(BytesIO(response.content))\\n        description_generator = pipeline(\\'text-generation\\', model=\\'microsoft/git-large-r-textcaps\\')\\n        image_description = description_generator(image)\\n        return image_description\\n    except Exception as e:\\n        raise Exception(\\'Failed to load image from URL: \\' + str(e))', test_function_code='def test_generate_image_description():\\n    \"\"\"\\n    Test the generate_image_description function.\\n    \"\"\"\\n    # Test case 1: A kitten image\\n    image_url = \\'https://placekitten.com/200/300\\'\\n    description = generate_image_description(image_url)\\n    assert isinstance(description, str), \\'The description should be a string.\\'\\n\\n    # Test case 2: A puppy image\\n    image_url = \\'https://images.dog.ceo/breeds/hound-afghan/n02088094_1003.jpg\\'\\n    description = generate_image_description(image_url)\\n    assert isinstance(description, str), \\'The description should be a string.\\'\\n\\n    # Test case 3: An invalid image URL\\n    image_url = \\'https://invalid.url\\'\\n    try:\\n        description = generate_image_description(image_url)\\n    except Exception as e:\\n        assert str(e) == \\'Failed to load image from URL: Invalid URL: \\' + image_url,\\n        \\'The function should raise an exception for invalid image URLs.\\'\\n\\n    return \\'All Tests Passed\\'', call_test_function_code='test_generate_image_description()', function_import_fixed='from transformers import pipeline\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO', requirements_file='transformers\\nPillow\\nrequests')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 6\n",
    "print(hf_eval_data[idx])\n",
    "\n",
    "err = get_v2_err(\"output/hf-eval-data-v2\", idx + 1)\n",
    "print(err)\n",
    "\n",
    "resp = get_function_from_data(hf_eval_data[idx], err)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a74ddb-8c5f-4d8e-ba61-5294292d0e2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': \"###Instruction: Create a program to generate a description for an image provided as input.\\n###Output: <<<domain>>>: Multimodal Image-to-Text\\n<<<api_call>>>: pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>:1. Import the pipeline function from the transformers library provided by Hugging Face.\\n2. Use the pipeline function to create a text generation model.\\n3. Specify the model 'microsoft/git-large-r-textcaps' to be loaded. This model has been fine-tuned on the TextCaps dataset and is capable of generating image descriptions based on the content of the image.\\n4. The created model can be used to generate a description for a given input image by simply passing the image into the pipeline's generate method.\\n<<<code>>>: from transformers import pipeline\\ndescription_generator = pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\nimage_description = description_generator(image)\\n\", 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\n",
      "----------------------\n",
      "transformers\n",
      "Pillow\n",
      "requests\n",
      "----------------------\n",
      "from transformers import pipeline\n",
      "from PIL import Image\n",
      "import requests\n",
      "from io import BytesIO\n",
      "----------------------\n",
      "from transformers import pipeline\n",
      "from PIL import Image\n",
      "import requests\n",
      "from io import BytesIO\n",
      "----------------------\n",
      "def generate_image_description(image_url: str) -> str:\n",
      "    \"\"\"\n",
      "    Generate a description for an image provided as input.\n",
      "\n",
      "    Args:\n",
      "        image_url (str): The URL of the image to be described.\n",
      "\n",
      "    Returns:\n",
      "        str: The generated description of the image.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If the image cannot be loaded from the provided URL.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        response = requests.get(image_url)\n",
      "        image = Image.open(BytesIO(response.content))\n",
      "        description_generator = pipeline('text-generation', model='microsoft/git-large-r-textcaps')\n",
      "        image_description = description_generator(image)\n",
      "        return image_description\n",
      "    except Exception as e:\n",
      "        raise Exception('Failed to load image from URL: ' + str(e))\n",
      "----------------------\n",
      "def test_generate_image_description():\n",
      "    \"\"\"\n",
      "    Test the generate_image_description function.\n",
      "    \"\"\"\n",
      "    # Test case 1: A kitten image\n",
      "    image_url = 'https://placekitten.com/200/300'\n",
      "    description = generate_image_description(image_url)\n",
      "    assert isinstance(description, str), 'The description should be a string.'\n",
      "\n",
      "    # Test case 2: A puppy image\n",
      "    image_url = 'https://images.dog.ceo/breeds/hound-afghan/n02088094_1003.jpg'\n",
      "    description = generate_image_description(image_url)\n",
      "    assert isinstance(description, str), 'The description should be a string.'\n",
      "\n",
      "    # Test case 3: An invalid image URL\n",
      "    image_url = 'https://invalid.url'\n",
      "    try:\n",
      "        description = generate_image_description(image_url)\n",
      "    except Exception as e:\n",
      "        assert str(e) == 'Failed to load image from URL: Invalid URL: ' + image_url,\n",
      "        'The function should raise an exception for invalid image URLs.'\n",
      "\n",
      "    return 'All Tests Passed'\n",
      "----------------------\n",
      "test_generate_image_description()\n"
     ]
    }
   ],
   "source": [
    "print(hf_eval_data[idx])\n",
    "print(\"----------------------\")\n",
    "print(resp.requirements_file)\n",
    "print(\"----------------------\")\n",
    "print(resp.function_import)\n",
    "print(\"----------------------\")\n",
    "print(resp.function_import_fixed)\n",
    "print(\"----------------------\")\n",
    "print(resp.function_code)\n",
    "print(\"----------------------\")\n",
    "print(resp.test_function_code)\n",
    "print(\"----------------------\")\n",
    "print(resp.call_test_function_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d359825-bf96-4f56-bdb7-55c8deb4f64c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1...skip...2...skip...3...skip...4...skip...5...skip...6...skip...7...skip...8...9...skip...10...skip...11...skip...12...13...14...skip...15...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....16...skip...17...skip...18..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 222 (char 1388)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 222 (char 1388) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"extract_student_info\",\n",
      "\"function_import\": \"from transformers import LayoutLMv3ForQuestionAnswering, LayoutLMv3Tokenizer\\nfrom PIL import Image\",\n",
      "\"function_code\": \"def extract_student_info(image_path: str, question: str) -> str:\\n    \\\"\\\"\\\"\\n    Extracts information from student enrollment forms using LayoutLMv3ForQuestionAnswering model.\\n\\n    Args:\\n        image_path (str): The path to the image file of the enrollment form.\\n        question (str): The question to be answered by the model.\\n\\n    Returns:\\n        str: The answer to the question provided.\\n    \\\"\\\"\\\"\\n    # Load the pre-trained model\\n    model = LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\\n    tokenizer = LayoutLMv3Tokenizer.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\\n\\n    # Load and preprocess the enrollment form image\\n    form_image = Image.open(image_path)\\n\\n    # Tokenize the text\\n    inputs = tokenizer(question, form_image, return_tensors='pt')\\n\\n    # Ask questions and get the answer\\n    outputs = model(**inputs)\\n\\n    return outputs\",\n",
      "\"test_function_code\": \"def test_extract_student_info():\\n    \\\"\\\"\\\"\\n    Tests the function extract_student_info.\\n    \\\"\\\"\\\"\\n    # Test case 1\\n    image_path = 'path/to/test/image1'\\n    question = 'What is the student\\'s name?'\\n    assert isinstance(extract_student_info(image_path, question), str)\\n\\n    # Test case 2\\n    image_path = 'path/to/test/image2'\\n    question = 'What is the student\\'s age?'\\n    assert isinstance(extract_student_info(image_path, question), str)\\n\\n    # Test case 3\\n    image_path = 'path/to/test/image3'\\n    question = 'What is the student\\'s address?'\\n    assert isinstance(extract_student_info(image_path, question), str)\\n\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_extract_student_info()\",\n",
      "\"function_import_fixed\": \"from transformers import LayoutLMv3ForQuestionAnswering, LayoutLMv3Tokenizer\\nfrom PIL import Image\",\n",
      "\"requirements_file\": \"transformers\\nPIL\"\n",
      "}; pos=1388; lineno=5; colno=222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19...skip...20...21...22...skip...23...skip...24...25...26...skip...27...skip...28...29...skip...30...31...skip...32...skip...33...34...35...skip...36...37...skip...38...39...skip...40...41...skip...42...skip...43...44...45...46...47...48...49...50...51...52...53...54...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....55..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 296 (char 909)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 296 (char 909) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"translate_english_to_french\",\n",
      "  \"function_import\": \"from transformers import pipeline\",\n",
      "  \"function_code\": \"def translate_english_to_french(input_text):\\n    \\\"\\\"\\\"\\n    Translate English text to French using the Helsinki-NLP/opus-mt-en-fr model.\\n\\n    Args:\\n        input_text (str): The text in English to be translated to French.\\n\\n    Returns:\\n        str: The translated text in French.\\n    \\\"\\\"\\\"\\n    translate = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\\n    translated_text = translate(input_text)\\n    return translated_text[0]['translation_text']\",\n",
      "  \"test_function_code\": \"def test_translate_english_to_french():\\n    \\\"\\\"\\\"\\n    Test the function translate_english_to_french.\\n    \\\"\\\"\\\"\\n    assert translate_english_to_french('Hello, how are you?') == 'Bonjour, comment ça va?'\\n    assert translate_english_to_french('I love you') == 'Je t\\'aime'\\n    assert translate_english_to_french('Good morning') == 'Bonjour'\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_translate_english_to_french()\",\n",
      "  \"function_import_fixed\": \"from transformers import pipeline\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=909; lineno=5; colno=296)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56...57...58...59...60...61...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....62..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 763 (char 1804)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 763 (char 1804) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"summarize_text\",\n",
      "  \"function_import\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\",\n",
      "  \"function_code\": \"def summarize_text(article_text: str, model_name: str = 'csebuetnlp/mT5_multilingual_XLSum') -> str:\\n    \\\"\\\"\\\"\\n    Summarize a given text using a pre-trained model.\\n\\n    Args:\\n        article_text (str): The text to be summarized.\\n        model_name (str, optional): The name of the pre-trained model to use. Defaults to 'csebuetnlp/mT5_multilingual_XLSum'.\\n\\n    Returns:\\n        str: The summarized text.\\n    \\\"\\\"\\\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\n\\n    input_ids = tokenizer.encode(article_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\\n    output_ids = model.generate(input_ids, max_length=84, no_repeat_ngram_size=2, num_beams=4)\\n    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\n\\n    return summary\",\n",
      "  \"test_function_code\": \"def test_summarize_text():\\n    \\\"\\\"\\\"\\n    Test the summarize_text function.\\n    \\\"\\\"\\\"\\n    article_text = 'Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites.'\\n    summary = summarize_text(article_text)\\n    assert isinstance(summary, str), 'The output should be a string.'\\n    assert len(summary) > 0, 'The output should not be empty.'\\n\\n    article_text = 'In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.'\\n    summary = summarize_text(article_text)\\n    assert isinstance(summary, str), 'The output should be a string.'\\n    assert len(summary) > 0, 'The output should not be empty.'\\n\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"print(test_summarize_text())\",\n",
      "  \"function_import_fixed\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=1804; lineno=5; colno=763)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63...64...65...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....66..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 168 (char 741)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 168 (char 741) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"predict_missing_text\",\n",
      "\"function_import\": \"from transformers import pipeline\",\n",
      "\"function_code\": \"def predict_missing_text(sentence: str) -> str:\\n    \\\"\\\"\\\"\\n    Predicts the most plausible missing text in a given sentence.\\n\\n    Args:\\n        sentence (str): The sentence with missing text, represented by [MASK].\\n\\n    Returns:\\n        str: The sentence with the missing text filled in.\\n    \\\"\\\"\\\"\\n    unmasker = pipeline('fill-mask', model='albert-base-v2')\\n    filled_sentence = unmasker(sentence)\\n    return filled_sentence[0]['sequence']\",\n",
      "\"test_function_code\": \"def test_predict_missing_text():\\n    \\\"\\\"\\\"\\n    Tests the predict_missing_text function.\\n    \\\"\\\"\\\"\\n    assert predict_missing_text('Hello I\\'m a [MASK] model.') == 'Hello I\\'m a language model.'\\n    assert predict_missing_text('The weather is [MASK].') != ''\\n    assert predict_missing_text('I love to [MASK] in the park.') != ''\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_predict_missing_text()\",\n",
      "\"function_import_fixed\": \"from transformers import pipeline\",\n",
      "\"requirements_file\": \"transformers\"\n",
      "}; pos=741; lineno=5; colno=168)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67...68...69...70...71...72...73...74...75...76...77...78...79...80...81...82...83...84...85...86...87...88...89...90...91...92...93...94...95...96...97...98...99...100...101...102...103...104...105...106...107...108...109...110...111...112...113...114...115...116...117...118...119...120...121...122...123...124...125...126...127...128...129...130...Retrying... (Attempt 1/3)...131...132...133...134...135...136...137...138...139...140...141...142...143...144...145...146...147...148...149...150...151...152...153...154...155...156...157...158...159...160...161...162...163...164...165...166...167...168...169...170...171...172...173...174...175...176...177...178...179...Retrying... (Attempt 1/3)...180...181...182...183...184...185...186...187...188...189...190...191...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...192...193...194...195...196...197...198...199...200...201...202...203...204...205...206...207...208...209...210...211...212...213...214...215...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...216...217...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...218...219...220...221...222...223...224...225...226...227...228...229...230...231...232...233...234...235...236...237...238...239...240...241...242...243...244...245...246...247...248...249...250...251...252...253...254...255...256...257...258...259...260...261...262...263...264...265...266...267...268...269...270...271...272...273...274...275...276...277...278...279...280...281...282...283...284...285...286...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....287..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 365 (char 1319)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 365 (char 1319) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"classify_image\",\n",
      "  \"function_import\": \"from transformers import pipeline\",\n",
      "  \"function_code\": \"def classify_image(image_path: str, class_names: str = 'cat, dog, bird') -> dict:\\n    \\\"\\\"\\\"\\n    Classify an image into one of the given classes using a pre-trained model.\\n\\n    Args:\\n        image_path (str): The path to the image to be classified.\\n        class_names (str): A string of class names separated by commas. Default is 'cat, dog, bird'.\\n\\n    Returns:\\n        dict: The predicted class and the confidence score.\\n\\n    Raises:\\n        OSError: If the model or the image file does not exist.\\n    \\\"\\\"\\\"\\n    try:\\n        model = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\n        results = model(image_path, class_names=class_names)\\n        return results\\n    except Exception as e:\\n        raise OSError('Model or image file does not exist.') from e\",\n",
      "  \"test_function_code\": \"def test_classify_image():\\n    \\\"\\\"\\\"Test the classify_image function.\\\"\\\"\\\"\\n    # Test with a cat image\\n    cat_image = 'https://placekitten.com/200/300'\\n    result = classify_image(cat_image)\\n    assert isinstance(result, dict), 'The result should be a dictionary.'\\n    assert 'cat' in result, 'The result should contain the class \\'cat\\'.\\n\\n    # Test with a dog image\\n    dog_image = 'https://placedog.net/500'\\n    result = classify_image(dog_image)\\n    assert isinstance(result, dict), 'The result should be a dictionary.'\\n    assert 'dog' in result, 'The result should contain the class \\'dog\\'.\\n\\n    # Test with a bird image\\n    bird_image = 'https://placebird.com/500/500'\\n    result = classify_image(bird_image)\\n    assert isinstance(result, dict), 'The result should be a dictionary.'\\n    assert 'bird' in result, 'The result should contain the class \\'bird\\'.\\n\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_classify_image()\",\n",
      "  \"function_import_fixed\": \"from transformers import pipeline\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=1319; lineno=5; colno=365)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288...289...290...291...292...293...294...295...296...297...298...299...300...301...302...303...304...305...306...307...308...309...310...311...312...313...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...314...315...316...317...318...319...320...321...322...323...324...325...326...327...Retrying... (Attempt 1/3)...328...329...330...331...332...333...334...335...336...337...338...339...340...341...342...343...344...345...346...347...348...349...350...351...352...Retrying... (Attempt 1/3)...353...354...355...356...357...358...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....359..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 207 (char 916)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 207 (char 916) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"document_question_answer\",\n",
      "\"function_import\": \"from transformers import pipeline\",\n",
      "\"function_code\": \"def document_question_answer(document: str, question: str) -> str:\\n    '''\\n    This function uses a pre-trained model to answer a question based on a given document.\\n\\n    Args:\\n        document (str): The document content that the question is based on.\\n        question (str): The question to be answered.\\n\\n    Returns:\\n        str: The answer to the question.\\n    '''\\n    qa_model = pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\\n    answer = qa_model({'question': question, 'context': document})['answer']\\n    return answer\",\n",
      "\"test_function_code\": \"def test_document_question_answer():\\n    '''\\n    This function tests the document_question_answer function.\\n    '''\\n    document = 'Our company policy restricts the loan applicant\\'s eligibility to the citizens of United States. The applicant needs to be 18 years old or above and their monthly salary should at least be $4,000. FetchTypeOfYear: 2019.'\\n    question = 'Can anyone with a monthly salary of $3,000 apply?'\\n    assert document_question_answer(document, question) == 'No'\\n    document = 'Our company policy allows the loan applicant\\'s eligibility to the citizens of United States. The applicant needs to be 18 years old or above and their monthly salary should at least be $3,000. FetchTypeOfYear: 2019.'\\n    question = 'Can anyone with a monthly salary of $3,000 apply?'\\n    assert document_question_answer(document, question) == 'Yes'\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_document_question_answer()\",\n",
      "\"function_import_fixed\": \"from transformers import pipeline\",\n",
      "\"requirements_file\": \"transformers\\ntorch\\ndatasets\\ntokenizers\"\n",
      "}; pos=916; lineno=5; colno=207)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360...361...362...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....363..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 398 (char 1103)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 398 (char 1103) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"detect_table_structure\",\n",
      "\"function_import\": \"from transformers import pipeline\\nimport PIL.Image\",\n",
      "\"function_code\": \"def detect_table_structure(table_image_path):\\n    \\\"\\\"\\\"\\n    Detects the structure of a table in a given image using the Hugging Face Transformers library.\\n\\n    Args:\\n        table_image_path (str): The path to the image file containing the table.\\n\\n    Returns:\\n        dict: The detected table structure.\\n    \\\"\\\"\\\"\\n    table_detector = pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\\n    table_image = PIL.Image.open(table_image_path)\\n    table_structure = table_detector(table_image)\\n    return table_structure\",\n",
      "\"test_function_code\": \"def test_detect_table_structure():\\n    \\\"\\\"\\\"\\n    Tests the detect_table_structure function with a sample table image.\\n    \\\"\\\"\\\"\\n    table_structure = detect_table_structure('sample_table_image.jpg')\\n    assert isinstance(table_structure, dict), 'The function should return a dictionary.'\\n    assert 'boxes' in table_structure, 'The dictionary should contain the key \\'boxes\\'.'\\n    assert 'labels' in table_structure, 'The dictionary should contain the key \\'labels\\'.'\\n    assert 'scores' in table_structure, 'The dictionary should contain the key \\'scores\\'.'\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_detect_table_structure()\",\n",
      "\"function_import_fixed\": \"from transformers import pipeline\\nimport PIL.Image\",\n",
      "\"requirements_file\": \"transformers\\nPIL\"\n",
      "}; pos=1103; lineno=5; colno=398)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364...365...366...367...368...369...370...371...372...373...374...375...376...377...378...379...380...381...382...383...384...385...386...387...388...389...390...391...392...393...394...395...396...397...398...399...400...401...402...403...404...405...406...407...408...409...410...411...412...413...414...415...416...417...418...419...420...421...422...423...424...Retrying... (Attempt 1/3)...425...426...427...428...429...430...431...432...433...434...435...436...437...438...439...440...441...442...443...444...445...446...447...448...449...450...451...452...453...454...455...456...457...458...459...460...461...462...463...464...465...466...467...468...469...470...471...472...473...474...475...476...477...478...479...480...481...482...483...484...485...486...487...488...489...490...491...492...493...494...495...496...497...498...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...499...500...501...502...503...504...505...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....506..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 147 (char 1213)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 147 (char 1213) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"summarize_french_news\",\n",
      "  \"function_import\": \"from transformers import BarthezTokenizer, BartForConditionalGeneration\",\n",
      "  \"function_code\": \"def summarize_french_news(news_article_french):\\n    \\\"\\\"\\\"\\n    Summarize a French news article using a pre-trained model.\\n\\n    Args:\\n        news_article_french (str): The French news article to be summarized.\\n\\n    Returns:\\n        str: The summary of the news article.\\n\\n    Raises:\\n        ValueError: If the input is not a string.\\n    \\\"\\\"\\\"\\n    if not isinstance(news_article_french, str):\\n        raise ValueError('Input must be a string.')\\n\\n    tokenizer = BarthezTokenizer.from_pretrained('moussaKam/barthez-orangesum-abstract')\\n    model = BartForConditionalGeneration.from_pretrained('moussaKam/barthez-orangesum-abstract')\\n\\n    inputs = tokenizer(news_article_french, return_tensors='pt', max_length=512, truncation=True)\\n    outputs = model.generate(input_ids=inputs['input_ids'])\\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n    return summary\",\n",
      "  \"test_function_code\": \"def test_summarize_french_news():\\n    \\\"\\\"\\\"Test the summarize_french_news function.\\\"\\\"\\\"\\n    news_article_french = 'L\\'article de presse en français ici...'\\n    summary = summarize_french_news(news_article_french)\\n    assert isinstance(summary, str), 'The output must be a string.'\\n\\n    news_article_french = 'Un autre article de presse en français ici...'\\n    summary = summarize_french_news(news_article_french)\\n    assert isinstance(summary, str), 'The output must be a string.'\\n\\n    try:\\n        summarize_french_news(123)\\n    except ValueError as e:\\n        assert str(e) == 'Input must be a string.', 'The function must raise a ValueError when the input is not a string.'\\n\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_summarize_french_news()\",\n",
      "  \"function_import_fixed\": \"from transformers import BarthezTokenizer, BartForConditionalGeneration\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=1213; lineno=5; colno=147)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507...508...509...510...511...512...513...514...Retrying... (Attempt 1/3)...515...516...517...518...519...520...521...522...523...524...525...526...527...528...529...530...531...532...533...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...534...535...536...537...538...539...540...541...542...543...544...545...546...547...548...549...550...551...552...553...554...555...556...557...558...559...560...561...562...563...564...565...566...567...568...569...570...571...572...573...574...575...576...577...578...579...580...581...582...583...584...585...586...587...588...589...590...591...592...593...594...595...596...597...598...599...600...601...602...603...604...605...606...607...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....608..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 202 (char 821)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 202 (char 821) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"fill_mask_with_bert\",\n",
      "\"function_import\": \"from transformers import pipeline\",\n",
      "\"function_code\": \"def fill_mask_with_bert(input_text: str) -> str:\\n    '''\\n    This function uses the pretrained 'bert-large-cased' model to fill the masked position in the input sentence.\\n\\n    Args:\\n        input_text (str): The input text with a mask token '[MASK]' to be filled by the model.\\n\\n    Returns:\\n        str: The completed sentence.\\n    '''\\n    unmasker = pipeline('fill-mask', model='bert-large-cased')\\n    completed_sentence = unmasker(input_text)\\n    return completed_sentence[0]['sequence']\",\n",
      "\"test_function_code\": \"def test_fill_mask_with_bert():\\n    '''\\n    This function tests the 'fill_mask_with_bert' function with different test cases.\\n    '''\\n    assert fill_mask_with_bert('Hello, I\\'m a [MASK]...') == 'Hello, I\\'m a man...'\\n    assert fill_mask_with_bert('The sky is [MASK].') == 'The sky is blue.'\\n    assert fill_mask_with_bert('I love to [MASK].') == 'I love to dance.'\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_fill_mask_with_bert()\",\n",
      "\"function_import_fixed\": \"from transformers import pipeline\",\n",
      "\"requirements_file\": \"transformers\"\n",
      "}; pos=821; lineno=5; colno=202)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609...610...611...612...613...614...615...616...617...618...619...620...621...622...623...624...625...626...627...628...Retrying... (Attempt 1/3)...629...630...631...632...633...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....634..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 209 (char 887)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 209 (char 887) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"generate_marketing_message\",\n",
      "  \"function_import\": \"from transformers import BartTokenizer, BartModel\",\n",
      "  \"function_code\": \"def generate_marketing_message(input_text):\\n    \\\"\\\"\\\"\\n    Generate a marketing message using the pre-trained BART model.\\n\\n    Args:\\n        input_text (str): The input text to generate a marketing message from.\\n\\n    Returns:\\n        str: The generated marketing message.\\n    \\\"\\\"\\\"\\n    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\n    model = BartModel.from_pretrained('facebook/bart-large')\\n    inputs = tokenizer(input_text, return_tensors='pt')\\n    outputs = model(**inputs)\\n    return outputs\",\n",
      "  \"test_function_code\": \"def test_generate_marketing_message():\\n    \\\"\\\"\\\"\\n    Test the generate_marketing_message function.\\n    \\\"\\\"\\\"\\n    assert isinstance(generate_marketing_message('Promote our client\\'s product using creative marketing messages.'), str)\\n    assert isinstance(generate_marketing_message('Our client has a new product that needs promotion.'), str)\\n    assert isinstance(generate_marketing_message('We need to create engaging content for our client\\'s product.'), str)\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_generate_marketing_message()\",\n",
      "  \"function_import_fixed\": \"from transformers import BartTokenizer, BartModel\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=887; lineno=5; colno=209)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635...Retrying... (Attempt 1/3)...636...637...638...639...640...641...642...643...644...645...646...647...648...649...650...651...652...653...Retrying... (Attempt 1/3)...654...655...656...657...658...659...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...660...661...662...663...664...665...666...667...668...669...670...671...672...673...674...675...676...677...678...679...680...681...682...683...684...685...686...687...688...689...690...691...692...693...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...694...695...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....696..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 171 (char 800)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 171 (char 800) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"summarize_conversation\",\n",
      "  \"function_import\": \"from transformers import pipeline\",\n",
      "  \"function_code\": \"def summarize_conversation(conversation: str) -> str:\\n    \\\"\\\"\\\"\\n    Summarizes a conversation using the 'philschmid/distilbart-cnn-12-6-samsum' model from Hugging Face Transformers.\\n\\n    Args:\\n        conversation (str): The conversation to be summarized.\\n\\n    Returns:\\n        str: The summarized conversation.\\n    \\\"\\\"\\\"\\n    summarizer = pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\n    summary = summarizer(conversation)\\n    return summary[0]['summary_text']\",\n",
      "  \"test_function_code\": \"def test_summarize_conversation():\\n    \\\"\\\"\\\"\\n    Tests the 'summarize_conversation' function.\\n    \\\"\\\"\\\"\\n    conversation1 = 'Anna: In today\\'s meeting, we discussed increasing marketing budget. Tom: I suggested allocating more funds to social media campaigns. Sarah: I proposed focusing on improving SEO. Anna: We agreed on investing in content creation, too. Tom: The team will revise the strategy and present it next week. Sarah: Let\\'s determine new KPIs for evaluating our progress.'\\n    assert len(summarize_conversation(conversation1)) > 0\\n\\n    conversation2 = 'Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? Philipp: Sure you can use the new Hugging Face Deep Learning Container. Jeff: ok. Jeff: and how can I get started? Jeff: where can I find documentation? Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face'\\n    assert len(summarize_conversation(conversation2)) > 0\\n\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_summarize_conversation()\",\n",
      "  \"function_import_fixed\": \"from transformers import pipeline\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=800; lineno=5; colno=171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697...698...699...700...701...702...703...704...705...706...707...708...709...710...711...712...713...714...715...716...717...718...719...720...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....721..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 402 (char 1251)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 402 (char 1251) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"voice_activity_detection\",\n",
      "\"function_import\": \"from pyannote.audio.core.inference import Inference\",\n",
      "\"function_code\": \"def voice_activity_detection(audio_file: str, device: str = 'cuda') -> dict:\\n    \\\"\\\"\\\"\\n    This function uses the Hugging Face Transformers model 'julien-c/voice-activity-detection' to detect voice activity in an audio file.\\n    It separates the segments where there is voice activity from the silent parts.\\n\\n    Args:\\n        audio_file (str): The path to the audio file.\\n        device (str, optional): The device to run the inference on. Defaults to 'cuda'.\\n\\n    Returns:\\n        dict: The result of the voice activity detection.\\n    \\\"\\\"\\\"\\n    model = Inference('julien-c/voice-activity-detection', device=device)\\n    result = model({\\n        'audio': audio_file\\n    })\\n    return result\",\n",
      "\"test_function_code\": \"def test_voice_activity_detection():\\n    \\\"\\\"\\\"\\n    This function tests the voice_activity_detection function.\\n    \\\"\\\"\\\"\\n    # Test case: audio file with voice activity\\n    result = voice_activity_detection('TheBigBangTheory.wav')\\n    assert isinstance(result, dict), 'The result should be a dictionary.'\\n    assert 'audio' in result, 'The result should contain the key \\'audio\\'.'\\n\\n    # Test case: audio file without voice activity\\n    result = voice_activity_detection('SilentAudio.wav')\\n    assert isinstance(result, dict), 'The result should be a dictionary.'\\n    assert 'audio' not in result, 'The result should not contain the key \\'audio\\'.'\\n\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_voice_activity_detection()\",\n",
      "\"function_import_fixed\": \"from pyannote.audio.core.inference import Inference\",\n",
      "\"requirements_file\": \"pyannote.audio\"\n",
      "}; pos=1251; lineno=5; colno=402)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722...723...724...725...726...727...728...729...730...731...732...733...734...735...736...737...738...739...740...741...742...743...744...745...746...747...748...749...750...751...752...753...754...755...756...757...758...759...760...761...762...763...764...765...766...767...768...769...770...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....771..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 202 (char 1081)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 202 (char 1081) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"extract_entities\",\n",
      "\"function_import\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\",\n",
      "\"function_code\": \"def extract_entities(text):\\n    \\\"\\\"\\\"\\n    Extracts entities such as dates and company names from a given text using a pre-trained model.\\n\\n    Args:\\n        text (str): The text from which to extract entities.\\n\\n    Returns:\\n        list: A list of dictionaries. Each dictionary represents an entity and contains the entity's word, score, entity type, index, start position, and end position.\\n    \\\"\\\"\\\"\\n    tokenizer = AutoTokenizer.from_pretrained('Jean-Baptiste/camembert-ner')\\n    model = AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')\\n    nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple')\\n    result = nlp(text)\\n    return result\",\n",
      "\"test_function_code\": \"def test_extract_entities():\\n    \\\"\\\"\\\"\\n    Tests the extract_entities function.\\n    \\\"\\\"\\\"\\n    test_text1 = 'Apple a été créé le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne.'\\n    test_text2 = 'Microsoft a été fondée par Bill Gates et Paul Allen le 4 avril 1975.'\\n    test_text3 = 'Google a été créé par Larry Page et Sergey Brin alors qu'ils étaient étudiants à l'Université de Stanford.'\\n\\n    assert isinstance(extract_entities(test_text1), list)\\n    assert isinstance(extract_entities(test_text2), list)\\n    assert isinstance(extract_entities(test_text3), list)\\n\\n    print('All Tests Passed')\",\n",
      "\"call_test_function_code\": \"test_extract_entities()\",\n",
      "\"function_import_fixed\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\",\n",
      "\"requirements_file\": \"transformers\"\n",
      "}; pos=1081; lineno=5; colno=202)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772...773...774...775...776...777...778...779...780...781...782...783...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....784..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 276 (char 1104)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 276 (char 1104) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"translate_french_to_english\",\n",
      "\"function_import\": \"from transformers import pipeline\",\n",
      "\"function_code\": \"def translate_french_to_english(french_text):\\n    \\\"\\\"\\\"\\n    Translates French text to English using the 'Helsinki-NLP/opus-mt-fr-en' model from the Transformers package.\\n\\n    Args:\\n        french_text (str): The text in French to be translated.\\n\\n    Returns:\\n        str: The translated text in English.\\n\\n    Raises:\\n        ValueError: If the input is not a string.\\n    \\\"\\\"\\\"\\n    if not isinstance(french_text, str):\\n        raise ValueError('Input must be a string')\\n    translator = pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\\n    translated_text = translator(french_text)\\n    english_text = translated_text[0]['translation_text']\\n    return english_text\",\n",
      "\"test_function_code\": \"def test_translate_french_to_english():\\n    assert translate_french_to_english('Bonjour, comment ça va?') == 'Hello, how are you?'\\n    assert translate_french_to_english('Je suis content') == 'I am happy'\\n    assert translate_french_to_english('Je t\\'aime') == 'I love you'\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_translate_french_to_english()\",\n",
      "\"function_import_fixed\": \"from transformers import pipeline\",\n",
      "\"requirements_file\": \"transformers\\ntorch\"\n",
      "}; pos=1104; lineno=5; colno=276)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785...786...787...788...789...790...791...792...793...794...795...796...797...798...799...800...801...802...803...804...805...806...807...808...809...810...811...812...813...814...815...816...817...818...819...820...821...822...823...824...825...826...827...828...829...830...831...832...833...834...835...836...837...838...839...840...841...842...843...844...845...846...847...848...849...850...851...852...853...854...855...856...857...858...859...860...861...862...863...864...865...866...867...868...869...870...871...872...873...874...875...876...877...878...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...879...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....880..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 468 (char 1371)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 468 (char 1371) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"translate_english_to_french\",\n",
      "  \"function_import\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\",\n",
      "  \"function_code\": \"def translate_english_to_french(input_text: str) -> str:\\n    \\\"\\\"\\\"\\n    Translate English text to French using the T5 model from Hugging Face Transformers.\\n\\n    Args:\\n        input_text (str): The English text to be translated.\\n\\n    Returns:\\n        str: The translated French text.\\n\\n    Raises:\\n        OSError: If there is a problem with loading the model or tokenizing the input.\\n    \\\"\\\"\\\"\\n    tokenizer = T5Tokenizer.from_pretrained('t5-3b')\\n    model = T5ForConditionalGeneration.from_pretrained('t5-3b')\\n    inputs = tokenizer.encode('translate English to French: ' + input_text, return_tensors='pt')\\n    outputs = model.generate(inputs)\\n    translated_text = tokenizer.decode(outputs[0])\\n    return translated_text\",\n",
      "  \"test_function_code\": \"def test_translate_english_to_french():\\n    \\\"\\\"\\\"\\n    Test the translate_english_to_french function with some sample texts.\\n    \\\"\\\"\\\"\\n    assert translate_english_to_french('Hello, world!') == 'Bonjour, monde!'\\n    assert translate_english_to_french('The quick brown fox jumps over the lazy dog.') == 'Le renard brun rapide saute par-dessus le chien paresseux.'\\n    assert translate_english_to_french('I love machine learning.') == 'J\\'aime l\\'apprentissage automatique.'\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_translate_english_to_french()\",\n",
      "  \"function_import_fixed\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\",\n",
      "  \"requirements_file\": \"transformers\"\n",
      "}; pos=1371; lineno=5; colno=468)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....882..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 290 (char 1071)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 5 column 290 (char 1071) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "  \"function_name\": \"translate_english_to_french\",\n",
      "  \"function_import\": \"from transformers import AutoTokenizer, pipeline\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\",\n",
      "  \"function_code\": \"def translate_english_to_french(text):\\n    \\\"\\\"\\\"\\n    Translate English text to French using the T5 small model from the Transformers library.\\n\\n    Args:\\n        text (str): English text to be translated.\\n\\n    Returns:\\n        str: Translated French text.\\n    \\\"\\\"\\\"\\n    tokenizer = AutoTokenizer.from_pretrained('optimum/t5-small')\\n    model = ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\n    translator = pipeline('translation_en_to_fr', model=model, tokenizer=tokenizer)\\n    translation = translator(text)\\n    return translation[0]['translation_text']\",\n",
      "  \"test_function_code\": \"def test_translate_english_to_french():\\n    \\\"\\\"\\\"\\n    Test the translate_english_to_french function with some example sentences.\\n    \\\"\\\"\\\"\\n    assert translate_english_to_french('This is a story about a superhero who saves the day from evil villains.') == 'C\\'est une histoire sur un super-héros qui sauve la journée des méchants.'\\n    assert translate_english_to_french('The quick brown fox jumps over the lazy dog.') == 'Le renard brun rapide saute par-dessus le chien paresseux.'\\n    assert translate_english_to_french('Hello, world!') == 'Bonjour, monde!'\\n    return 'All Tests Passed'\",\n",
      "  \"call_test_function_code\": \"test_translate_english_to_french()\",\n",
      "  \"function_import_fixed\": \"from transformers import AutoTokenizer, pipeline\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\",\n",
      "  \"requirements_file\": \"transformers\\noptimum.onnxruntime\"\n",
      "}; pos=1071; lineno=5; colno=290)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883...884...885...Retrying... (Attempt 1/3)...886...887...888...889...890...891...892...893...894...895...896...Retrying... (Attempt 1/3)...Retrying... (Attempt 2/3)...Max retries reached. Exiting....897..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 539, in parse_raw\n",
      "    obj = load_str_bytes(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/parse.py\", line 37, in load_str_bytes\n",
      "    return json_loads(b)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/root/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 699 (char 946)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5093/1340527589.py\", line 22, in <cell line: 6>\n",
      "    resp = get_function_from_data(d, err)\n",
      "  File \"/tmp/ipykernel_5093/1013548028.py\", line 117, in get_function_from_data\n",
      "    resp = runnable.invoke({\"input\": str(data), \"err\": str(err), \"code_desc\": code_desc})\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 1213, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 85, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/base.py\", line 715, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/schema/output_parser.py\", line 86, in <lambda>\n",
      "    lambda inner_input: self.parse_result(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/langchain/output_parsers/openai_functions.py\", line 162, in parse_result\n",
      "    pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py\", line 548, in parse_raw\n",
      "    raise ValidationError([ErrorWrapper(e, loc=ROOT_KEY)], cls)\n",
      "pydantic.v1.error_wrappers.ValidationError: 1 validation error for CodeResp\n",
      "__root__\n",
      "  Invalid \\escape: line 4 column 699 (char 946) (type=value_error.jsondecode; msg=Invalid \\escape; doc={\n",
      "\"function_name\": \"transcribe_and_analyze_sentiment\",\n",
      "\"function_import\": \"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\nfrom datasets import load_dataset\\nfrom sentiment_analysis_model import SentimentAnalysisModel\",\n",
      "\"function_code\": \"def transcribe_and_analyze_sentiment(audio_sample):\\n    '''\\n    Transcribes an audio sample using the Whisper ASR model and analyzes the sentiment of the transcription.\\n\\n    Args:\\n        audio_sample (dict): A dictionary containing the 'array' and 'sampling_rate' of the audio sample.\\n\\n    Returns:\\n        str: The sentiment of the transcribed text.\\n\\n    Raises:\\n        ValueError: If the audio sample is not a dictionary or does not contain 'array' and 'sampling_rate'.\\n    '''\\n    if not isinstance(audio_sample, dict) or 'array' not in audio_sample or 'sampling_rate' not in audio_sample:\\n        raise ValueError('audio_sample must be a dictionary containing \\'array\\' and \\'sampling_rate\\'.')\\n\\n    processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\n    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\n\\n    input_features = processor(audio_sample['array'], sampling_rate=audio_sample['sampling_rate'], return_tensors='pt').input_features\\n    predicted_ids = model.generate(input_features)\\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\n\\n    sentiment_analysis_model = SentimentAnalysisModel()\\n    sentiment = sentiment_analysis_model.analyze_sentiment(transcription)\\n\\n    return sentiment\",\n",
      "\"test_function_code\": \"def test_transcribe_and_analyze_sentiment():\\n    '''\\n    Tests the transcribe_and_analyze_sentiment function.\\n    '''\\n    ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\n    sample = ds[0]['audio']\\n\\n    sentiment = transcribe_and_analyze_sentiment(sample)\\n    assert isinstance(sentiment, str), 'The sentiment should be a string.'\\n\\n    return 'All Tests Passed'\",\n",
      "\"call_test_function_code\": \"test_transcribe_and_analyze_sentiment()\",\n",
      "\"function_import_fixed\": \"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\nfrom datasets import load_dataset\",\n",
      "\"requirements_file\": \"transformers\\ndatasets\\nsentiment_analysis_model\"\n",
      "}; pos=946; lineno=4; colno=699)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898...899...900...901...902...903...904...Retrying... (Attempt 1/3)...905...906...907...908...909...910...911..."
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "err_dir = \"output/hf-eval-data-v2\"\n",
    "output_dir = \"output/hf-eval-data-v3\"\n",
    "\n",
    "for idx, d in enumerate(hf_eval_data):\n",
    "    print(idx + 1, end=\"...\")\n",
    "    formatted_number = str(idx + 1).zfill(5)\n",
    "    \n",
    "    # 跳过已生成的\n",
    "    matching_files = glob.glob(f\"{output_dir}/f{formatted_number}_*\")\n",
    "    if matching_files:\n",
    "        print(\"skip\", end=\"...\")\n",
    "        continue\n",
    "    \n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            err = get_v2_err(err_dir, idx + 1)\n",
    "            resp = get_function_from_data(d, err)\n",
    "\n",
    "            # 写入 prompt\n",
    "            with open(f\"{output_dir}/f{formatted_number}_{resp.function_name}.prompt\", 'w') as f:\n",
    "                f.write(str(d) + \"\\n\\n\")\n",
    "                f.write(str(err))\n",
    "\n",
    "            # 写入 requirements\n",
    "            with open(f\"{output_dir}/f{formatted_number}_{resp.function_name}.txt\", 'w') as f:\n",
    "                f.write(resp.requirements_file)\n",
    "\n",
    "            # 写入 python\n",
    "            with open(f\"{output_dir}/f{formatted_number}_{resp.function_name}.py\", 'w') as f:\n",
    "                f.write(\"# function_import --------------------\\n\\n\")\n",
    "                f.write(resp.function_import_fixed)\n",
    "\n",
    "                f.write(\"\\n\\n# function_code --------------------\\n\\n\")\n",
    "                f.write(resp.function_code)\n",
    "\n",
    "                f.write(\"\\n\\n# test_function_code --------------------\\n\\n\")\n",
    "                f.write(resp.test_function_code)\n",
    "\n",
    "                f.write(\"\\n\\n# call_test_function_code --------------------\\n\\n\")\n",
    "                f.write(resp.call_test_function_code)\n",
    "\n",
    "            break  # Break out of the loop if the operation is successful\n",
    "        except Exception:\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{max_retries})\", end=\"...\")\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\", end=\"...\")\n",
    "                traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc4efd-42dd-435c-9682-ea4c336aa189",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Dataset\n",
    "- search from {lib}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fed617-29d7-4c09-8499-51940dd39c55",
   "metadata": {},
   "source": [
    "# 4. Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ffcd705-b1fb-4cda-a5be-c0a3f61a283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...40...41...42...43...44...45...46...47...48...49...50...51...52...53...54...55...56...57...58...59...60...61...62...63...64...65...66...67...68...69...70...71...72...73...74...75...76...77...78...79...80...81...82...83...84...85...86...87...88...89...90...91...92...93...94...95...96...97...98...99...100...101...102...103...104...105...106...107...108...109...110...111...112...113...114...115...116...117...118...119...120...121...122...123...124...125...126...127...128...129...130...131...132...133...134...135...136...137...138...139...140...141...142...143...144...145...146...147...148...149...150...151...152...153...154...155...156...157...158...159...160...161...162...163...164...165...166...167...168...169...170...171...172...173...174...175...176...177...178...179...180...181...182...183...184...185...186...187...188...189...190...191...192...193...194...195...196...197...198...199...200...201...202...203...204...205...206...207...208...209...210...211...212...213...214...215...216...217...218...219...220...221...222...223...224...225...226...227...228...229...230...231...232...233...234...235...236...237...238...239...240...241...242...243...244...245...246...247...248...249...250...251...252...253...254...255...256...257...258...259...260...261...262...263...264...265...266...267...268...269...270...271...272...273...274...275...276...277...278...279...280...281...282...283...284...285...286...287...288...289...290...291...292...293...294...295...296...297...298...299...300...301...302...303...304...305...306...307...308...309...310...311...312...313...314...315...316...317...318...319...320...321...322...323...324...325...326...327...328...329...330...331...332...333...334...335...336...337...338...339...340...341...342...343...344...345...346...347...348...349...350...351...352...353...354...355...356...357...358...359...360...361...362...363...364...365...366...367...368...369...370...371...372...373...374...375...376...377...378...379...380...381...382...383...384...385...386...387...388...389...390...391...392...393...394...395...396...397...398...399...400...401...402...403...404...405...406...407...408...409...410...411...412...413...414...415...416...417...418...419...420...421...422...423...424...425...426...427...428...429...430...431...432...433...434...435...436...437...438...439...440...441...442...443...444...445...446...447...448...449...450...451...452...453...454...455...456...457...458...459...460...461...462...463...464...465...466...467...468...469...470...471...472...473...474...475...476...477...478...479...480...481...482...483...484...485...486...487...488...489...490...491...492...493...494...495...496...497...498...499...500...501...502...503...504...505...506...507...508...509...510...511...512...513...514...515...516...517...518...519...520...521...522...523...524...525...526...527...528...529...530...531...532...533...534...535...536...537...538...539...540...541...542...543...544...545...546...547...548...549...550...551...552...553...554...555...556...557...558...559...560...561...562...563...564...565...566...567...568...569...570...571...572...573...574...575...576...577...578...579...580...581...582...583...584...585...586...587...588...589...590...591...592...593...594...595...596...597...598...599...600...601...602...603...604...605...606...607...608...609...610...611...612...613...614...615...616...617...618...619...620...621...622...623...624...625...626...627...628...629...630...631...632...633...634...635...636...637...638...639...640...641...642...643...644...645...646...647...648...649...650...651...652...653...654...655...656...657...658...659...660...661...662...663...664...665...666...667...668...669...670...671...672...673...674...675...676...677...678...679...680...681...682...683...684...685...686...687...688...689...690...691...692...693...694...695...696...697...698...699...700...701...702...703...704...705...706...707...708...709...710...711...712...713...714...715...716...717...718...719...720...721...722...723...724...725...726...727...728...729...730...731...732...733...734...735...736...737...738...739...740...741...742...743...744...745...746...747...748...749...750...751...752...753...754...755...756...757...758...759...760...761...762...763...764...765...766...767...768...769...770...771...772...773...774...775...776...777...778...779...780...781...782...783...784...785...786...787...788...789...790...791...792...793...794...795...796...797...798...799...800...801...802...803...804...805...806...807...808...809...810...811...812...813...814...815...816...817...818...819...820...821...822...823...824...825...826...827...828...829...830...831...832...833...834...835...836...837...838...839...840...841...842...843...844...845...846...847...848...849...850...851...852...853...854...855...856...857...858...859...860...861...862...863...864...865...866...867...868...869...870...871...872...873...874...875...876...877...878...879...880...881...882...883...884...885...886...887...888...889...890...891...892...893...894...895...896...897...898...899...900...901...902...903...904...905...906...907...908...909...910...911..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "(237, 894, 0.2651006711409396)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import shutil\n",
    "\n",
    "output_dir = \"output/hf-eval-data-v3\"\n",
    "valid_dir = \"output/hf-eval-data-v3-valid\"\n",
    "\n",
    "matched = 0\n",
    "valid = 0\n",
    "\n",
    "for idx, d in enumerate(hf_eval_data):\n",
    "    print(idx + 1, end=\"...\")\n",
    "    formatted_number = str(idx + 1).zfill(5)\n",
    "\n",
    "    # check err\n",
    "    matching_files_err = glob.glob(f\"{output_dir}/f{formatted_number}_*.err\")\n",
    "    if not matching_files_err:\n",
    "        continue\n",
    "    \n",
    "    matched += 1\n",
    "        \n",
    "    with open(matching_files_err[0]) as f:\n",
    "        content = f.read()\n",
    "        if 'Error' in content:\n",
    "            continue\n",
    "\n",
    "    # check out\n",
    "    matching_files_out = glob.glob(f\"{output_dir}/f{formatted_number}_*.out\")\n",
    "    if not matching_files_out:\n",
    "        continue\n",
    "        \n",
    "    with open(matching_files_out[0]) as f:\n",
    "        content = f.read()\n",
    "        if 'failed' in content:\n",
    "            continue\n",
    "            \n",
    "    # copy files\n",
    "    matching_files = glob.glob(f\"{output_dir}/f{formatted_number}_*\")\n",
    "    for source_file in matching_files:\n",
    "        dest_file = valid_dir + source_file.split(f\"{output_dir}\")[-1]\n",
    "        shutil.copy(source_file, dest_file)\n",
    "        \n",
    "    valid += 1\n",
    "    \n",
    "valid, matched, valid/matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0c6b537-54ee-493a-810a-85a7a4b03e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "894*valid/matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524a928-af33-467b-ac61-5a89d61adada",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "\n",
    "- 处理文件为 prompt\n",
    "- 跑 codellama\n",
    "- evaluate 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c2f45c-3d78-495e-8d90-32ff831a2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\n",
    "# function_import --------------------\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# function_code --------------------\n",
    "\n",
    "def extract_medical_term_relationships(medical_term):\n",
    "    \\\"\\\"\\\"\n",
    "    This function uses the pretrained model 'GanjinZero/UMLSBert_ENG' from Hugging Face Transformers to find relationships between medical terms.\n",
    "    It converts the medical terms into embeddings (dense vectors) which can be compared to find similarities and relationships.\n",
    "\n",
    "    Args:\n",
    "        medical_term (str): The medical term to be converted into an embedding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The embedding of the input medical term.\n",
    "    \\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90399be-66db-468b-97bb-b7870bee05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source activate py38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8767187e-fbc0-4c90-9565-97ffa30f5396",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_full_repo_name' from 'huggingface_hub' (/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     logging,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_full_repo_name' from 'huggingface_hub' (/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b8b747-01f7-4070-9953-b9c4064f73cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_full_repo_name' from 'huggingface_hub' (/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-7b-Python-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     logging,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_full_repo_name' from 'huggingface_hub' (/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"codellama/CodeLlama-7b-Python-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d43067-28fe-45ce-ae90-9bb575a8aef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
