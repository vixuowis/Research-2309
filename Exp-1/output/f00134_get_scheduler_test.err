/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Traceback (most recent call last):
  File "output/f00134_get_scheduler_test.py", line 27, in <module>
    test_get_scheduler()
  File "output/f00134_get_scheduler_test.py", line 10, in test_get_scheduler
    scheduler = get_scheduler('linear', optimizer, num_warmup_steps, num_training_steps)
  File "/root/Experiments/output/f00134_get_scheduler.py", line 23, in get_scheduler
    scheduler = get_linear_schedule_with_warmup(
NameError: name 'get_linear_schedule_with_warmup' is not defined
