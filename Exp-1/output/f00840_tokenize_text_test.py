from f00840_tokenize_text import *
def test_tokenize_text():
	text = "Don't you love ğŸ¤— Transformers? We sure do."
	expected_tokens = ["â–Don", "'", "t", "â–you", "â–love", "â–", "ğŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
	tokens = tokenize_text(text)
	assert tokens == expected_tokens

# Additional test cases
# test case 1
# text = "I have a new GPU"
# expected_tokens = ["I", "have", "a", "new", "GP", "##U"]
# tokens = tokenize_text(text)
# assert tokens == expected_tokens

# test case 2
# text = "Hello, world!"
# expected_tokens = ["Hello", ",", "â–world", "!"]
# tokens = tokenize_text(text)
# assert tokens == expected_tokens

# test case 3
# text = "This is a test."
# expected_tokens = ["This", "â–is", "â–a", "â–test", "."]
# tokens = tokenize_text(text)
# assert tokens == expected_tokens

# test case 4
# text = "I love coding."
# expected_tokens = ["I", "â–love", "â–coding", "."]
# tokens = tokenize_text(text)
# assert tokens == expected_tokens

# test case 5
# text = "I am happy."
# expected_tokens = ["I", "â–am", "â–happy", "."]
# tokens = tokenize_text(text)
# assert tokens == expected_tokens

test_tokenize_text()
