from typing import *
import bitsandbytes as bnb
from torch import nn
from transformers.trainer_pt_utils import get_parameter_names


def initialize_optimizer(model, training_args):
    decay_parameters = get_parameter_names(model, [nn.LayerNorm])
    decay_parameters = [name for name in decay_parameters if 'bias' not in name]
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.named_parameters() if n in decay_parameters],
            'weight_decay': training_args.weight_decay,
        },
        {
            'params': [p for n, p in model.named_parameters() if n not in decay_parameters],
            'weight_decay': 0.0,
        },
    ]

    optimizer_kwargs = {
        'betas': (training_args.adam_beta1, training_args.adam_beta2),
        'eps': training_args.adam_epsilon,
    }
    optimizer_kwargs['lr'] = training_args.learning_rate
    adam_bnb_optim = bnb.optim.Adam8bit(
        optimizer_grouped_parameters,
        betas=(training_args.adam_beta1, training_args.adam_beta2),
        eps=training_args.adam_epsilon,
        lr=training_args.learning_rate,
    )
    return adam_bnb_optim
