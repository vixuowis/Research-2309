Downloading (…)lve/main/config.json:   0%|                                                                     | 0.00/1.47k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████| 1.47k/1.47k [00:00<00:00, 164kB/s]
Downloading pytorch_model.bin:   0%|                                                                            | 0.00/242M [00:00<?, ?B/s]Downloading pytorch_model.bin:   4%|██▉                                                                | 10.5M/242M [00:01<00:42, 5.48MB/s]Downloading pytorch_model.bin:   9%|█████▊                                                             | 21.0M/242M [00:02<00:24, 9.04MB/s]Downloading pytorch_model.bin:  13%|████████▋                                                          | 31.5M/242M [00:03<00:18, 11.4MB/s]Downloading pytorch_model.bin:  17%|███████████▌                                                       | 41.9M/242M [00:03<00:15, 13.3MB/s]Downloading pytorch_model.bin:  22%|██████████████▌                                                    | 52.4M/242M [00:04<00:13, 14.5MB/s]Downloading pytorch_model.bin:  26%|█████████████████▍                                                 | 62.9M/242M [00:05<00:12, 14.8MB/s]Downloading pytorch_model.bin:  30%|████████████████████▎                                              | 73.4M/242M [00:05<00:10, 15.4MB/s]Downloading pytorch_model.bin:  35%|███████████████████████▏                                           | 83.9M/242M [00:06<00:10, 15.7MB/s]Downloading pytorch_model.bin:  39%|██████████████████████████                                         | 94.4M/242M [00:06<00:09, 16.1MB/s]Downloading pytorch_model.bin:  43%|█████████████████████████████▍                                      | 105M/242M [00:07<00:08, 16.2MB/s]Downloading pytorch_model.bin:  48%|████████████████████████████████▍                                   | 115M/242M [00:08<00:07, 16.4MB/s]Downloading pytorch_model.bin:  52%|███████████████████████████████████▎                                | 126M/242M [00:08<00:06, 16.8MB/s]Downloading pytorch_model.bin:  56%|██████████████████████████████████████▎                             | 136M/242M [00:09<00:06, 16.2MB/s]Downloading pytorch_model.bin:  61%|█████████████████████████████████████████▏                          | 147M/242M [00:10<00:06, 15.2MB/s]Downloading pytorch_model.bin:  65%|████████████████████████████████████████████▏                       | 157M/242M [00:11<00:05, 14.9MB/s]Downloading pytorch_model.bin:  69%|███████████████████████████████████████████████▏                    | 168M/242M [00:12<00:06, 12.1MB/s]Downloading pytorch_model.bin:  74%|██████████████████████████████████████████████████                  | 178M/242M [00:13<00:05, 12.5MB/s]Downloading pytorch_model.bin:  78%|█████████████████████████████████████████████████████               | 189M/242M [00:13<00:04, 12.9MB/s]Downloading pytorch_model.bin:  82%|███████████████████████████████████████████████████████▉            | 199M/242M [00:14<00:03, 12.9MB/s]Downloading pytorch_model.bin:  87%|██████████████████████████████████████████████████████████▉         | 210M/242M [00:15<00:02, 11.5MB/s]Downloading pytorch_model.bin:  91%|█████████████████████████████████████████████████████████████▊      | 220M/242M [00:16<00:01, 11.6MB/s]Downloading pytorch_model.bin:  95%|████████████████████████████████████████████████████████████████▊   | 231M/242M [00:17<00:00, 12.2MB/s]Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████▋| 241M/242M [00:18<00:00, 12.9MB/s]Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████| 242M/242M [00:18<00:00, 13.3MB/s]
Downloading (…)okenizer_config.json:   0%|                                                                     | 0.00/2.35k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████| 2.35k/2.35k [00:00<00:00, 1.28MB/s]
Downloading (…)/main/tokenizer.json:   0%|                                                                     | 0.00/2.42M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████| 2.42M/2.42M [00:01<00:00, 1.75MB/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████| 2.42M/2.42M [00:01<00:00, 1.75MB/s]
Downloading (…)cial_tokens_map.json:   0%|                                                                     | 0.00/2.20k [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████████| 2.20k/2.20k [00:00<00:00, 1.19MB/s]
Your max_length is set to 200, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)
Traceback (most recent call last):
  File "output/f00394_summarization_pipeline_test.py", line 6, in <module>
    assert summary == [{"summary_text": "The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country."}]
AssertionError
