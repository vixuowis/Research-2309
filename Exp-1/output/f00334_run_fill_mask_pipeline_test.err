Downloading (…)lve/main/config.json:   0%|                                                                       | 0.00/677 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████████| 677/677 [00:00<00:00, 60.7kB/s]
Downloading pytorch_model.bin:   0%|                                                                            | 0.00/329M [00:00<?, ?B/s]Downloading pytorch_model.bin:   3%|██▏                                                                | 10.5M/329M [00:01<00:57, 5.52MB/s]Downloading pytorch_model.bin:   6%|████▎                                                              | 21.0M/329M [00:02<00:34, 9.00MB/s]Downloading pytorch_model.bin:  10%|██████▍                                                            | 31.5M/329M [00:03<00:25, 11.7MB/s]Downloading pytorch_model.bin:  13%|████████▌                                                          | 41.9M/329M [00:03<00:21, 13.4MB/s]Downloading pytorch_model.bin:  16%|██████████▋                                                        | 52.4M/329M [00:04<00:18, 14.6MB/s]Downloading pytorch_model.bin:  19%|████████████▊                                                      | 62.9M/329M [00:04<00:17, 15.4MB/s]Downloading pytorch_model.bin:  22%|██████████████▉                                                    | 73.4M/329M [00:05<00:15, 16.1MB/s]Downloading pytorch_model.bin:  26%|█████████████████                                                  | 83.9M/329M [00:06<00:14, 16.4MB/s]Downloading pytorch_model.bin:  29%|███████████████████▏                                               | 94.4M/329M [00:06<00:14, 16.5MB/s]Downloading pytorch_model.bin:  32%|█████████████████████▋                                              | 105M/329M [00:07<00:13, 17.0MB/s]Downloading pytorch_model.bin:  35%|███████████████████████▊                                            | 115M/329M [00:07<00:12, 17.0MB/s]Downloading pytorch_model.bin:  38%|██████████████████████████                                          | 126M/329M [00:08<00:11, 17.1MB/s]Downloading pytorch_model.bin:  41%|████████████████████████████▏                                       | 136M/329M [00:09<00:11, 17.3MB/s]Downloading pytorch_model.bin:  45%|██████████████████████████████▎                                     | 147M/329M [00:09<00:10, 17.0MB/s]Downloading pytorch_model.bin:  48%|████████████████████████████████▌                                   | 157M/329M [00:10<00:09, 17.4MB/s]Downloading pytorch_model.bin:  51%|██████████████████████████████████▋                                 | 168M/329M [00:10<00:09, 17.5MB/s]Downloading pytorch_model.bin:  54%|████████████████████████████████████▊                               | 178M/329M [00:11<00:08, 17.3MB/s]Downloading pytorch_model.bin:  57%|███████████████████████████████████████                             | 189M/329M [00:12<00:07, 17.6MB/s]Downloading pytorch_model.bin:  61%|█████████████████████████████████████████▏                          | 199M/329M [00:12<00:07, 17.2MB/s]Downloading pytorch_model.bin:  64%|███████████████████████████████████████████▍                        | 210M/329M [00:13<00:06, 17.1MB/s]Downloading pytorch_model.bin:  67%|█████████████████████████████████████████████▌                      | 220M/329M [00:14<00:06, 17.2MB/s]Downloading pytorch_model.bin:  70%|███████████████████████████████████████████████▋                    | 231M/329M [00:14<00:05, 17.3MB/s]Downloading pytorch_model.bin:  73%|█████████████████████████████████████████████████▉                  | 241M/329M [00:15<00:05, 17.4MB/s]Downloading pytorch_model.bin:  77%|████████████████████████████████████████████████████                | 252M/329M [00:15<00:04, 16.6MB/s]Downloading pytorch_model.bin:  80%|██████████████████████████████████████████████████████▏             | 262M/329M [00:16<00:03, 16.9MB/s]Downloading pytorch_model.bin:  83%|████████████████████████████████████████████████████████▍           | 273M/329M [00:17<00:03, 16.9MB/s]Downloading pytorch_model.bin:  86%|██████████████████████████████████████████████████████████▌         | 283M/329M [00:17<00:02, 17.0MB/s]Downloading pytorch_model.bin:  89%|████████████████████████████████████████████████████████████▋       | 294M/329M [00:18<00:02, 15.9MB/s]Downloading pytorch_model.bin:  93%|██████████████████████████████████████████████████████████████▉     | 304M/329M [00:19<00:01, 16.0MB/s]Downloading pytorch_model.bin:  96%|█████████████████████████████████████████████████████████████████   | 315M/329M [00:19<00:00, 16.4MB/s]Downloading pytorch_model.bin:  99%|███████████████████████████████████████████████████████████████████▏| 325M/329M [00:20<00:00, 16.9MB/s]Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████| 329M/329M [00:20<00:00, 16.7MB/s]Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████| 329M/329M [00:20<00:00, 16.0MB/s]
Downloading (…)okenizer_config.json:   0%|                                                                       | 0.00/386 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████████████████| 386/386 [00:00<00:00, 216kB/s]
Downloading (…)olve/main/vocab.json:   0%|                                                                      | 0.00/798k [00:00<?, ?B/s]Downloading (…)olve/main/vocab.json: 100%|███████████████████████████████████████████████████████████████| 798k/798k [00:00<00:00, 854kB/s]Downloading (…)olve/main/vocab.json: 100%|███████████████████████████████████████████████████████████████| 798k/798k [00:00<00:00, 853kB/s]
Downloading (…)olve/main/merges.txt:   0%|                                                                      | 0.00/456k [00:00<?, ?B/s]Downloading (…)olve/main/merges.txt: 100%|██████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.76MB/s]Downloading (…)olve/main/merges.txt: 100%|██████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.72MB/s]
Downloading (…)/main/tokenizer.json:   0%|                                                                     | 0.00/2.11M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████| 2.11M/2.11M [00:00<00:00, 4.95MB/s]Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████| 2.11M/2.11M [00:00<00:00, 4.92MB/s]
Downloading (…)cial_tokens_map.json:   0%|                                                                       | 0.00/279 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████| 279/279 [00:00<00:00, 229kB/s]
Traceback (most recent call last):
  File "output/f00334_run_fill_mask_pipeline_test.py", line 6, in <module>
    predictions = run_fill_mask_pipeline(text, model_name, top_k)
  File "/root/Experiments/output/f00334_run_fill_mask_pipeline.py", line 6, in run_fill_mask_pipeline
    return mask_filler(text, top_k=top_k)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 239, in __call__
    outputs = super().__call__(inputs, **kwargs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1140, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py", line 1146, in run_single
    model_inputs = self.preprocess(inputs, **preprocess_params)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 97, in preprocess
    self.ensure_exactly_one_mask_token(model_inputs)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 91, in ensure_exactly_one_mask_token
    self._ensure_exactly_one_mask_token(input_ids)
  File "/root/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/pipelines/fill_mask.py", line 79, in _ensure_exactly_one_mask_token
    raise PipelineException(
transformers.pipelines.base.PipelineException: No mask_token (<mask>) found on the input
